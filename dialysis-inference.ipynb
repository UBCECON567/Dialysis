{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\ntitle       : \"Assignment: reproducing Grieco & McDevitt (2017) : Inference\"\nsubtitle    : \nauthor      : Paul Schrimpf\ndate        : `j using Dates; print(Dates.today())`\nbibliography: \"dialysis.bib\"\nlink-citations: true\noptions:\n      out_width : 100%\n      wrap : true\n      fig_width : 800\n      dpi : 192\n---\n\n[![](https://i.creativecommons.org/l/by-sa/4.0/88x31.png)](http://creativecommons.org/licenses/by-sa/4.0/)\n\nThis work is licensed under a [Creative Commons Attribution-ShareAlike\n4.0 International\nLicense](http://creativecommons.org/licenses/by-sa/4.0/) \n\n$$\n\\def\\indep{\\perp\\!\\!\\!\\perp}\n\\def\\Er{\\mathrm{E}}\n\\def\\R{\\mathbb{R}}\n\\def\\En{{\\mathbb{E}_n}}\n\\def\\Pr{\\mathrm{P}}\n\\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert}\n\\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\inprob{\\,{\\buildrel p \\over \\rightarrow}\\,} \n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,} \n$$\n\nThis document discusses inference for the two-step production function\nestimator in Grieco \\& McDevitt. A notebook version is available [here.](dialysis-inference.ipynb)\n\n# Inference for Grieco \\& McDevitt Replication\n\nMany, perhaps most, estimators in econometrics are\nextrumem estimators. That is, many estimators are defined by\n$$\n\\hat{\\theta} = \\argmax_{\\theta \\in \\Theta} \\hat{Q}_n(\\theta)\n$$\nwhere $\\hat{Q}_n(\\theta)$ is some objective function that depends on\ndata. Examples include maximum likelihood,\n$$\n\\hat{Q}_n(\\theta) = \\frac{1}{n} \\sum_{i=1}^n f(z_i | \\theta)\n$$\nnonlinear least squares,\n$$\n\\hat{Q}_n(\\theta) = \\frac{1}{n} \\sum_{i=1}^n (y_i - h(x_i,\\theta))^2\n$$\nand as we are using for this example, GMM,\n$$\n\\hat{Q}_n(\\theta) = \\left(\\frac{1}{n} \\sum_{i=1}^n g(z_i,\n\\theta)\\right)' \\hat{W} \\left(\\frac{1}{n} \\sum_{i=1}^n g(z_i,\n\\theta)\\right).\n$$\nSee @newey1994 for more details and examples.\n\nWe will encounter extremum estimators often in this course, so it is\nuseful to be familiar with their statistical properties. However,\nsince this course is not focused on econometrics, we will just state\nsome basic \"high-level\" conditions for consistency and asymptotic\nnormality, and only give brief sketches of proofs. Our main goal is to find\nthe asymptotic distribution of $\\hat{\\theta}$, so that we can report\nstandard errors and confidence regions. If $Q_n$ is differentiable,\nthen \n$$\n  0 = \\nabla \\hat{Q}_n(\\hat{\\theta})\n$$\nTaking a first order expansion around $\\theta_0$, \n$$\n  0 \\approx \\nabla \\hat{Q}_n(\\theta_0) + (\\hat{\\theta}-\\theta_0) \\nabla^2 Q_n(\\theta_0)\n$$\nRearranging and multiplying by $\\sqrt{n}$, gives\n$$\n\\sqrt{n}(\\hat{\\theta}-\\theta_0) \\approx (\\nabla^2 Q_n(\\theta_0))^{-1}\n\\sqrt{n} \\hat{Q}_n(\\theta_0)\n$$\nIf a law of large number applies to $\\nabla^2 Q_n(\\theta_0)$ and a\ncentral limit theorem applies to $\\sqrt{n} \\hat{Q}_n(\\theta_0)$, then\n$\\sqrt{n}(\\hat{\\theta}-\\theta_0)$ will be asymptotically normal. The\nfollowing theorem states this idea somewhat more precisely.\n\n### Consistency\n\n**Consistency for extremum estimators**: assume\n\n1. $\\hat{Q}_n(\\theta)$ converges uniformly in probability to\n   $Q_0(\\theta)$\n\n2. $Q_0(\\theta)$ is uniquely maximized at $\\theta_0$.\n\n3. $\\Theta$ is compact and $Q_0(\\theta)$ is continuous.\n\nThen $\\hat{\\theta} \\inprob \\theta_0$\n\n\n### Asymptotic normality\n\n**Asymptotic normality for extremum estimators**: assume\n\n1. $\\hat{\\theta} \\inprob \\theta_0$\n\n2. $\\theta_0 \\in interior(\\Theta)$\n\n3. $\\hat{Q}_n(\\theta)$ is twice continuously differentiable in open $N$\n   containing $\\theta$, and $\\sup_{\\theta \\in N} \\Vert \\nabla^2\n                             \\hat{Q}_n(\\theta) - H(\\theta) \\Vert\n                             \\inprob 0$ with $H(\\theta_0)$ nonsingular\n4. $\\sqrt{n} \\nabla \\hat{Q}_n(\\theta_0) \\indist N(0,\\Sigma)$\n\nThen $\\sqrt{n} (\\hat{\\theta} - \\theta_0) \\indist N\\left(0,H^{-1} \\Sigma\n  H^{-1} \\right)$\n\n### GMM \n\nFor a GMM objective function of the form:\n$$ [1/n \\sum_i g_i(\\theta)] W_n [1/n \\sum g_i(\\theta)]$$, \nif we assume:\n\n1. $1/\\sqrt{n} \\sum_i g_i(\\theta_0) \\indist N(0,\\Sigma)$\n\n2. $1/n \\sum_i \\nabla g_i(\\theta) \\inprob E[\\nabla g(\\theta)] = D$, \n   $W_n \\inprob W$\n\n3. $(D'WD)$ is nonsingular.\n\nthen the above theorem for asymptotic normality of extremum\nestimators implies that \n$$\n\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\indist N(0,\\Omega)\n$$\nwhere \n$$\n \\Omega= (D'WD)^{-1} (D' W \\Sigma W D) (D'WD)^{-1}.\n$$\nIf we additionally assume $W_n \\inprob \\Sigma^{-1}$, e.g. observations\nare independent and $W_n =\n\\widehat{Var}(g_i(\\theta))^{-1}$, then the asymptotic variance\nsimplifies to $(D' \\Sigma D)^{-1}$. This choice of $W$ is efficient in\nthat it leads to the smallest asymptotic variance. \n\n### 2-step estimators\n\nThe above applies to estimators that come from minimizing a single\nobjective function. This application involves a multi-step\nestimator. First, we estimated $\\alpha$ and $\\Phi()$, then we\nestimated $\\beta$ by GMM using moments of the form:\n$$\n\\hat{\\beta} = \\argmin [1/n \\sum_i g_i(\\beta, \\hat{\\alpha},\n \\hat{\\Phi})] W_n [1/n \\sum g_i(\\beta, \\hat{\\alpha}, \\hat{\\Phi}) ] \n$$\nA similar expansion as above will give\n$$\n\\sqrt{n}(\\hat{\\beta} - \\beta_0) \\approx -(D'WD)^{-1} D' W \\left(1/\\sqrt{n}\n\\sum g_i(\\beta, \\hat{\\alpha}, \\hat{\\Phi}) \\right)\n$$\nwhere $D=\\Er[\\nabla_\\beta g(\\beta, \\alpha, \\Phi)]$. If we also expand\n$g_i$ in terms of $\\alpha$ and $\\Phi$, we will get, \n$$\n\\sqrt{n}(\\hat{\\beta} - \\beta_0) \\approx -(D'WD)^{-1} D' W \\left(1/\\sqrt{n}\n\\sum g_i(\\beta_0, \\alpha_0, \\Phi_0) \\right) + \\Er[\\frac{\\partial g_i}{\\partial\n\\alpha}(\\beta,\\alpha,\\Phi)] \\sqrt{n}(\\hat{\\alpha}-\\alpha_0) + \\Er[\\frac{\\partial g_i}{\\partial\n\\Phi}(\\beta,\\alpha,\\Phi) \\sqrt{n}(\\hat{\\Phi}-\\Phi_0) ]. \n$$\nSo, there will be additional variance in $\\hat{\\beta}$ from the\nestimation of $\\hat{\\alpha}$ and $\\hat{\\Phi}$. Since $\\alpha$ is\nfinite dimensional, it is not too difficult to derive the distribution\nof $\\sqrt{n}(\\hat{\\alpha}-\\alpha_0)$ and estimate $\\Er[\\frac{\\partial g_i}{\\partial\n\\alpha}(\\beta,\\alpha,\\Phi)]$. However, $\\hat{\\Phi}$ is a function and\nis more difficult to deal with. Under some strong assumptions, \n$$\n \\Er[\\frac{\\partial g_i}{\\partial\n\\Phi}(\\beta,\\alpha,\\Phi) \\sqrt{n}(\\hat{\\Phi}-\\Phi_0) ] \\indist N,\n$$\nbut the needed assumptions are slightly restrictive and are tedious to\nstate. An alternative approach is to redefine $g_i$ to ensure that\n$E[\\frac{\\partial g_i}{\\partial \\Phi}] = 0$. This can be done by\nletting \n$$\n\\begin{align*}\n   g_{jt}(\\beta, \\alpha, \\Phi) = & \\left(y_{jt} - \\alpha q_{jt} -\n   x_{jt} \\beta - h(\\Phi(w_{jt-1}) -\n   x_{jt-1}\\beta) \\right) \\left(x_{jt} -\n   \\Er[x|\\Phi(w_{jt-1}) - x_{jt-1}\\beta]\\right) + \\\\\n   & - \\left(y_{jt-1} - \\alpha\n   q_{jt-1} - \\Phi(w_{jt-1})\\right) h'(\\Phi(w_{jt-1}) - x_{jt-1}\\beta) \\left(x_{jt} -\n   \\Er[x|\\Phi(w_{jt-1}) - x_{jt-1}\\beta]\\right)\n\\end{align*}\n$$\nwhere $x = (k, l)$, and \n$h(\\Phi(w_{jt-1}) - x_{jt-1}) =  \\Er[y_{jt} - \\alpha q_{jt} - x_{jt}\n \\beta |\\Phi(w_{jt-1}) - x_{jt-1}\\beta]$. It is not too difficult to\nverify that \n$$\n\\Er[g_{jt}(\\beta_0,\\alpha_0,\\Phi_0)] = 0\n$$\nand \n$$\n0 = D_\\Phi \\Er[g_{jt}(\\beta_0,\\alpha_0,\\Phi_0)]\\;\\;,\\;\\; 0 = D_h \\Er[g_{jt}(\\beta_0,\\alpha_0,\\Phi_0)] \n$$\nIn other words, these moment conditions are orthogonal in the sense of\n@chernozhukov2018. Estimation error in $\\Phi$ and $h$ only has second\norder effects on the estimate of $\\beta$. Under appropriate\nassumptions, these second order effects will vanish quickly enough\nthat they can be ignored in the asymptotic distribution of\n$\\hat{\\beta}$. \n\nWe can similarly deal with the uncertainty in $\\hat{\\alpha}$ by\nredefining the moment condition to be orthogonal with respect to\n$\\alpha$. Let\n$$\n\\tilde{g}_{jt}(\\beta,\\alpha,\\Phi) = g_{jt}(\\beta,\n\\alpha, \\Phi) - \\Er[ D_\\alpha g_{jt}(\\beta_0,\n\\alpha, \\Phi)] \\frac{(y_{jt} - \\alpha q_{jt} - \\Phi(w_{jt}))\n(\\Er[q|z_{jt},w_{jt}] - \\Er[q|w_{jt}])}{\\Er[(q -\n\\Er[q|w])(\\Er[q|z,w]-\\Er[q|w])]}.\n$$\nThen $\\Er[\\tilde{g}_{jt}(\\beta_0,\\alpha_0,\\Phi_0)] = 0$ and \n$$\n0 = D_\\Phi \\Er[\\tilde{g}_{jt}(\\beta_0,\\alpha_0,\\Phi_0)]\\;\\;,\\;\\; 0 = D_\\alpha\n\\Er[\\tilde{g}_{jt}(\\beta_0,\\alpha_0,\\Phi_0)].\n$$\nHence, \n$$\n\\begin{align*}\n\\frac{1}{\\sqrt{n}} \\sum_{j,t} \\tilde{g}_{jt}(\\beta_0, \\hat{\\alpha}, \\hat{\\Phi} )\n= & \\frac{1}{\\sqrt{n}} \\sum_{j,t} \\tilde{g}_{jt}(\\beta_0, \\alpha_0,\n\\Phi_0 ) + o_p(1) \n\\end{align*}\n$$\n\nUnder appropriate assumptions a CLT will apply to $\\tilde{g}_{jt}$, so \n$$\n\\frac{1}{\\sqrt{n}}  \\sum_{j,t} \\tilde{g}_{jt}(\\hat{\\beta}, \\hat{\\alpha},\n\\hat{\\Phi} ) = \\frac{1}{\\sqrt{n}}  \\sum_{j,t} \\tilde{g}_{jt}(\\beta_0,\\alpha_0,\\Phi_0) + o_p(1)\n\\indist N(0,\\Sigma)\n$$\nFurthermore, $\\Sigma$ can estimated by taking the sample (clustered)\ncovariance of\n$\\tilde{g}_{jt}(\\beta,\\hat{\\alpha},\\hat{\\Phi})$. Denote this by\n$\\hat{\\Sigma}(\\beta)$. We then have\n$$\n    Q_n^{CUE}(\\beta_0) = \\left(\\frac{1}{n}  \\sum_{j,t} \\tilde{g}_{jt}(\\beta_0, \\hat{\\alpha},\n    \\hat{\\Phi} ) \\right)' \\hat{\\Sigma(\\beta_0)}^{-1} \\left(\\frac{1}{n}\n    \\sum_{j,t} \\tilde{g}_{jt}(\\beta_0, \\hat{\\alpha}, \\hat{\\Phi} )\n    \\right) \\indist \\chi^2_{dim(\\tilde{g}_{jt})}.\n$$\nThis can be used to test $H_0: \\beta = \\beta_0$, or to form a\nconfidence region by taking all values of $\\beta$ for which the test\nfails to reject. Such an inference procedure is robust to\nidentification problems in that it does not require an assumption\nabout $D = D_\\beta \\Er[g_{jt}(\\beta,\\alpha_0,\\Phi_0)]$ being full rank or $Q_n$ being uniquely minimized\nat $\\beta_0$. If we are willing to assume strong identification in that \n$(D' \\Sigma(\\beta_0)^{-1} D)$ is invertible (and, loosely speaking, the estimated\nversion of this is far from singular relative to its estimation\nerror), then we can take an expansion to get \n$$\n\\sqrt{n}(\\hat{\\beta} - \\beta_0) = (D'\\Sigma^{-1} D)^{-1} D'\\Sigma^{-1}\n\\frac{1}{\\sqrt{n}} \\tilde{g}_{jt}(\\beta_0, \\alpha_0, \\Phi_0) + o_p(1)\n\\indist N(0, (D' \\Sigma^{-1} D)^{-1}).\n$$\n                                              \n\nSee my notes from 628 and 622 on [extremum estimation](https://schrimpf.github.io/GMMInference.jl/extremumEstimation/)\nand on\n[bootstrap](https://schrimpf.github.io/GMMInference.jl/bootstrap/)\nfor more details.\n\n# Results\n\nFirst, we must load the data and prepare the data. (This repeats what was done in the assignment)."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Pkg\ntry \n  using Dialysis # This assignment itself is in the \"Dialysis\" package. We will use some of the functions from it. \ncatch\n  Pkg.add(\"https://github.com/UBCECON567/Dialysis\")\nend\ndocdir = normpath(joinpath(dirname(Base.pathof(Dialysis)), \"..\",\"docs\"))\nPkg.activate(docdir)\nPkg.instantiate()\n\nusing DataFrames                  \ndialysis = loaddata()\nsort!(dialysis, (:provfs, :year))\ndialysis[!,:invest] = panellag(:stations, dialysis, :provfs, :year, -1) -\n  dialysis[!,:stations]; \ndialysis[!,:labor] = (dialysis[!,:nurseFT] + 0.5*dialysis[!,:nursePT]+\n                    dialysis[!,:ptcareFT] + 0.5*dialysis[!,:ptcarePT] +\n                    dialysis[!,:dieticiansFT] + 0.5*dialysis[!,:dieticiansPT] +\n                    dialysis[!,:social_workerFT] + 0.5*dialysis[!,:social_workerPT])\ndialysis[!,:hiring] = panellag(:labor, dialysis, :provfs, :year, -1) -\n  dialysis[!,:labor];\ndialysis[!,:for_profit] = dialysis[!,:profit_status].==\"For Profit\"\ndialysis[!,:fresenius] = dialysis[!,:chain_name].==\"FRESENIUS\"\ndialysis[!,:davita] = dialysis[!,:chain_name].==\"DAVITA\";\nusing Statistics # for mean, std, and so on \ndialysis[!,:inspected_this_year] =\n  ((dialysis[!,:days_since_inspection].>=0) .&\n   (dialysis[!,:days_since_inspection].<365))\nstateRates = by(dialysis, [:state, :year],\n                df -> mean(skipmissing(df[!,:inspected_this_year])))\nrename!(stateRates, :x1 => :state_inspection_rate)\ndialysis = join(dialysis, stateRates, on = [:state, :year]);\ndialysis[!,:city] = uppercase.(dialysis[!,:city]) \ncomps = by(dialysis,[:city,:year],\n           df -> mapreduce((x) -> ifelse(ismissing(x),0,1*(x>0)), +, df[!,:patient_months])\n           )\nrename!(comps, :x1 => :competitors)\ndialysis = join(dialysis, comps, on = [:city,:year]);\n\nusing FixedEffectModels\n\ndialysis[!,:idcat] = categorical(dialysis[!,:provfs])\nqreg = reg(dialysis, @formula(pct_septic ~ days_since_inspection + patient_age +\n                              pct_female + patient_esrd_years + pct_fistula + comorbidities +\n                              hemoglobin), Vcov.cluster(:idcat),save=true) # saves residuals in augmentdf\ndialysis[!,:quality] = -qreg.augmentdf[!,:residuals];\nlog_infmiss = x->ifelse(!ismissing(x) && x>0, log(x), missing) # -Inf confuses reg()\ndialysis[!,:lpy] = log_infmiss.(dialysis[!,:patient_months]./12);\ndialysis[!,:logL] = log_infmiss.(dialysis[!,:labor]);\ndialysis[!,:logK] = log_infmiss.(dialysis[!,:stations]);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we re-run the first step of estimation."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "inc1 = ((dialysis[!,:patient_months] .> 0) .& (dialysis[!,:labor] .> 0) .&\n           (dialysis[!,:stations] .> 0) .&\n           .!ismissing.(dialysis[!,:quality]) .&\n           .!ismissing.(dialysis[!,:std_mortality]) .&\n           (dialysis[!,:invest].==0) .&\n           (dialysis[!,:hiring].!=0));\ninc1[ismissing.(inc1)] .= false;\ndialysis[!,:inc1] = inc1;\n\ndialysis[!,:lsmr] = log.(dialysis[!,:std_mortality] .+ .01)\n\n(α, Φ, αreg, eyqz)=partiallinearIV(:lpy,  # y \n                         :quality, # q\n                         :lsmr,   # z\n                         [:hiring, :logL, :logK,\n                         :state_inspection_rate, :competitors], # w\n                         dialysis[findall(dialysis[!,:inc1]),:];\n                         npregress=(xp, xd,yd)->polyreg(xp,xd,yd,degree=1),\n                         parts=true                      \n                         ) \n\n# we will need these later in step 2\ndialysis[!,:Φ] = similar(dialysis[!,:lpy])\ndialysis[:,:Φ] .= missing\nrows = findall(dialysis[!,:inc1])\ndialysis[rows,:Φ] = Φ\ndialysis[!,:ey] = similar(dialysis[!,:lpy])\ndialysis[:,:ey] .= missing\ndialysis[rows,:ey] = eyqz[:,1]\ndialysis[!,:eq] = similar(dialysis[!,:lpy])\ndialysis[:,:eq] .= missing\ndialysis[rows,:eq] = eyqz[:,2]\ndialysis[!,:ez] = similar(dialysis[!,:lpy])\ndialysis[:,:ez] .= missing\ndialysis[rows,:ez] = eyqz[:,3]\n\nα"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `objective_gm` function in Dialysis.jl constructs the $Q_n^{CUE}$\nobjective function described above. The following code minimizes it\nand plots a confidence region for $\\beta$ based on inverting the\n$\\chi^2$ test described above."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Optim\n\n(obj, momenti, cue, varfn) = objective_gm(:lpy, :logK, :logL, :quality, :Φ,\n                                          :provfs, :year,\n                                          [:logK, :logL], dialysis,\n                                          clusterid=:idcat,\n                                          npregress=(xp,xd,yd)->polyreg(xp,xd,yd,degree=1,\n                                                                        deriv=true));\nres = optimize(b->cue(b,α),    # objective\n               [0.0, 0.0], # lower bounds, should not be needed, but\n               # may help numerical performance \n               [1.0, 1.0], # upper bounds \n               [0.4, 0.2], # initial value               \n               Fminbox(BFGS()),  # algorithm\n               autodiff=:forward)\nres\n# calculate standard errors based on normal approximation\nβhat = res.minimizer\ngi = momenti(βhat,α)\n(Σ, n) = varfn(gi)\nusing ForwardDiff\nusing LinearAlgebra # for diag\nD = ForwardDiff.jacobian(β->mean(momenti(β,α), dims=1), βhat)\nVβ = inv(D'*inv(Σ)*D)/n\n@show βhat\n@show sqrt.(diag(Vβ))\nnothing;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Plots, Distributions\nPlots.gr()\nlb = [0., 0.]\nub = [1.,  1.]\nntest = 200\nβtest = [rand(2).*(ub-lb) .+ lb for i in 1:ntest];\nβtest = vcat(βtest'...)\n\nfn = β->cue(β,α)\npval = Vector{Float64}(undef, ntest);\nqval = similar(pval);\nThreads.@threads for i in 1:size(βtest,1)\n  qval[i] = fn(βtest[i,:]);\n  pval[i] = cdf(Chisq(2),qval[i]);\nend\n\ncrit = 0.9\nfig=scatter(βtest[:,1],βtest[:,2], group=(pval.<crit), legend=false,\n            markersize=4, markerstrokewidth=0.0, seriesalpha=1.0,\n            palette=:isolum, xlabel=\"βk\", ylabel=\"βl\")\nngrid = 20\nb = range.(lb,ub, length=ngrid)\npc = Matrix{Float64}(undef, length(b[1]), length(b[2]))\nqc = similar(pc)\nThreads.@threads for i in CartesianIndices(pc)\n  qc[i] = fn([b[1][i[1]],b[2][i[2]]]);\n  pc[i] = cdf(Chisq(2),qc[i]);\nend\n\nfig=contour!(b[1],b[2],pc',\n            levels = [0.75, 0.9, 0.95, 0.99],\n             contour_labels=true)\ndisplay(fig)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we plot $Q^{CUE}_n(\\beta)$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Plots.plotly()\nsurface(b[1],b[2],qc', xlabel=\"βk\", ylabel=\"βl\")"
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.3.1"
    },
    "kernelspec": {
      "name": "julia-1.3",
      "display_name": "Julia 1.3.1",
      "language": "julia"
    }
  },
  "nbformat": 4
}
