{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dialysis \u00b6 This package contains functions useful for reproducing Grieco & McDevitt (2017). See this assignment. Function Reference \u00b6 # Dialysis.clustercov \u2014 Method . clustercov(x::Array{Real,2}, clusterid) Compute clustered, heteroskedasticity robust covariance matrix estimate for Var(\u2211 x). Assumes that observations with different clusterid are independent, and observations with the same clusterid may be arbitrarily correlated. Uses number of observations - 1 as the degrees of freedom. Arguments x number of observations by dimension of x matrix clusterid number of observations length vector Returns V dimension of x by dimension of x matrix source # Dialysis.errors_gm \u2014 Method . errors_gm(y::Symbol, k::Symbol, l::Symbol, \u03a6::Symbol, id::Symbol, t::Symbol, data::DataFrame; npregress::Function=polyreg) Returns functions that given \u03b2 calculate \u03c9(\u03b2) and \u03b7(\u03b2). Arguments y Symbol for y variable in data k Symbol for k variable in data l Symbol for l variable in data q Symbol for q variable in data \u03a6 Symbol for \u03a6 variable in data id Symbol for id variable in data t Symbol for t variable in data data DataFrame containing variables \u03b1 estimate of \u03b1 Returns \u03c9func(\u03b2) computes \u03c9 given \u03b2 for the data and \u03b1 passed in as input. length(\u03c9func(\u03b2)) == nrow(data) \u03c9func(\u03b2) will contain missings if the data does. \u03b7func(\u03b2) computes \u03b7 given \u03b2. for the data and \u03b1 passed in as input. length(\u03b7func(\u03b2)) == nrow(data) \u03b7func(\u03b2) will contain missings. Warning: this function is not thread safe! source # Dialysis.loaddata \u2014 Method . loaddata() Loads \u201cdialysisFacilityReports.rda\u201d. Returns a DataFrame. source # Dialysis.locallinear \u2014 Method . locallinear(xpred::AbstractMatrix, xdata::AbstractMatrix, ydata::AbstractMatrix) Computes local linear regression of ydata on xdata. Returns predicted y at x=xpred. Uses Scott\u2019s rule of thumb for the bandwidth and a Gaussian kernel. xdata should not include an intercept. Arguments xpred x values to compute fitted y xdata observed x ydata observed y, must have size(y)[1] == size(xdata)[1] bandwidth_multiplier multiply Scott\u2019s rule of thumb bandwidth by this number Returns Estimates of f(xpred) source # Dialysis.objective_gm \u2014 Method . objective_gm(y::Symbol, k::Symbol, l::Symbol, q::Symbol, \u03a6::Symbol, id::Symbol, t::Symbol, instruments::Array{Symbol,1}, data::DataFrame ; W=UniformScaling(1.), npregress::Function=(xp,xd,yd)->polyreg(xp,xd,yd,degree=1)) source # Dialysis.panellag \u2014 Function . panellag(x::Symbol, data::AbstractDataFrame, id::Symbol, t::Symbol, lags::Integer=1) Create lags of variables in panel data. Arguments x variable to create lag of data DataFrame containing x , id , and t id cross-section identifier t time variable lags number of lags. Can be negative, in which cause leads will be created Returns A vector containing lags of data[x]. Will be missing for id and t combinations where the lag is not contained in data . source # Dialysis.partiallinear \u2014 Method . function partiallinear(y::Symbol, x::Array{Symbol, 1}, data::DataFrame; npregress::Function=polyreg, clustervar::Symbol=Symbol()) Estimates a partially linear model. That is, estimate y = x\u03b2 + f(controls) + \u03f5 Assuming that E[\u03f5|x, controls] = 0. Arguments y symbol specificying y variable x controls list of control variables entering f data DataFrame where all variables are found npregress function for estimating E[w|x] nonparametrically. Used to partial out E[y|controls] and E[x|controls] and E[q|controls]. clustervar symbol specifying categorical variable on which to cluster when calculating standard errors Returns regression output from FixedEffectModels.jl Details Uses orthogonal (with respect to f) moments to estimate \u03b2. In particular, it uses 0 = E[(y - E[y|controls]) - (x - E[x|controls])\u03b2)*(x - E[x|controls])] to estimate \u03b2. In practice this can be done by regressing (y - E[y|controls]) on (x - E[x|controls]). FixedEffectModels is used for this regression. Due to the orthogonality of the moment condition the standard errors on \u03b2 will be the same as if E[y|controls] and E[x|controls] were observed (i.e. FixedEffectModels will report valid standard errors) source # Dialysis.partiallinearIV \u2014 Method . partiallinearIV(y::Symbol, q::Symbol, z::Symbol, controls::Array{Symbol,1}, data::DataFrame; npregress::Function) Estimates a partially linear model using IV. That is, estimate y = \u03b1q + \u03a6(controls) + \u03f5 using z as an instrument for q with first stage q = h(z,controls) + u It assumes that E[\u03f5|z, controls] = 0. Uses orthogonal (wrt \u03a6 and other nuisance functions) moments for estimating \u03b1. In particular, it uses 0 = E[(y - E[y|controls] - \u03b1(q - E[q|controls]))*(E[q|z,controls] - E[q|controls])] See section 4.2 (in particular footnote 8) of Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, and Robins (2018) for more information. In practice \u03b1 can be estimated by an iv regression of (y - E[y|controls]) on (q - E[q|controls]) using (E[q|z,controls] - E[q|controls]) as an instrument. FixedEffectModels is used for this regression. Due to the orthogonality of the moment condition, the standard error on \u03b1 will be the same as if E[y|controls] and E[q|controls] were observed (i.e. FixedEffectModels will report valid standard errors) Arguments y symbol specificying y variable q z list of instruments controls list of control variables entering \u03a6 data DataFrame where all variables are found npregress function for estimating E[w|x] nonparametrically. Used to partial out E[y|controls], E[q|z,controls], and E[q|controls]. Syntax should be the same as locallinear or polyreg Returns \u03b1 estimate of \u03b1 \u03a6 estimate of \u03a6(controls) regest regression output with standard error for \u03b1 source # Dialysis.polyreg \u2014 Method . polyreg(xpred::AbstractMatrix, xdata::AbstractMatrix, ydata::AbstractMatrix; degree=1) Computes polynomial regression of ydata on xdata. Returns predicted y at x=xpred. Arguments xpred x values to compute fitted y xdata observed x ydata observed y, must have size(y)[1] == size(xdata)[1] degree deriv whether to also return df(xpred). Only implemented when xdata is one dimentional Returns Estimates of f(xpred) source","title":"Function Reference"},{"location":"#dialysis","text":"This package contains functions useful for reproducing Grieco & McDevitt (2017). See this assignment.","title":"Dialysis"},{"location":"#function-reference","text":"# Dialysis.clustercov \u2014 Method . clustercov(x::Array{Real,2}, clusterid) Compute clustered, heteroskedasticity robust covariance matrix estimate for Var(\u2211 x). Assumes that observations with different clusterid are independent, and observations with the same clusterid may be arbitrarily correlated. Uses number of observations - 1 as the degrees of freedom. Arguments x number of observations by dimension of x matrix clusterid number of observations length vector Returns V dimension of x by dimension of x matrix source # Dialysis.errors_gm \u2014 Method . errors_gm(y::Symbol, k::Symbol, l::Symbol, \u03a6::Symbol, id::Symbol, t::Symbol, data::DataFrame; npregress::Function=polyreg) Returns functions that given \u03b2 calculate \u03c9(\u03b2) and \u03b7(\u03b2). Arguments y Symbol for y variable in data k Symbol for k variable in data l Symbol for l variable in data q Symbol for q variable in data \u03a6 Symbol for \u03a6 variable in data id Symbol for id variable in data t Symbol for t variable in data data DataFrame containing variables \u03b1 estimate of \u03b1 Returns \u03c9func(\u03b2) computes \u03c9 given \u03b2 for the data and \u03b1 passed in as input. length(\u03c9func(\u03b2)) == nrow(data) \u03c9func(\u03b2) will contain missings if the data does. \u03b7func(\u03b2) computes \u03b7 given \u03b2. for the data and \u03b1 passed in as input. length(\u03b7func(\u03b2)) == nrow(data) \u03b7func(\u03b2) will contain missings. Warning: this function is not thread safe! source # Dialysis.loaddata \u2014 Method . loaddata() Loads \u201cdialysisFacilityReports.rda\u201d. Returns a DataFrame. source # Dialysis.locallinear \u2014 Method . locallinear(xpred::AbstractMatrix, xdata::AbstractMatrix, ydata::AbstractMatrix) Computes local linear regression of ydata on xdata. Returns predicted y at x=xpred. Uses Scott\u2019s rule of thumb for the bandwidth and a Gaussian kernel. xdata should not include an intercept. Arguments xpred x values to compute fitted y xdata observed x ydata observed y, must have size(y)[1] == size(xdata)[1] bandwidth_multiplier multiply Scott\u2019s rule of thumb bandwidth by this number Returns Estimates of f(xpred) source # Dialysis.objective_gm \u2014 Method . objective_gm(y::Symbol, k::Symbol, l::Symbol, q::Symbol, \u03a6::Symbol, id::Symbol, t::Symbol, instruments::Array{Symbol,1}, data::DataFrame ; W=UniformScaling(1.), npregress::Function=(xp,xd,yd)->polyreg(xp,xd,yd,degree=1)) source # Dialysis.panellag \u2014 Function . panellag(x::Symbol, data::AbstractDataFrame, id::Symbol, t::Symbol, lags::Integer=1) Create lags of variables in panel data. Arguments x variable to create lag of data DataFrame containing x , id , and t id cross-section identifier t time variable lags number of lags. Can be negative, in which cause leads will be created Returns A vector containing lags of data[x]. Will be missing for id and t combinations where the lag is not contained in data . source # Dialysis.partiallinear \u2014 Method . function partiallinear(y::Symbol, x::Array{Symbol, 1}, data::DataFrame; npregress::Function=polyreg, clustervar::Symbol=Symbol()) Estimates a partially linear model. That is, estimate y = x\u03b2 + f(controls) + \u03f5 Assuming that E[\u03f5|x, controls] = 0. Arguments y symbol specificying y variable x controls list of control variables entering f data DataFrame where all variables are found npregress function for estimating E[w|x] nonparametrically. Used to partial out E[y|controls] and E[x|controls] and E[q|controls]. clustervar symbol specifying categorical variable on which to cluster when calculating standard errors Returns regression output from FixedEffectModels.jl Details Uses orthogonal (with respect to f) moments to estimate \u03b2. In particular, it uses 0 = E[(y - E[y|controls]) - (x - E[x|controls])\u03b2)*(x - E[x|controls])] to estimate \u03b2. In practice this can be done by regressing (y - E[y|controls]) on (x - E[x|controls]). FixedEffectModels is used for this regression. Due to the orthogonality of the moment condition the standard errors on \u03b2 will be the same as if E[y|controls] and E[x|controls] were observed (i.e. FixedEffectModels will report valid standard errors) source # Dialysis.partiallinearIV \u2014 Method . partiallinearIV(y::Symbol, q::Symbol, z::Symbol, controls::Array{Symbol,1}, data::DataFrame; npregress::Function) Estimates a partially linear model using IV. That is, estimate y = \u03b1q + \u03a6(controls) + \u03f5 using z as an instrument for q with first stage q = h(z,controls) + u It assumes that E[\u03f5|z, controls] = 0. Uses orthogonal (wrt \u03a6 and other nuisance functions) moments for estimating \u03b1. In particular, it uses 0 = E[(y - E[y|controls] - \u03b1(q - E[q|controls]))*(E[q|z,controls] - E[q|controls])] See section 4.2 (in particular footnote 8) of Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, and Robins (2018) for more information. In practice \u03b1 can be estimated by an iv regression of (y - E[y|controls]) on (q - E[q|controls]) using (E[q|z,controls] - E[q|controls]) as an instrument. FixedEffectModels is used for this regression. Due to the orthogonality of the moment condition, the standard error on \u03b1 will be the same as if E[y|controls] and E[q|controls] were observed (i.e. FixedEffectModels will report valid standard errors) Arguments y symbol specificying y variable q z list of instruments controls list of control variables entering \u03a6 data DataFrame where all variables are found npregress function for estimating E[w|x] nonparametrically. Used to partial out E[y|controls], E[q|z,controls], and E[q|controls]. Syntax should be the same as locallinear or polyreg Returns \u03b1 estimate of \u03b1 \u03a6 estimate of \u03a6(controls) regest regression output with standard error for \u03b1 source # Dialysis.polyreg \u2014 Method . polyreg(xpred::AbstractMatrix, xdata::AbstractMatrix, ydata::AbstractMatrix; degree=1) Computes polynomial regression of ydata on xdata. Returns predicted y at x=xpred. Arguments xpred x values to compute fitted y xdata observed x ydata observed y, must have size(y)[1] == size(xdata)[1] degree deriv whether to also return df(xpred). Only implemented when xdata is one dimentional Returns Estimates of f(xpred) source","title":"Function Reference"},{"location":"dialysis-inference/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} \\def\\inprob{\\,{\\buildrel p \\over \\rightarrow}\\,} \\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,} This document discusses inference for the two-step production function estimator in Grieco & McDevitt. A notebook version is available here. Inference for Grieco & McDevitt Replication \u00b6 Many, perhaps most, estimators in econometrics are extrumem estimators. That is, many estimators are defined by \\hat{\\theta} = \\argmax_{\\theta \\in \\Theta} \\hat{Q}_n(\\theta) where $\\hat{Q}_n(\\theta)$ is some objective function that depends on data. Examples include maximum likelihood, \\hat{Q}_n(\\theta) = \\frac{1}{n} \\sum_{i=1}^n f(z_i | \\theta) nonlinear least squares, \\hat{Q}_n(\\theta) = \\frac{1}{n} \\sum_{i=1}^n (y_i - h(x_i,\\theta))^2 and as we are using for this example, GMM, \\hat{Q}_n(\\theta) = \\left(\\frac{1}{n} \\sum_{i=1}^n g(z_i, \\theta)\\right)' \\hat{W} \\left(\\frac{1}{n} \\sum_{i=1}^n g(z_i, \\theta)\\right). See Newey and McFadden ( 1994 ) for more details and examples. We will encounter extremum estimators often in this course, so it is useful to be familiar with their statistical properties. However, since this course is not focused on econometrics, we will just state some basic \u201chigh-level\u201d conditions for consistency and asymptotic normality, and only give brief sketches of proofs. Our main goal is to find the asymptotic distribution of $\\hat{\\theta}$, so that we can report standard errors and confidence regions. If $Q_n$ is differentiable, then 0 = \\nabla \\hat{Q}_n(\\hat{\\theta}) Taking a first order expansion around $\\theta_0$, 0 \\approx \\nabla \\hat{Q}_n(\\theta_0) + (\\hat{\\theta}-\\theta_0) \\nabla^2 Q_n(\\theta_0) Rearranging and multiplying by $\\sqrt{n}$, gives \\sqrt{n}(\\hat{\\theta}-\\theta_0) \\approx (\\nabla^2 Q_n(\\theta_0))^{-1} \\sqrt{n} \\hat{Q}_n(\\theta_0) If a law of large number applies to $\\nabla^2 Q_n(\\theta_0)$ and a central limit theorem applies to $\\sqrt{n} \\hat{Q}_n(\\theta_0)$, then $\\sqrt{n}(\\hat{\\theta}-\\theta_0)$ will be asymptotically normal. The following theorem states this idea somewhat more precisely. Consistency \u00b6 Consistency for extremum estimators : assume $\\hat{Q}_n(\\theta)$ converges uniformly in probability to $Q_0(\\theta)$ $Q_0(\\theta)$ is uniquely maximized at $\\theta_0$. $\\Theta$ is compact and $Q_0(\\theta)$ is continuous. Then $\\hat{\\theta} \\inprob \\theta_0$ Asymptotic normality \u00b6 Asymptotic normality for extremum estimators : assume $\\hat{\\theta} \\inprob \\theta_0$ $\\theta_0 \\in interior(\\Theta)$ $\\hat{Q} n(\\theta)$ is twice continuously differentiable in open $N$ containing $\\theta$, and $\\sup \\Vert \\nabla^2 \\hat{Q}_n(\\theta) - H(\\theta) \\Vert \\inprob 0$ with $H(\\theta_0)$ nonsingular $\\sqrt{n} \\nabla \\hat{Q}_n(\\theta_0) \\indist N(0,\\Sigma)$ Then $\\sqrt{n} (\\hat{\\theta} - \\theta_0) \\indist N\\left(0,H^{-1} \\Sigma H^{-1} \\right)$ GMM \u00b6 For a GMM objective function of the form: [1/n \\sum_i g_i(\\theta)] W_n [1/n \\sum g_i(\\theta)] , if we assume: $1/\\sqrt{n} \\sum_i g_i(\\theta_0) \\indist N(0,\\Sigma)$ $1/n \\sum_i \\nabla g_i(\\theta) \\inprob E[\\nabla g(\\theta)] = D$, $W_n \\inprob W$ $(D\u2019WD)$ is nonsingular. then the above theorem for asymptotic normality of extremum estimators implies that \\sqrt{n}(\\hat{\\theta} - \\theta_0) \\indist N(0,\\Omega) where \\Omega= (D'WD)^{-1} (D' W \\Sigma W D) (D'WD)^{-1}. If we additionally assume $W_n \\inprob \\Sigma^{-1}$, e.g. observations are independent and $W_n = \\widehat{Var}(g_i(\\theta))^{-1}$, then the asymptotic variance simplifies to $(D\u2019 \\Sigma D)^{-1}$. This choice of $W$ is efficient in that it leads to the smallest asymptotic variance. 2-step estimators \u00b6 The above applies to estimators that come from minimizing a single objective function. This application involves a multi-step estimator. First, we estimated $\\alpha$ and $\\Phi()$, then we estimated $\\beta$ by GMM using moments of the form: \\hat{\\beta} = \\argmin [1/n \\sum_i g_i(\\beta, \\hat{\\alpha}, \\hat{\\Phi})] W_n [1/n \\sum g_i(\\beta, \\hat{\\alpha}, \\hat{\\Phi}) ] A similar expansion as above will give \\sqrt{n}(\\hat{\\beta} - \\beta_0) \\approx -(D'WD)^{-1} D' W \\left(1/\\sqrt{n} \\sum g_i(\\beta, \\hat{\\alpha}, \\hat{\\Phi}) \\right) where $D=\\Er[\\nabla_\\beta g(\\beta, \\alpha, \\Phi)]$. If we also expand $g_i$ in terms of $\\alpha$ and $\\Phi$, we will get, \\sqrt{n}(\\hat{\\beta} - \\beta_0) \\approx -(D'WD)^{-1} D' W \\left(1/\\sqrt{n} \\sum g_i(\\beta_0, \\alpha_0, \\Phi_0) \\right) + \\Er[\\frac{\\partial g_i}{\\partial \\alpha}(\\beta,\\alpha,\\Phi)] \\sqrt{n}(\\hat{\\alpha}-\\alpha_0) + \\Er[\\frac{\\partial g_i}{\\partial \\Phi}(\\beta,\\alpha,\\Phi) \\sqrt{n}(\\hat{\\Phi}-\\Phi_0) ]. So, there will be additional variance in $\\hat{\\beta}$ from the estimation of $\\hat{\\alpha}$ and $\\hat{\\Phi}$. Since $\\alpha$ is finite dimensional, it is not too difficult to derive the distribution of $\\sqrt{n}(\\hat{\\alpha}-\\alpha_0)$ and estimate $\\Er[\\frac{\\partial g_i}{\\partial \\alpha}(\\beta,\\alpha,\\Phi)]$. However, $\\hat{\\Phi}$ is a function and is more difficult to deal with. Under some strong assumptions, \\Er[\\frac{\\partial g_i}{\\partial \\Phi}(\\beta,\\alpha,\\Phi) \\sqrt{n}(\\hat{\\Phi}-\\Phi_0) ] \\indist N, but the needed assumptions are slightly restrictive and are tedious to state. An alternative approach is to redefine $g_i$ to ensure that $E[\\frac{\\partial g_i}{\\partial \\Phi}] = 0$. This can be done by letting \\begin{align*} g_{jt}(\\beta, \\alpha, \\Phi) = & \\left(y_{jt} - \\alpha q_{jt} - x_{jt} \\beta - h(\\Phi(w_{jt-1}) - x_{jt-1}\\beta) \\right) \\left(x_{jt} - \\Er[x|\\Phi(w_{jt-1}) - x_{jt-1}\\beta]\\right) + \\\\ & - \\left(y_{jt-1} - \\alpha q_{jt-1} - \\Phi(w_{jt-1})\\right) h'(\\Phi(w_{jt-1}) - x_{jt-1}\\beta) \\left(x_{jt} - \\Er[x|\\Phi(w_{jt-1}) - x_{jt-1}\\beta]\\right) \\end{align*} where $x = (k, l)$, and $h(\\Phi(w_{jt-1}) - x_{jt-1}) = \\Er[y_{jt} - \\alpha q_{jt} - x_{jt} \\beta |\\Phi(w_{jt-1}) - x_{jt-1}\\beta]$. It is not too difficult to verify that \\Er[g_{jt}(\\beta_0,\\alpha_0,\\Phi_0)] = 0 and 0 = D_\\Phi \\Er[g_{jt}(\\beta_0,\\alpha_0,\\Phi_0)]\\;\\;,\\;\\; 0 = D_h \\Er[g_{jt}(\\beta_0,\\alpha_0,\\Phi_0)] In other words, these moment conditions are orthogonal in the sense of Chernozhukov et al. ( 2018 ). Estimation error in $\\Phi$ and $h$ only has second order effects on the estimate of $\\beta$. Under appropriate assumptions, these second order effects will vanish quickly enough that they can be ignored in the asymptotic distribution of $\\hat{\\beta}$. We can similarly deal with the uncertainty in $\\hat{\\alpha}$ by redefining the moment condition to be orthogonal with respect to $\\alpha$. Let \\tilde{g}_{jt}(\\beta,\\alpha,\\Phi) = g_{jt}(\\beta, \\alpha, \\Phi) - \\Er[ D_\\alpha g_{jt}(\\beta_0, \\alpha, \\Phi)] \\frac{(y_{jt} - \\alpha q_{jt} - \\Phi(w_{jt})) (\\Er[q|z_{jt},w_{jt}] - \\Er[q|w_{jt}])}{\\Er[(q - \\Er[q|w])(\\Er[q|z,w]-\\Er[q|w])]}. Then $\\Er[\\tilde{g}_{jt}(\\beta_0,\\alpha_0,\\Phi_0)] = 0$ and 0 = D_\\Phi \\Er[\\tilde{g}_{jt}(\\beta_0,\\alpha_0,\\Phi_0)]\\;\\;,\\;\\; 0 = D_\\alpha \\Er[\\tilde{g}_{jt}(\\beta_0,\\alpha_0,\\Phi_0)]. Hence, \\begin{align*} \\frac{1}{\\sqrt{n}} \\sum_{j,t} \\tilde{g}_{jt}(\\beta_0, \\hat{\\alpha}, \\hat{\\Phi} ) = & \\frac{1}{\\sqrt{n}} \\sum_{j,t} \\tilde{g}_{jt}(\\beta_0, \\alpha_0, \\Phi_0 ) + o_p(1) \\end{align*} Under appropriate assumptions a CLT will apply to $\\tilde{g} {jt}$, so \\frac{1}{\\sqrt{n}} \\sum_{j,t} \\tilde{g}_{jt}(\\hat{\\beta}, \\hat{\\alpha}, \\hat{\\Phi} ) = \\frac{1}{\\sqrt{n}} \\sum_{j,t} \\tilde{g}_{jt}(\\beta_0,\\alpha_0,\\Phi_0) + o_p(1) \\indist N(0,\\Sigma) Furthermore, $\\Sigma$ can estimated by taking the sample (clustered) covariance of $\\tilde{g} (\\beta,\\hat{\\alpha},\\hat{\\Phi})$. Denote this by $\\hat{\\Sigma}(\\beta)$. We then have Q_n^{CUE}(\\beta_0) = \\left(\\frac{1}{n} \\sum_{j,t} \\tilde{g}_{jt}(\\beta_0, \\hat{\\alpha}, \\hat{\\Phi} ) \\right)' \\hat{\\Sigma(\\beta_0)}^{-1} \\left(\\frac{1}{n} \\sum_{j,t} \\tilde{g}_{jt}(\\beta_0, \\hat{\\alpha}, \\hat{\\Phi} ) \\right) \\indist \\chi^2_{dim(\\tilde{g}_{jt})}. This can be used to test $H_0: \\beta = \\beta_0$, or to form a confidence region by taking all values of $\\beta$ for which the test fails to reject. Such an inference procedure is robust to identification problems in that it does not require an assumption about $D = D_\\beta \\Er[g_{jt}(\\beta,\\alpha_0,\\Phi_0)]$ being full rank or $Q_n$ being uniquely minimized at $\\beta_0$. If we are willing to assume strong identification in that $(D\u2019 \\Sigma(\\beta_0)^{-1} D)$ is invertible (and, loosely speaking, the estimated version of this is far from singular relative to its estimation error), then we can take an expansion to get \\sqrt{n}(\\hat{\\beta} - \\beta_0) = (D'\\Sigma^{-1} D)^{-1} D'\\Sigma^{-1} \\frac{1}{\\sqrt{n}} \\tilde{g}_{jt}(\\beta_0, \\alpha_0, \\Phi_0) + o_p(1) \\indist N(0, (D' \\Sigma^{-1} D)^{-1}). See my notes from 628 and 622 on extremum estimation and on bootstrap for more details. Results \u00b6 First, we must load the data and prepare the data. (This repeats what was done in the assignment). using Pkg try using Dialysis # This assignment itself is in the \"Dialysis\" package. We will use some of the functions from it. catch Pkg.add(\"https://github.com/UBCECON567/Dialysis\") end docdir = normpath(joinpath(dirname(Base.pathof(Dialysis)), \"..\",\"docs\")) Pkg.activate(docdir) Activating environment at `~/.julia/dev/Dialysis/docs/Project.toml` Pkg.instantiate() using DataFrames dialysis = loaddata() sort!(dialysis, (:provfs, :year)) dialysis[!,:invest] = panellag(:stations, dialysis, :provfs, :year, -1) - dialysis[!,:stations]; dialysis[!,:labor] = (dialysis[!,:nurseFT] + 0.5*dialysis[!,:nursePT]+ dialysis[!,:ptcareFT] + 0.5*dialysis[!,:ptcarePT] + dialysis[!,:dieticiansFT] + 0.5*dialysis[!,:dieticiansPT] + dialysis[!,:social_workerFT] + 0.5*dialysis[!,:social_workerPT]) dialysis[!,:hiring] = panellag(:labor, dialysis, :provfs, :year, -1) - dialysis[!,:labor]; dialysis[!,:for_profit] = dialysis[!,:profit_status].==\"For Profit\" dialysis[!,:fresenius] = dialysis[!,:chain_name].==\"FRESENIUS\" dialysis[!,:davita] = dialysis[!,:chain_name].==\"DAVITA\"; using Statistics # for mean, std, and so on dialysis[!,:inspected_this_year] = ((dialysis[!,:days_since_inspection].>=0) .& (dialysis[!,:days_since_inspection].<365)) stateRates = by(dialysis, [:state, :year], df -> mean(skipmissing(df[!,:inspected_this_year]))) rename!(stateRates, :x1 => :state_inspection_rate) dialysis = join(dialysis, stateRates, on = [:state, :year]); dialysis[!,:city] = uppercase.(dialysis[!,:city]) comps = by(dialysis,[:city,:year], df -> mapreduce((x) -> ifelse(ismissing(x),0,1*(x>0)), +, df[!,:patient_months]) ) rename!(comps, :x1 => :competitors) dialysis = join(dialysis, comps, on = [:city,:year]); using FixedEffectModels dialysis[!,:idcat] = categorical(dialysis[!,:provfs]) qreg = reg(dialysis, @formula(pct_septic ~ days_since_inspection + patient_age + pct_female + patient_esrd_years + pct_fistula + comorbidities + hemoglobin), Vcov.cluster(:idcat),save=true) # saves residuals in augmentdf dialysis[!,:quality] = -qreg.augmentdf[!,:residuals]; log_infmiss = x->ifelse(!ismissing(x) && x>0, log(x), missing) # -Inf confuses reg() dialysis[!,:lpy] = log_infmiss.(dialysis[!,:patient_months]./12); dialysis[!,:logL] = log_infmiss.(dialysis[!,:labor]); dialysis[!,:logK] = log_infmiss.(dialysis[!,:stations]); Now, we re-run the first step of estimation. inc1 = ((dialysis[!,:patient_months] .> 0) .& (dialysis[!,:labor] .> 0) .& (dialysis[!,:stations] .> 0) .& .!ismissing.(dialysis[!,:quality]) .& .!ismissing.(dialysis[!,:std_mortality]) .& (dialysis[!,:invest].==0) .& (dialysis[!,:hiring].!=0)); inc1[ismissing.(inc1)] .= false; dialysis[!,:inc1] = inc1; dialysis[!,:lsmr] = log.(dialysis[!,:std_mortality] .+ .01) (\u03b1, \u03a6, \u03b1reg, eyqz)=partiallinearIV(:lpy, # y :quality, # q :lsmr, # z [:hiring, :logL, :logK, :state_inspection_rate, :competitors], # w dialysis[findall(dialysis[!,:inc1]),:]; npregress=(xp, xd,yd)->polyreg(xp,xd,yd,degree=1), parts=true ) # we will need these later in step 2 dialysis[!,:\u03a6] = similar(dialysis[!,:lpy]) dialysis[:,:\u03a6] .= missing rows = findall(dialysis[!,:inc1]) dialysis[rows,:\u03a6] = \u03a6 dialysis[!,:ey] = similar(dialysis[!,:lpy]) dialysis[:,:ey] .= missing dialysis[rows,:ey] = eyqz[:,1] dialysis[!,:eq] = similar(dialysis[!,:lpy]) dialysis[:,:eq] .= missing dialysis[rows,:eq] = eyqz[:,2] dialysis[!,:ez] = similar(dialysis[!,:lpy]) dialysis[:,:ez] .= missing dialysis[rows,:ez] = eyqz[:,3] \u03b1 -0.016731503377462428 The objective_gm function in Dialysis.jl constructs the $Q_n^{CUE}$ objective function described above. The following code minimizes it and plots a confidence region for $\\beta$ based on inverting the $\\chi^2$ test described above. using Optim (obj, momenti, cue, varfn) = objective_gm(:lpy, :logK, :logL, :quality, :\u03a6, :provfs, :year, [:logK, :logL], dialysis, clusterid=:idcat, npregress=(xp,xd,yd)->polyreg(xp,xd,yd,degree=1, deriv=true)); res = optimize(b->cue(b,\u03b1), # objective [0.0, 0.0], # lower bounds, should not be needed, but # may help numerical performance [1.0, 1.0], # upper bounds [0.4, 0.2], # initial value Fminbox(BFGS()), # algorithm autodiff=:forward) res * Status: success * Candidate solution Minimizer: [2.01e-20, 1.50e-01] Minimum: 2.340586e+00 * Found with Algorithm: Fminbox with BFGS Initial Point: [4.00e-01, 2.00e-01] * Convergence measures |x - x'| = 3.00e-17 \u2270 0.0e+00 |x - x'|/|x'| = 2.00e-16 \u2270 0.0e+00 |f(x) - f(x')| = 0.00e+00 \u2264 0.0e+00 |f(x) - f(x')|/|f(x')| = 0.00e+00 \u2264 0.0e+00 |g(x)| = 4.44e+00 \u2270 1.0e-08 * Work counters Seconds run: 187 (vs limit Inf) Iterations: 7 f(x) calls: 1273 \u2207f(x) calls: 1273 # calculate standard errors based on normal approximation \u03b2hat = res.minimizer gi = momenti(\u03b2hat,\u03b1) (\u03a3, n) = varfn(gi) using ForwardDiff using LinearAlgebra # for diag D = ForwardDiff.jacobian(\u03b2->mean(momenti(\u03b2,\u03b1), dims=1), \u03b2hat) V\u03b2 = inv(D'*inv(\u03a3)*D)/n @show \u03b2hat \u03b2hat = [2.0072100763983107e-20, 0.14987299477155056] @show sqrt.(diag(V\u03b2)) sqrt.(diag(V\u03b2)) = [1.4692926354291502, 0.05948171117514949] nothing; using Plots, Distributions Plots.gr() lb = [0., 0.] ub = [1., 1.] ntest = 200 \u03b2test = [rand(2).*(ub-lb) .+ lb for i in 1:ntest]; \u03b2test = vcat(\u03b2test'...) fn = \u03b2->cue(\u03b2,\u03b1) pval = Vector{Float64}(undef, ntest); qval = similar(pval); Threads.@threads for i in 1:size(\u03b2test,1) qval[i] = fn(\u03b2test[i,:]); pval[i] = cdf(Chisq(2),qval[i]); end crit = 0.9 fig=scatter(\u03b2test[:,1],\u03b2test[:,2], group=(pval.<crit), legend=false, markersize=4, markerstrokewidth=0.0, seriesalpha=1.0, palette=:isolum, xlabel=\"\u03b2k\", ylabel=\"\u03b2l\") ngrid = 20 b = range.(lb,ub, length=ngrid) pc = Matrix{Float64}(undef, length(b[1]), length(b[2])) qc = similar(pc) Threads.@threads for i in CartesianIndices(pc) qc[i] = fn([b[1][i[1]],b[2][i[2]]]); pc[i] = cdf(Chisq(2),qc[i]); end fig=contour!(b[1],b[2],pc', levels = [0.75, 0.9, 0.95, 0.99], contour_labels=true) display(fig) Here we plot $Q^{CUE}_n(\\beta)$. Plots.plotly() surface(b[1],b[2],qc', xlabel=\"\u03b2k\", ylabel=\"\u03b2l\") Plot{Plots.PlotlyBackend() n=1} Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. \u201cDouble/Debiased Machine Learning for Treatment and Structural Parameters.\u201d The Econometrics Journal 21 (1): C1\u2013C68. https://doi.org/10.1111/ectj.12097 . Newey, Whitney K., and Daniel McFadden. 1994. \u201cChapter 36 Large Sample Estimation and Hypothesis Testing.\u201d In, 4:2111\u20132245. Handbook of Econometrics. Elsevier. https://doi.org/https://doi.org/10.1016/S1573-4412(05)80005-4 .","title":"Inference"},{"location":"dialysis-inference/#inference-for-grieco-mcdevitt-replication","text":"Many, perhaps most, estimators in econometrics are extrumem estimators. That is, many estimators are defined by \\hat{\\theta} = \\argmax_{\\theta \\in \\Theta} \\hat{Q}_n(\\theta) where $\\hat{Q}_n(\\theta)$ is some objective function that depends on data. Examples include maximum likelihood, \\hat{Q}_n(\\theta) = \\frac{1}{n} \\sum_{i=1}^n f(z_i | \\theta) nonlinear least squares, \\hat{Q}_n(\\theta) = \\frac{1}{n} \\sum_{i=1}^n (y_i - h(x_i,\\theta))^2 and as we are using for this example, GMM, \\hat{Q}_n(\\theta) = \\left(\\frac{1}{n} \\sum_{i=1}^n g(z_i, \\theta)\\right)' \\hat{W} \\left(\\frac{1}{n} \\sum_{i=1}^n g(z_i, \\theta)\\right). See Newey and McFadden ( 1994 ) for more details and examples. We will encounter extremum estimators often in this course, so it is useful to be familiar with their statistical properties. However, since this course is not focused on econometrics, we will just state some basic \u201chigh-level\u201d conditions for consistency and asymptotic normality, and only give brief sketches of proofs. Our main goal is to find the asymptotic distribution of $\\hat{\\theta}$, so that we can report standard errors and confidence regions. If $Q_n$ is differentiable, then 0 = \\nabla \\hat{Q}_n(\\hat{\\theta}) Taking a first order expansion around $\\theta_0$, 0 \\approx \\nabla \\hat{Q}_n(\\theta_0) + (\\hat{\\theta}-\\theta_0) \\nabla^2 Q_n(\\theta_0) Rearranging and multiplying by $\\sqrt{n}$, gives \\sqrt{n}(\\hat{\\theta}-\\theta_0) \\approx (\\nabla^2 Q_n(\\theta_0))^{-1} \\sqrt{n} \\hat{Q}_n(\\theta_0) If a law of large number applies to $\\nabla^2 Q_n(\\theta_0)$ and a central limit theorem applies to $\\sqrt{n} \\hat{Q}_n(\\theta_0)$, then $\\sqrt{n}(\\hat{\\theta}-\\theta_0)$ will be asymptotically normal. The following theorem states this idea somewhat more precisely.","title":"Inference for Grieco &amp; McDevitt Replication"},{"location":"dialysis-inference/#consistency","text":"Consistency for extremum estimators : assume $\\hat{Q}_n(\\theta)$ converges uniformly in probability to $Q_0(\\theta)$ $Q_0(\\theta)$ is uniquely maximized at $\\theta_0$. $\\Theta$ is compact and $Q_0(\\theta)$ is continuous. Then $\\hat{\\theta} \\inprob \\theta_0$","title":"Consistency"},{"location":"dialysis-inference/#asymptotic-normality","text":"Asymptotic normality for extremum estimators : assume $\\hat{\\theta} \\inprob \\theta_0$ $\\theta_0 \\in interior(\\Theta)$ $\\hat{Q} n(\\theta)$ is twice continuously differentiable in open $N$ containing $\\theta$, and $\\sup \\Vert \\nabla^2 \\hat{Q}_n(\\theta) - H(\\theta) \\Vert \\inprob 0$ with $H(\\theta_0)$ nonsingular $\\sqrt{n} \\nabla \\hat{Q}_n(\\theta_0) \\indist N(0,\\Sigma)$ Then $\\sqrt{n} (\\hat{\\theta} - \\theta_0) \\indist N\\left(0,H^{-1} \\Sigma H^{-1} \\right)$","title":"Asymptotic normality"},{"location":"dialysis-inference/#gmm","text":"For a GMM objective function of the form: [1/n \\sum_i g_i(\\theta)] W_n [1/n \\sum g_i(\\theta)] , if we assume: $1/\\sqrt{n} \\sum_i g_i(\\theta_0) \\indist N(0,\\Sigma)$ $1/n \\sum_i \\nabla g_i(\\theta) \\inprob E[\\nabla g(\\theta)] = D$, $W_n \\inprob W$ $(D\u2019WD)$ is nonsingular. then the above theorem for asymptotic normality of extremum estimators implies that \\sqrt{n}(\\hat{\\theta} - \\theta_0) \\indist N(0,\\Omega) where \\Omega= (D'WD)^{-1} (D' W \\Sigma W D) (D'WD)^{-1}. If we additionally assume $W_n \\inprob \\Sigma^{-1}$, e.g. observations are independent and $W_n = \\widehat{Var}(g_i(\\theta))^{-1}$, then the asymptotic variance simplifies to $(D\u2019 \\Sigma D)^{-1}$. This choice of $W$ is efficient in that it leads to the smallest asymptotic variance.","title":"GMM"},{"location":"dialysis-inference/#2-step-estimators","text":"The above applies to estimators that come from minimizing a single objective function. This application involves a multi-step estimator. First, we estimated $\\alpha$ and $\\Phi()$, then we estimated $\\beta$ by GMM using moments of the form: \\hat{\\beta} = \\argmin [1/n \\sum_i g_i(\\beta, \\hat{\\alpha}, \\hat{\\Phi})] W_n [1/n \\sum g_i(\\beta, \\hat{\\alpha}, \\hat{\\Phi}) ] A similar expansion as above will give \\sqrt{n}(\\hat{\\beta} - \\beta_0) \\approx -(D'WD)^{-1} D' W \\left(1/\\sqrt{n} \\sum g_i(\\beta, \\hat{\\alpha}, \\hat{\\Phi}) \\right) where $D=\\Er[\\nabla_\\beta g(\\beta, \\alpha, \\Phi)]$. If we also expand $g_i$ in terms of $\\alpha$ and $\\Phi$, we will get, \\sqrt{n}(\\hat{\\beta} - \\beta_0) \\approx -(D'WD)^{-1} D' W \\left(1/\\sqrt{n} \\sum g_i(\\beta_0, \\alpha_0, \\Phi_0) \\right) + \\Er[\\frac{\\partial g_i}{\\partial \\alpha}(\\beta,\\alpha,\\Phi)] \\sqrt{n}(\\hat{\\alpha}-\\alpha_0) + \\Er[\\frac{\\partial g_i}{\\partial \\Phi}(\\beta,\\alpha,\\Phi) \\sqrt{n}(\\hat{\\Phi}-\\Phi_0) ]. So, there will be additional variance in $\\hat{\\beta}$ from the estimation of $\\hat{\\alpha}$ and $\\hat{\\Phi}$. Since $\\alpha$ is finite dimensional, it is not too difficult to derive the distribution of $\\sqrt{n}(\\hat{\\alpha}-\\alpha_0)$ and estimate $\\Er[\\frac{\\partial g_i}{\\partial \\alpha}(\\beta,\\alpha,\\Phi)]$. However, $\\hat{\\Phi}$ is a function and is more difficult to deal with. Under some strong assumptions, \\Er[\\frac{\\partial g_i}{\\partial \\Phi}(\\beta,\\alpha,\\Phi) \\sqrt{n}(\\hat{\\Phi}-\\Phi_0) ] \\indist N, but the needed assumptions are slightly restrictive and are tedious to state. An alternative approach is to redefine $g_i$ to ensure that $E[\\frac{\\partial g_i}{\\partial \\Phi}] = 0$. This can be done by letting \\begin{align*} g_{jt}(\\beta, \\alpha, \\Phi) = & \\left(y_{jt} - \\alpha q_{jt} - x_{jt} \\beta - h(\\Phi(w_{jt-1}) - x_{jt-1}\\beta) \\right) \\left(x_{jt} - \\Er[x|\\Phi(w_{jt-1}) - x_{jt-1}\\beta]\\right) + \\\\ & - \\left(y_{jt-1} - \\alpha q_{jt-1} - \\Phi(w_{jt-1})\\right) h'(\\Phi(w_{jt-1}) - x_{jt-1}\\beta) \\left(x_{jt} - \\Er[x|\\Phi(w_{jt-1}) - x_{jt-1}\\beta]\\right) \\end{align*} where $x = (k, l)$, and $h(\\Phi(w_{jt-1}) - x_{jt-1}) = \\Er[y_{jt} - \\alpha q_{jt} - x_{jt} \\beta |\\Phi(w_{jt-1}) - x_{jt-1}\\beta]$. It is not too difficult to verify that \\Er[g_{jt}(\\beta_0,\\alpha_0,\\Phi_0)] = 0 and 0 = D_\\Phi \\Er[g_{jt}(\\beta_0,\\alpha_0,\\Phi_0)]\\;\\;,\\;\\; 0 = D_h \\Er[g_{jt}(\\beta_0,\\alpha_0,\\Phi_0)] In other words, these moment conditions are orthogonal in the sense of Chernozhukov et al. ( 2018 ). Estimation error in $\\Phi$ and $h$ only has second order effects on the estimate of $\\beta$. Under appropriate assumptions, these second order effects will vanish quickly enough that they can be ignored in the asymptotic distribution of $\\hat{\\beta}$. We can similarly deal with the uncertainty in $\\hat{\\alpha}$ by redefining the moment condition to be orthogonal with respect to $\\alpha$. Let \\tilde{g}_{jt}(\\beta,\\alpha,\\Phi) = g_{jt}(\\beta, \\alpha, \\Phi) - \\Er[ D_\\alpha g_{jt}(\\beta_0, \\alpha, \\Phi)] \\frac{(y_{jt} - \\alpha q_{jt} - \\Phi(w_{jt})) (\\Er[q|z_{jt},w_{jt}] - \\Er[q|w_{jt}])}{\\Er[(q - \\Er[q|w])(\\Er[q|z,w]-\\Er[q|w])]}. Then $\\Er[\\tilde{g}_{jt}(\\beta_0,\\alpha_0,\\Phi_0)] = 0$ and 0 = D_\\Phi \\Er[\\tilde{g}_{jt}(\\beta_0,\\alpha_0,\\Phi_0)]\\;\\;,\\;\\; 0 = D_\\alpha \\Er[\\tilde{g}_{jt}(\\beta_0,\\alpha_0,\\Phi_0)]. Hence, \\begin{align*} \\frac{1}{\\sqrt{n}} \\sum_{j,t} \\tilde{g}_{jt}(\\beta_0, \\hat{\\alpha}, \\hat{\\Phi} ) = & \\frac{1}{\\sqrt{n}} \\sum_{j,t} \\tilde{g}_{jt}(\\beta_0, \\alpha_0, \\Phi_0 ) + o_p(1) \\end{align*} Under appropriate assumptions a CLT will apply to $\\tilde{g} {jt}$, so \\frac{1}{\\sqrt{n}} \\sum_{j,t} \\tilde{g}_{jt}(\\hat{\\beta}, \\hat{\\alpha}, \\hat{\\Phi} ) = \\frac{1}{\\sqrt{n}} \\sum_{j,t} \\tilde{g}_{jt}(\\beta_0,\\alpha_0,\\Phi_0) + o_p(1) \\indist N(0,\\Sigma) Furthermore, $\\Sigma$ can estimated by taking the sample (clustered) covariance of $\\tilde{g} (\\beta,\\hat{\\alpha},\\hat{\\Phi})$. Denote this by $\\hat{\\Sigma}(\\beta)$. We then have Q_n^{CUE}(\\beta_0) = \\left(\\frac{1}{n} \\sum_{j,t} \\tilde{g}_{jt}(\\beta_0, \\hat{\\alpha}, \\hat{\\Phi} ) \\right)' \\hat{\\Sigma(\\beta_0)}^{-1} \\left(\\frac{1}{n} \\sum_{j,t} \\tilde{g}_{jt}(\\beta_0, \\hat{\\alpha}, \\hat{\\Phi} ) \\right) \\indist \\chi^2_{dim(\\tilde{g}_{jt})}. This can be used to test $H_0: \\beta = \\beta_0$, or to form a confidence region by taking all values of $\\beta$ for which the test fails to reject. Such an inference procedure is robust to identification problems in that it does not require an assumption about $D = D_\\beta \\Er[g_{jt}(\\beta,\\alpha_0,\\Phi_0)]$ being full rank or $Q_n$ being uniquely minimized at $\\beta_0$. If we are willing to assume strong identification in that $(D\u2019 \\Sigma(\\beta_0)^{-1} D)$ is invertible (and, loosely speaking, the estimated version of this is far from singular relative to its estimation error), then we can take an expansion to get \\sqrt{n}(\\hat{\\beta} - \\beta_0) = (D'\\Sigma^{-1} D)^{-1} D'\\Sigma^{-1} \\frac{1}{\\sqrt{n}} \\tilde{g}_{jt}(\\beta_0, \\alpha_0, \\Phi_0) + o_p(1) \\indist N(0, (D' \\Sigma^{-1} D)^{-1}). See my notes from 628 and 622 on extremum estimation and on bootstrap for more details.","title":"2-step estimators"},{"location":"dialysis-inference/#results","text":"First, we must load the data and prepare the data. (This repeats what was done in the assignment). using Pkg try using Dialysis # This assignment itself is in the \"Dialysis\" package. We will use some of the functions from it. catch Pkg.add(\"https://github.com/UBCECON567/Dialysis\") end docdir = normpath(joinpath(dirname(Base.pathof(Dialysis)), \"..\",\"docs\")) Pkg.activate(docdir) Activating environment at `~/.julia/dev/Dialysis/docs/Project.toml` Pkg.instantiate() using DataFrames dialysis = loaddata() sort!(dialysis, (:provfs, :year)) dialysis[!,:invest] = panellag(:stations, dialysis, :provfs, :year, -1) - dialysis[!,:stations]; dialysis[!,:labor] = (dialysis[!,:nurseFT] + 0.5*dialysis[!,:nursePT]+ dialysis[!,:ptcareFT] + 0.5*dialysis[!,:ptcarePT] + dialysis[!,:dieticiansFT] + 0.5*dialysis[!,:dieticiansPT] + dialysis[!,:social_workerFT] + 0.5*dialysis[!,:social_workerPT]) dialysis[!,:hiring] = panellag(:labor, dialysis, :provfs, :year, -1) - dialysis[!,:labor]; dialysis[!,:for_profit] = dialysis[!,:profit_status].==\"For Profit\" dialysis[!,:fresenius] = dialysis[!,:chain_name].==\"FRESENIUS\" dialysis[!,:davita] = dialysis[!,:chain_name].==\"DAVITA\"; using Statistics # for mean, std, and so on dialysis[!,:inspected_this_year] = ((dialysis[!,:days_since_inspection].>=0) .& (dialysis[!,:days_since_inspection].<365)) stateRates = by(dialysis, [:state, :year], df -> mean(skipmissing(df[!,:inspected_this_year]))) rename!(stateRates, :x1 => :state_inspection_rate) dialysis = join(dialysis, stateRates, on = [:state, :year]); dialysis[!,:city] = uppercase.(dialysis[!,:city]) comps = by(dialysis,[:city,:year], df -> mapreduce((x) -> ifelse(ismissing(x),0,1*(x>0)), +, df[!,:patient_months]) ) rename!(comps, :x1 => :competitors) dialysis = join(dialysis, comps, on = [:city,:year]); using FixedEffectModels dialysis[!,:idcat] = categorical(dialysis[!,:provfs]) qreg = reg(dialysis, @formula(pct_septic ~ days_since_inspection + patient_age + pct_female + patient_esrd_years + pct_fistula + comorbidities + hemoglobin), Vcov.cluster(:idcat),save=true) # saves residuals in augmentdf dialysis[!,:quality] = -qreg.augmentdf[!,:residuals]; log_infmiss = x->ifelse(!ismissing(x) && x>0, log(x), missing) # -Inf confuses reg() dialysis[!,:lpy] = log_infmiss.(dialysis[!,:patient_months]./12); dialysis[!,:logL] = log_infmiss.(dialysis[!,:labor]); dialysis[!,:logK] = log_infmiss.(dialysis[!,:stations]); Now, we re-run the first step of estimation. inc1 = ((dialysis[!,:patient_months] .> 0) .& (dialysis[!,:labor] .> 0) .& (dialysis[!,:stations] .> 0) .& .!ismissing.(dialysis[!,:quality]) .& .!ismissing.(dialysis[!,:std_mortality]) .& (dialysis[!,:invest].==0) .& (dialysis[!,:hiring].!=0)); inc1[ismissing.(inc1)] .= false; dialysis[!,:inc1] = inc1; dialysis[!,:lsmr] = log.(dialysis[!,:std_mortality] .+ .01) (\u03b1, \u03a6, \u03b1reg, eyqz)=partiallinearIV(:lpy, # y :quality, # q :lsmr, # z [:hiring, :logL, :logK, :state_inspection_rate, :competitors], # w dialysis[findall(dialysis[!,:inc1]),:]; npregress=(xp, xd,yd)->polyreg(xp,xd,yd,degree=1), parts=true ) # we will need these later in step 2 dialysis[!,:\u03a6] = similar(dialysis[!,:lpy]) dialysis[:,:\u03a6] .= missing rows = findall(dialysis[!,:inc1]) dialysis[rows,:\u03a6] = \u03a6 dialysis[!,:ey] = similar(dialysis[!,:lpy]) dialysis[:,:ey] .= missing dialysis[rows,:ey] = eyqz[:,1] dialysis[!,:eq] = similar(dialysis[!,:lpy]) dialysis[:,:eq] .= missing dialysis[rows,:eq] = eyqz[:,2] dialysis[!,:ez] = similar(dialysis[!,:lpy]) dialysis[:,:ez] .= missing dialysis[rows,:ez] = eyqz[:,3] \u03b1 -0.016731503377462428 The objective_gm function in Dialysis.jl constructs the $Q_n^{CUE}$ objective function described above. The following code minimizes it and plots a confidence region for $\\beta$ based on inverting the $\\chi^2$ test described above. using Optim (obj, momenti, cue, varfn) = objective_gm(:lpy, :logK, :logL, :quality, :\u03a6, :provfs, :year, [:logK, :logL], dialysis, clusterid=:idcat, npregress=(xp,xd,yd)->polyreg(xp,xd,yd,degree=1, deriv=true)); res = optimize(b->cue(b,\u03b1), # objective [0.0, 0.0], # lower bounds, should not be needed, but # may help numerical performance [1.0, 1.0], # upper bounds [0.4, 0.2], # initial value Fminbox(BFGS()), # algorithm autodiff=:forward) res * Status: success * Candidate solution Minimizer: [2.01e-20, 1.50e-01] Minimum: 2.340586e+00 * Found with Algorithm: Fminbox with BFGS Initial Point: [4.00e-01, 2.00e-01] * Convergence measures |x - x'| = 3.00e-17 \u2270 0.0e+00 |x - x'|/|x'| = 2.00e-16 \u2270 0.0e+00 |f(x) - f(x')| = 0.00e+00 \u2264 0.0e+00 |f(x) - f(x')|/|f(x')| = 0.00e+00 \u2264 0.0e+00 |g(x)| = 4.44e+00 \u2270 1.0e-08 * Work counters Seconds run: 187 (vs limit Inf) Iterations: 7 f(x) calls: 1273 \u2207f(x) calls: 1273 # calculate standard errors based on normal approximation \u03b2hat = res.minimizer gi = momenti(\u03b2hat,\u03b1) (\u03a3, n) = varfn(gi) using ForwardDiff using LinearAlgebra # for diag D = ForwardDiff.jacobian(\u03b2->mean(momenti(\u03b2,\u03b1), dims=1), \u03b2hat) V\u03b2 = inv(D'*inv(\u03a3)*D)/n @show \u03b2hat \u03b2hat = [2.0072100763983107e-20, 0.14987299477155056] @show sqrt.(diag(V\u03b2)) sqrt.(diag(V\u03b2)) = [1.4692926354291502, 0.05948171117514949] nothing; using Plots, Distributions Plots.gr() lb = [0., 0.] ub = [1., 1.] ntest = 200 \u03b2test = [rand(2).*(ub-lb) .+ lb for i in 1:ntest]; \u03b2test = vcat(\u03b2test'...) fn = \u03b2->cue(\u03b2,\u03b1) pval = Vector{Float64}(undef, ntest); qval = similar(pval); Threads.@threads for i in 1:size(\u03b2test,1) qval[i] = fn(\u03b2test[i,:]); pval[i] = cdf(Chisq(2),qval[i]); end crit = 0.9 fig=scatter(\u03b2test[:,1],\u03b2test[:,2], group=(pval.<crit), legend=false, markersize=4, markerstrokewidth=0.0, seriesalpha=1.0, palette=:isolum, xlabel=\"\u03b2k\", ylabel=\"\u03b2l\") ngrid = 20 b = range.(lb,ub, length=ngrid) pc = Matrix{Float64}(undef, length(b[1]), length(b[2])) qc = similar(pc) Threads.@threads for i in CartesianIndices(pc) qc[i] = fn([b[1][i[1]],b[2][i[2]]]); pc[i] = cdf(Chisq(2),qc[i]); end fig=contour!(b[1],b[2],pc', levels = [0.75, 0.9, 0.95, 0.99], contour_labels=true) display(fig) Here we plot $Q^{CUE}_n(\\beta)$. Plots.plotly() surface(b[1],b[2],qc', xlabel=\"\u03b2k\", ylabel=\"\u03b2l\") Plot{Plots.PlotlyBackend() n=1} Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. \u201cDouble/Debiased Machine Learning for Treatment and Structural Parameters.\u201d The Econometrics Journal 21 (1): C1\u2013C68. https://doi.org/10.1111/ectj.12097 . Newey, Whitney K., and Daniel McFadden. 1994. \u201cChapter 36 Large Sample Estimation and Hypothesis Testing.\u201d In, 4:2111\u20132245. Handbook of Econometrics. Elsevier. https://doi.org/https://doi.org/10.1016/S1573-4412(05)80005-4 .","title":"Results"},{"location":"dialysis/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License About this document \u00b6 This document was created using Weave.jl. The code is available in on github . The same document generates both static webpages and a jupyter notebook. \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} \\def\\inprob{\\,{\\buildrel p \\over \\rightarrow}\\,} \\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,} Introduction \u00b6 This assignment will reproduce some of the results of Grieco and McDevitt ( 2017 ). Getting started \u00b6 https://vse.syzygy.ca provides a convenient browser based interface to Julia. Open it and log in. You now need to upload the Jupyter notebook for this assignment and open it. You could download it from that link and then upload to vse.syzygy.ca. Alternatively, open a terminal in syzygy (File -> New -> Terminal). This will open a Linux shell in your browser. To download the notebook to syzygy enter wget https://ubcecon567.github.io/Dialysis/dialysis.ipynb Clicking on the folder icon near the top left of the screen opens a file browser panel. Use it to open the dialysis.ipynb notebook. I recommend renaming your copy of this notebook. You can rename files by right clicking in the file browser panel. Now open your copy of the notebook. Notebooks consist of a series of \u201ccells\u201d of either text written in markdown or Julia code. If you double click on any of the text cells, you can see the markdown that created it. To go back to the formatted text, execute the cell by either clicking the play icon on the top of the page or typing ctrl and enter together. Julia resources \u00b6 This assignment will try to explain aspects of Julia as needed. However, if at some point you feel lost, you may want to consult some of the following resources. Reading the first few sections of either QuantEcon or Think Julia is recommended. Resources \u00b6 QuantEcon with Julia Think Julia A detailed introduction to Julia and programming more generally. Long, but recommended, especially if you\u2019re new to programming. From the julia prompt, you can access documentation with ?functionname . Some packages have better documentation than others. https://julialang.org/ is the website for Julia Documentation for core Julia can be found at https://docs.julialang.org/en/v1/ . All Julia packages also have a github page. Many of these include package specific documentation. Notes on Julia from ECON 622 much of this is part of QuantEcon, but not all The Julia Express short book with examples of Julia usage Part I: Loading and exploring the data \u00b6 Loading packages \u00b6 Like many programming environments (R, Python, etc), Julia relies on packages for lots of its functionality.The following code will download and install all the packages required for this assignment (but the packages will still need to be loaded with using ... ). Execute this cell. It will take some time. While the cell is running, there will be a [*] to the left of it. This will change to [1] (or some other number) after the cell is finished running. The number indicates the order in which the cell was executed. You can execute cells out of order. This can be useful during development, but you should always make sure that your notebook works correctly when cells are executed in order before considering it complete (that is, make sure the \u201cRun -> Restart Kernel and Run all Cells\u201d menu option produces the output you want). Don\u2019t worry about understanding the details of the code in this section. using Pkg try using Dialysis # This assignment itself is in the \"Dialysis\" package. We will use some of the functions from it. catch Pkg.update() Pkg.add(PackageSpec(url=\"https://github.com/UBCECON567/Dialysis\")) using Dialysis end docdir = normpath(joinpath(dirname(Base.pathof(Dialysis)), \"..\",\"docs\")) Pkg.activate(docdir) Activating environment at `~/.julia/dev/Dialysis/docs/Project.toml` Pkg.instantiate() Load the data \u00b6 Now let\u2019s get to work. I originally downloaded the data for this problem set from https://dialysisdata.org/content/dialysis-facility-report-data . As in Grieco and McDevitt ( 2017 ) the data comes from Dialysis Facility Reports (DFRs) created under contract to the Centers for Medicare and Medicaid Services (CMS). However, there are some differences. Most notably, this data covers 2006-2014, instead of 2004-2008 as in Grieco and McDevitt ( 2017 ) . The R script downloadDialysisData.R downloads, combines, and cleans the data. Unfortunately, dialysisdata.org has reorganized their website, and the data no longer seems to be available. Similar (likely identical) data is available from https://data.cms.gov/browse?q=dialysis . It might be useful to look at the documentation included with any of the \u201cDialysis Facility Report Data for FY20XX\u201d zip files. Anyway, the result of the R script is the dialysisFacilityReports.rda file contained in the git repository for this assignment. This R data file contains most of the variables used by Grieco and McDevitt ( 2017 ). using DataFrames # DataFrames.jl is a package for storing and # interacting with datasets dialysis = loaddata() # loaddata() is a function I wrote that is part # of Dialysis.jl. It returns a DataFrame typeof(dialysis) DataFrame We will begin our analysis with some exploratory statistics and figures. There are at least two reasons for this. First, we want to check for any anomalies in the data, which may indicate an error in our code, our understanding of the data, or the data itself. Second, we should try to see if there are any striking patterns in the data that deserve extra attention. We can get some information about all the variables in the data as follows describe(dialysis) 58\u00d78 DataFrame. Omitted printing of 1 columns \u2502 Row \u2502 variable \u2502 mean \u2502 min \u2502 median \u2502 max \u2502 nunique \u2502 nmissing \u2502 \u2502 \u2502 Symbol \u2502 Union\u2026 \u2502 Any \u2502 Union\u2026 \u2502 Any \u2502 Union\u2026 \u2502 Union\u2026 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 1 \u2502 provfs \u2502 \u2502 =\"012306\" \u2502 \u2502 =\"68256 7\" \u2502 7015 \u2502 \u2502 \u2502 2 \u2502 year \u2502 2010.23 \u2502 2006.0 \u2502 2010.0 \u2502 2014.0 \u2502 \u2502 \u2502 \u2502 3 \u2502 comorbidities \u2502 4.70985 \u2502 0.44444 \u2502 4.69811 \u2502 10.7619 \u2502 \u2502 7982 \u2502 \u2502 4 \u2502 comorbidities_p3 \u2502 4.70985 \u2502 0.44444 \u2502 4.69811 \u2502 10.7619 \u2502 \u2502 7982 \u2502 \u2502 5 \u2502 hemoglobin \u2502 9.77373 \u2502 3.6 \u2502 9.7727 \u2502 18.0 \u2502 \u2502 1608 \u2502 \u2502 6 \u2502 hemoglobin_p3 \u2502 9.77326 \u2502 3.6 \u2502 9.7715 \u2502 18.0 \u2502 \u2502 1608 \u2502 \u2502 7 \u2502 std_mortality \u2502 1.00545 \u2502 0.0 \u2502 0.96694 \u2502 4.97293 \u2502 \u2502 2239 \u2502 \u22ee \u2502 51 \u2502 patient_esrd_years \u2502 4.4062 \u2502 0.3121 \u2502 4.3491 \u2502 19.9425 \u2502 \u2502 131 \u2502 \u2502 52 \u2502 treatment_type \u2502 \u2502 Hemodialysis \u2502 \u2502 Unavail able \u2502 4 \u2502 \u2502 \u2502 53 \u2502 inspect_date \u2502 \u2502 1992-05-20 \u2502 \u2502 2015-05 -29 \u2502 3273 \u2502 329 \u2502 \u2502 54 \u2502 inspect_result \u2502 \u2502 . \u2502 \u2502 Unknown \u2502 12 \u2502 2011 \u2502 \u2502 55 \u2502 inspect_cfc_cites \u2502 0.293753 \u2502 0.0 \u2502 0.0 \u2502 10.0 \u2502 \u2502 3918 \u2502 \u2502 56 \u2502 inspect_std_cites \u2502 5.89393 \u2502 0.0 \u2502 4.0 \u2502 153.0 \u2502 \u2502 3921 \u2502 \u2502 57 \u2502 days_since_inspection \u2502 704.886 \u2502 -177.0 \u2502 557.0 \u2502 7454.0 \u2502 \u2502 329 \u2502 \u2502 58 \u2502 original_chain_name \u2502 \u2502 \u2502 \u2502 WELLSPA N DIALYSIS \u2502 189 \u2502 \u2502 The meaning of these variables are as follows: Variable Definition provfs provider identifier year year comorbidities average patient comorbidities hemoglobin average patient hemoglobin level std_mortality standardized mortality ratio std_hosp_days standardized hospitalization days std_hosp_admit standardized hospitalization admittance rate pct_septic percent of patients hospitalized due to septic infection n_hosp_admit number of hospitalizations n_hosp_patients patient_years_hd patient years at risk of hospitalization city city name provider name state state chain_name name of chain if provider is part of one profit_status whether for profit stations number of dialysis stations total_staff total staff dieticiansFT full-time renal dieticians dieticiansPT part-time renal dieticians nurseFT full-time nurses (>32 hours/week) nursePT part-time nurses (\\<32 hours/week) ptcareFT full-time patient care technicians ptcarePT part-time patient care technicians social_workerFT full-time social workers social_workerPT part-time social workers patient_months number of patient-months treated during the year patient_years_rom patient-years at risk of mortality pct_fistula the percentage of patient months in which the patient received dialysis through arteriovenous (AV) fistulae pct_female percent of female patients patient_age average age of patients patient_esrd_years average number of years patients have had end stage renal disease treatment_type types of treatment provided at facility inspect_date date of most recent inspection inspect_result result of most recent inspection inspect_cfc_cites number of condition for coverage deficiencies in most recent inspection inspect_std_cites number of standard deficiencies in most recent inspection days_since_inspection days since last inspection The raw data contains information on many variables in each of the previous 4 years. Staffing variables with no suffix are staff as of January 31, year as reported in year + 1. Staffing variables with \u201c.l1\u201d are staff as of January 31, year - 1 as reported in year + 1. If there were no reporting errors, the .l1 variables would equal the lag of the ones without .l1. However, you might find that this is not the case. As explained in downloadDialysisData.R , data collected in year Y has information on most variables in years Y-1, Y-2, Y-3, and Y-4. However, for some variables and survey years, only information in years Y-2, Y-3, Y-4 is included. For such variables, at year Y-1, I use the value reported in survey year Y if it is available. If not, I use the value reported in survey year Y+1. The variables ending with \u201c.p3\u201d instead use the convention to use use Y-2 values if available and the Y-1 ones if not. Again, if there were no reporting errors the variables with and without .p3 would be the same. There are three variables for the number of patients treated. The data documentation describes patient_months as \u201cPrevalent Hemodialysis Patient Months (7a): The monthly prevalent hemodialysis patient count at a facility includes all non-transient patients (home and in-center) who receive hemodialysis as of the last day of that calendar month. Incident patients (those who received ESRD treatment for the first time ever) are included in this count. Row 7a reports the number of prevalent hemodialysis patient months reported at the facility each year. The number of patient months over a time period is the sum of patients reported for the months covered by the time period. An individual patient may contribute up to 12 patient months per year.\u201d patient_years_rom is the number of patient years at risk of mortality. patient_years_hd is number of patient years at risk of hospitalization. Since hospitalization data is constructed from Medicare records, a patient is considered at risk of hospitalization only when one can be reasonably certain that a hospitalization would be billed to Medicare. Dialysis patients who pay for for hospitalization with other methods could have unobserved hospitalizations. The data guide explains, \u201cIdeally, this table includes only patients whose Medicare billing records include all hospitalizations for the period. To achieve this goal, we require that patients reach a certain level of Medicare-paid dialysis bills to be included in hospitalization statistics, or that patients have Medicare-paid inpatient claims during the period. For the purpose of analysis, each patient\u2019s follow-up time is broken into periods defined by time since dialysis initiation. For each patient, months within a given period are included if that month in the period is considered \u2018eligible\u2019; a month is deemed eligible if it is within two months of a month having at least \\$900 of Medicare-paid dialysis claims or at least one Medicare-paid inpatient claim. In setting this criterion, our aim is to achieve completeness of information on hospitalizations for all patients included in the years at risk.\u201d Create some variables \u00b6 Not all variables used Grieco and McDevitt ( 2017 ) are included here. Some variables will need to be transformed to be comparable to what is in the paper. For example, net investment in stations in year $t$ is the difference between the number of stations in year $t+1$ and year in $t$. # sort data by :provfs, :year # function names that end with ! indicate that the function will # modify one (or more) of its inputs. In this case, sort! modifies the # dialysis DataFrame sort!(dialysis, (:provfs, :year)) # things starting with : are Symbols. Names of variables within a # DataFrame must be Symbols, so they all start with : # we can access a single column of DataFrame by writing # dialysis[!,:stations] . This will be a 1 dimensional Array containing # of length equal to the number of rows in the dialysis DataFrame # panellag is a function defined in Dialysis.jl it creates lags and # leads of variables in panel data. It will insert missing values # where appropriate. # putting dialysis[!,:invest] on the left will create a new column in # the dialysis dataframe dialysis[!,:invest] = panellag(:stations, dialysis, :provfs, :year, -1) - dialysis[!,:stations]; # ; prevents the notebook from printing the # output of the last command. Otherwise, notebooks display the output # of the last command in each cell. We can also create labor and hiring. Note that the choices of giving 0.5 weight to part-time workers, including social workers, and weighting all types of staff equally are all somewhat arbitrary and may not agree exactly with what Grieco and McDevitt ( 2017 ) did. dialysis[!,:labor] = (dialysis[!,:nurseFT] + 0.5*dialysis[!,:nursePT]+ dialysis[!,:ptcareFT] + 0.5*dialysis[!,:ptcarePT] + dialysis[!,:dieticiansFT] + 0.5*dialysis[!,:dieticiansPT] + dialysis[!,:social_workerFT] + 0.5*dialysis[!,:social_workerPT]) dialysis[!,:hiring] = panellag(:labor, dialysis, :provfs, :year, -1) - dialysis[!,:labor]; Creating for profit and chain indicators. # create a Boolean for profit indicator dialysis[!,:for_profit] = dialysis[!,:profit_status].==\"For Profit\" # The dot in .== is an example of broadcasting. It's very common to # want to apply the same function to all elements of an # array. Broadcasting does exactly this. If A is an array, and f() is # a function that operates on scalars, then f.(A) will apply f to each # element of A and return an array of the results. The above .== # compares each element of the array dialysis[!,:profit_status] to the # scalar string \"For Profit\" # similarly create indicators for the two largest chains dialysis[!,:fresenius] = dialysis[!,:chain_name].==\"FRESENIUS\" dialysis[!,:davita] = dialysis[!,:chain_name].==\"DAVITA\"; State inspection rates are a bit more complicated to create. using Statistics # for mean, std, and so on # first make an indicator for inspected in the last year dialysis[!,:inspected_this_year] = ((dialysis[!,:days_since_inspection].>=0) .& (dialysis[!,:days_since_inspection].<365)) # then take the mean by state stateRates = by(dialysis, [:state, :year], # by(data, :var, f) will apply f to each group of data # with a different value of :var df -> mean(skipmissing(df[!,:inspected_this_year]))) # df -> mean(skipmissing(df[!,:inspected_this_year])) is a shorthand way # to define a function it's equalivant to # # function f(df) # mean(skipmissing(df[!,:inspected_this_year])) # end # # skipmissing skips missing values inside a DataFrame. Most arithmetic # functions will not do what you want if missing values are included. # rename the variable in the stateRates DataFrame rename!(stateRates, :x1 => :state_inspection_rate) # merge the stateRates with the dialysis data dialysis = join(dialysis, stateRates, on = [:state, :year]); Creating the number of competitors in the same city is somewhat similar. Note that Grieco and McDevitt ( 2017 ) use the number of competitors in the same HSA, which would be preferrable. However, this dataset does not contain information on HSAs. If you are feeling ambitious, you could try to find data linking city, state to HSA, and use that to calculate competitors in the same HSA. dialysis[!,:city] = uppercase.(dialysis[!,:city]) comps = by(dialysis,[:city,:year], df -> mapreduce((x) -> ifelse(ismissing(x),0,1*(x>0)), +, df[!,:patient_months]) ) rename!(comps, :x1 => :competitors) dialysis = join(dialysis, comps, on = [:city,:year]); Problem 1: Summary statistics \u00b6 Creata a table (or multiple tables) similar to Tables 1-3 of Grieco and McDevitt ( 2017 ). Comment on any notable differences. The following code will help you get started. using Statistics # at the very least, you will need to change this list vars = [:patient_years_rom, :labor, :hiring] # You shouldn't neeed to change this function, but you can if you want function summaryTable(df, vars; funcs=[mean, std, x->length(collect(x))], colnames=[:Variable, :Mean, :StDev, :N]) # In case you want to search for information about the syntax used here, # [XXX for XXX] is called a comprehension # The ... is called the splat operator DataFrame([vars [[f(skipmissing(df[!,v])) for v in vars] for f in funcs]...], colnames) end summaryTable(dialysis, vars) 3\u00d74 DataFrame \u2502 Row \u2502 Variable \u2502 Mean \u2502 StDev \u2502 N \u2502 \u2502 \u2502 Any \u2502 Any \u2502 Any \u2502 Any \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 1 \u2502 patient_years_rom \u2502 67.1459 \u2502 45.6089 \u2502 48224 \u2502 \u2502 2 \u2502 labor \u2502 13.1744 \u2502 8.34365 \u2502 42512 \u2502 \u2502 3 \u2502 hiring \u2502 0.422357 \u2502 4.30587 \u2502 34665 \u2502 Problem 2: exploratory figures \u00b6 Create some figures to explore the data. Try to be creative. Are there any strange patterns or other obvious problems with the data? Here are some examples to get started. You may want to look at the StatPlots.jl, Plots.jl, or VegaLite.jl github pages for more examples. Comparing output measures \u00b6 using StatsPlots , Plots Plots.pyplot() dialysis[!,:patient_years] = dialysis[!,:patient_months]/12 # missings will mess up corrplot vars = [:patient_years, :patient_years_hd, :patient_years_rom] inc = completecases(dialysis[:,vars]) @df dialysis[inc,vars] corrplot(cols(vars)) Trends over time \u00b6 function yearPlot(var) data = dialysis[completecases(dialysis[:,[:year, var]]),:] scatter(data[!,:year], data[!,var], alpha=0.1, legend=:none, markersize=3, markerstrokewidth=0.0) yearmeans = by(data, :year, mean = var => x->mean(skipmissing(x)), q01 = var => x->quantile(skipmissing(x), 0.01), q10 = var => x->quantile(skipmissing(x), 0.1), q25 = var => x->quantile(skipmissing(x), 0.25), q50 = var => x->quantile(skipmissing(x), 0.50), q75 = var => x->quantile(skipmissing(x), 0.75), q90 = var => x->quantile(skipmissing(x), 0.9), q99 = var => x->quantile(skipmissing(x), 0.99)) @df yearmeans plot!(:year, :mean, colour = ^(:black), linewidth=4) @df yearmeans plot!(:year, cols(3:ncol(yearmeans)), colour = ^(:red), alpha=0.4, legend=:none, xlabel=\"year\", ylabel=String(var)) end yearPlot(:labor) The above plot shows a scatter of labor vs year. The black lines are average labor each year. The red lines are the 0.01, 0.1, 0.25, 0.5, 0.75, 0.9, and 0.99 quantiles conditional on year. Part II: estimating the model \u00b6 Problem 3: Quality measures \u00b6 Grieco and McDevitt ( 2017 ) use the residuals from regressing the infection rate on patient characteristics as a measure of quality. Since the infection rate is a noisy measure of quality, they instrument with the standardized mortality ratio as a second measure of quality. Medicare collects the data we are using in part to create the \u201cDialysis Facility Compare\u201d website, which is meant to allow consumers to compare quality of dialysis facilities. Browsing around the Dialysis Facility Compare or by looking at the first few pages of a sample Dialysis Facility Report , you will see that there are a number of other variables that Medicare considers indicators of quality. Pick one of these (it may or may not be included in the extract of data I provided), and argue for or against using it instead of or in addition to the septic infection rate and standardized mortality ratio. We can construct residuals from an OLS regression as follows: \"\"\" ols_residuals(data::AbstractDataFrame, y::Symbol, x::Array{Symbol,1}; intecept::Bool=true) This is a doc string. After executing this cell, if you type ?ols_residuals, you will see this text. Calculate residuals from an OLS regression of data[y] on data[x] Inputs: - `data` DataFrame containg y and x - `y` Symbol specifying y variable - `x` Symbol specifying x variables Output: - Vector of residuals of length = nrow(data) \"\"\" function ols_residuals(data::DataFrame, y::Symbol, x::Array{Symbol,1}; # arguments following the # are optional intercept::Bool=true ) # The :: are type specifications. They could be left out, and this # function would still work just fine. One of their purposes are to # document what inputs this function expects, and throw useful error # messages if someone tries calling the function on the wrong types. inc = completecases(data[:,[y, x...]]) # deal with missing Y = disallowmissing(data[inc,y]) if (intercept) X = [ones(sum(inc)) data[inc,x]] else X = data[inc,x] end X = disallowmissing(convert(Matrix, X)) # you can type Greek and some other LaTeX characters by typing their LaTeX # code followed by tab, e.g. \\beta<TAB> and \\in<TAB> \u03b2 = X \\ Y # \u03b2 \u2208 argmin_b || X*b - Y || \u03f5 = Y - X*\u03b2 if (any(.!inc)) # add back in missings resid = Array{Union{Missing,eltype(\u03f5)},1}(undef, nrow(data)) resid .= missing resid[inc] = \u03f5 return(resid) else # no missing, just return \u03f5 return(\u03f5) end end q = -ols_residuals(dialysis, :pct_septic, [:days_since_inspection, :patient_age, :pct_female, :patient_esrd_years, :pct_fistula, :comorbidities, :hemoglobin]); Of course, regression is common enough that there are already Julia packages for it. I included the ols_residuals only for pedagogical purposes. Whenever there exists a well-used package, it is (usually) better to use the package than try to write your own functions. Here\u2019s how to accomplish the same thing using FixedEffectModels.jl. using FixedEffectModels dialysis[!,:idcat] = categorical(dialysis[!,:provfs]) # FixedEffectModels requires clustering and fixed effect variables to # be categorical qreg = reg(dialysis, @formula(pct_septic ~ days_since_inspection + patient_age + pct_female + patient_esrd_years + pct_fistula + comorbidities + hemoglobin), Vcov.cluster(:idcat),save=true) # saves residuals in augmentdf dialysis[!,:quality] = -qreg.augmentdf[!,:residuals] # Let's test that these results are the same from ols_residuals println(\"Mean absolute difference = $(mean(skipmissing( abs.(q.- dialysis[!,:quality]) )))\") Mean absolute difference = 5.453941239030147e-14 # using $(expr) in a string will insert the result of expr in the$ # string using Test @test all(skipmissing(q .\u2248 dialysis[!,:quality])) == true Test Passed Testing is an important part of software development. The Test.jl package provides help function for running tests. See these notes from 622 for more information about testing. Problem 4: OLS and fixed effects estimates \u00b6 Reproduce columns 2,3, 5, and 6 of Table 5. The syntax for fixed effects regression is shown below: # you may want to use patient_years_hd or patient_years_rom instead log_infmiss = x->ifelse(!ismissing(x) && x>0, log(x), missing) # -Inf confuses reg() dialysis[!,:lpy] = log_infmiss.(dialysis[!,:patient_months]./12) dialysis[!,:logL] = log_infmiss.(dialysis[!,:labor]) dialysis[!,:logK] = log_infmiss.(dialysis[!,:stations]) # you may want to restrict sample to match sample that can be used in model estimates reg(dialysis, @formula(lpy ~ quality + logK + logL + fe(idcat)), Vcov.cluster(:idcat)) Fixed Effect Model ========================================================================= Number of obs: 32013 Degrees of freedom: 4 R2: 0.882 R2 Adjusted: 0.882 F Statistic: 312.361 p-value: 0.000 R2 within: 0.144 Iterations: 1 Converged: true ========================================================================= Estimate Std.Error t value Pr(>|t|) Lower 95% Upper 95% ------------------------------------------------------------------------- quality -0.00921389 0.000589458 -15.6311 0.000 -0.0103692 -0.00805853 logK 0.171904 0.028534 6.02452 0.000 0.115976 0.227831 logL 0.470537 0.0171159 27.4912 0.000 0.43699 0.504085 ========================================================================= Be sure to add the other columns. If you\u2019d like, you could use RegressionTables.jl to produce tables that look a lot like the ones in the paper. Estimation of $\\alpha$ \u00b6 As discussed in section 5 of Grieco and McDevitt ( 2017 ), the coefficient on quality, $\\alpha$, is estimated from y_{jt} = \\alpha q_{jt} + \\Phi(\\underbrace{h_{jt}, k_{jt}, l_{jt}, x_{jt}}_{w_{jt}}) + \\epsilon_{jt} with a second noisy measure of quality, $z_{jt}$, used to instrument for $q_{jt}$. To estimate $\\alpha$, first the exogenous variables, $w$, can be partialed out to give: y_{jt} - \\Er[y_{jt} | w_{jt} ] = \\alpha (q_{jt} - \\Er[q_{jt}|w_{jt}]) + \\epsilon_{jt} where we used the assumption that $\\Er[\\epsilon_{jt} | w_{jt} ] = 0$ and the fact that $\\Er[\\Phi(w) | w] = \\Phi(w)$. Under the assumption that $\\Er[\\epsilon| z, w] = 0$, we can estimate $\\alpha$ based on the moment condition: \\begin{align*} 0 = & \\Er[\\epsilon f(z,w) ] \\\\ 0 = & \\Er\\left[ \\left(y_{jt} - \\Er[y_{jt} | w_{jt} ] - \\alpha (q_{jt} - \\Er[q_{jt}|w_{jt}])\\right) f(z_{jt},w_{jt}) \\right] \\end{align*} If $Var(\\epsilon|z,w)$ is constant, the efficient choice of $f(z,w)$ is \\Er[\\frac{\\partial \\epsilon}{\\partial \\alpha} |z, w ] = \\Er[q| z, w] - \\Er[q|w] To estimate $\\alpha$, we simply replace these conditional expectations with regression estimates, and replace the unconditional expectation with a sample average. Let $\\hat{\\Er}[y|w]$ denote a nonparmetric estimate of the regression of $y$ on $w$. Then, \\hat{\\alpha} = \\frac{\\sum_{j,t} (y_{jt} - \\hat{E}[y|w_{jt}])(\\hat{E}[q|z_{jt},w_{jt}] - \\hat{E}[q|w_{jt}])} {\\sum_{j,t} (q_{jt} - \\hat{E}[q|w_{jt}])(\\hat{E}[q|z_{jt},w_{jt}] - \\hat{E}[q|w_{jt}])} The function partiallinearIV in Dialysis.jl will estimate this model. Also included are two methods for estimating $\\hat{E}[y|w]$. polyreg estimates a polynomial series regression, that is it regresses $y$ on a polynomial of degree $d$ in $w$. To allow the regression to approximate any function, the degree must increase with the sample size, but to control variance, the degree must not increase too quickly. We will not worry too much about the choice of degree here. An alternative method (and what Grieco and McDevitt ( 2017 ) used) is local linear regression. To estimate $\\hat{E}[y|x_{jt}]$, local linear regression estimates a linear regression of $y$ on $x$, but weights observations by how close $x_{it}$ is to $x_{jt}$. That is, \\hat{E}[y|x_{jt}] = x_{jt} \\hat{\\beta}(x_jt) where \\hat{\\beta}(x_jt) = \\argmin_\\beta \\sum_{i,t} (y_{it} - x_{it}\\beta)^2 k((x_{it} - x_{jt})/h_n) Here $k()$ is some function with its maximum at 0 (and has some other properties), like $k(x) \\propto e^{-x^2}$. The bandwidth, $h_n$, determines how much weight to place on observations close to vs far from $x_{jt}$. Similar to the degree in polynomial regression, the bandwidth must decrease toward 0 with sample size allow local linear regression to approximate any function, but to control variance the bandwidth must not decrease too quickly. We will not worry too much about the choice of bandwidth. Anyway, the function locallinear in Dialysis.jl estimates a local linear regression. Problem 5: $\\alpha$ \u00b6 Estimate $\\alpha$ using the following code. You may want to modify some aspects of it and/or report estimates of $\\alpha$ for different choices of instrument, nonparametric estimation method, degree or bandwidth. Compare your estimate(s) of $\\alpha$ with the ones in Tables 5 and 6 of Grieco and McDevitt ( 2017 ). # create indicator for observations usable in estimation of \u03b1 inc1 = ((dialysis[!,:patient_months] .> 0) .& (dialysis[!,:labor] .> 0) .& (dialysis[!,:stations] .> 0) .& .!ismissing.(dialysis[!,:quality]) .& .!ismissing.(dialysis[!,:std_mortality]) .& (dialysis[!,:invest].==0) .& (dialysis[!,:hiring].!=0)); inc1[ismissing.(inc1)] .= false; dialysis[!,:inc1] = inc1; dialysis[!,:lsmr] = log.(dialysis[!,:std_mortality] .+ .01) # As degree \u2192 \u221e and/or bandwidth \u2192 0, whether we use :std_mortality or # some transformation as the instrument should not matter. However, # for fixed degree or bandwidth it will have some (hopefully small) # impact. (\u03b1, \u03a6, \u03b1reg, eyqz)=partiallinearIV(:lpy, # y :quality, # q :lsmr, # z [:hiring, :logL, :logK, :state_inspection_rate, :competitors], # w dialysis[findall(dialysis[!,:inc1]),:]; npregress=(xp, xd,yd)->polyreg(xp,xd,yd,degree=1), parts=true # You may want to change the degree here # # You could also change `polyreg` to # `locallinear` and `degree` to # `bandwidthmultiplier` # # locallinear will likely take some time to # compute (\u2248350 seconds on my computer) ) # we will need these later in step 2 dialysis[!,:\u03a6] = similar(dialysis[!,:lpy]) dialysis[:,:\u03a6] .= missing rows = findall(dialysis[!,:inc1]) dialysis[rows,:\u03a6] = \u03a6 dialysis[!,:ey] = similar(dialysis[!,:lpy]) dialysis[:,:ey] .= missing dialysis[rows,:ey] = eyqz[:,1] dialysis[!,:eq] = similar(dialysis[!,:lpy]) dialysis[:,:eq] .= missing dialysis[rows,:eq] = eyqz[:,2] dialysis[!,:ez] = similar(dialysis[!,:lpy]) dialysis[:,:ez] .= missing dialysis[rows,:ez] = eyqz[:,3] \u03b1 -0.016731503377462428 Brief introduction to GMM \u00b6 The coefficients on labor and capital are estimated by GMM. The idea of GMM is as follows. We have a model that implies \\Er[c(y,x;\\theta) | z ] = 0 where $y$, $x$, and $z$ are observed variables. $c(y,x;\\theta)$ is some known function of the data and some parameters we want to estimate, $\\theta$. Often, $c(y,x;\\theta)$ are the residuals from some equation. For example, for linear IV, we\u2019d have c(y,x;\\theta) = y - x\\theta The conditional moment restriction above implies that \\Er[c(y,x;\\theta)f(z) ] = 0 for any function $f()$. We can then estimate $\\theta$ by replacing the population expectation with a sample average and finding $\\hat{\\theta}$ such that \\En[c(y,x;\\hat{\\theta})f(z) ] \\approx 0 The dimension of $f(z)$ should be greater than or equal to the dimension of $\\theta$, so we have at least as many equations as unknowns. We find this $\\hat{\\theta}$ by minimizing a quadratic form of these equations. That is, \\hat{\\theta} = \\argmin_\\theta \\En[g_i(\\theta)] W_n \\En[g_i(\\theta)]' were $g_i(\\theta) = c(y_i, x_i;\\theta)f(z_i)$, and $W_n$ is some positive definite weighting matrix. Problem 6: OLS by GMM \u00b6 As practice with GMM, use it to estimate a simple regression model, y = x\\beta + \\epsilon assuming $\\Er[\\epsilon|x] = 0$. Test your code on simulated data. The following will help you get started. function sim_ols(n; \u03b2 = ones(3)) x = randn(n, length(\u03b2)) \u03f5 = randn(n) y = x*\u03b2 + \u03f5 return(x,y) end \u03b2 = ones(2) (x, y) = sim_ols(100; \u03b2=\u03b2) \u03b2ols = (x'*x) \\ (x'*y) function gmm_objective(\u03b2) gi = (y - x*\u03b2) .* x Egi = mean(gi, dims=1) error(\"This is incomplete; you must finish it\") # It is is likely that the code you will write will return a 1 x 1, # 2 dimensional array. For compatibility with Optim, you need to # return a scalar. If foo is a 1x1 array, write `foo[1]` to return a scalar instead of # 1x1 array end # minimizer gmm_objective using Optim # github page : https://github.com/JuliaNLSolvers/Optim.jl # docs : http://julianlsolvers.github.io/Optim.jl/stable/ try res = optimize(gmm_objective, zeros(size(\u03b2)), # initial value BFGS(), # algorithm, see http://julianlsolvers.github.io/Optim.jl/stable/ autodiff=:forward) \u03b2gmm = res.minimizer catch err @info err \u03b2gmm = \u03b2ols res = nothing end @test \u03b2gmm \u2248 \u03b2ols Error During Test at none:1 Test threw exception Expression: \u03b2gmm \u2248 \u03b2ols UndefVarError: \u03b2gmm not defined Stacktrace: [1] top-level scope at /home/paul/.julia/dev/Dialysis/docs/makeweave.jl: 10 [2] eval at ./boot.jl:330 [inlined] [3] capture_output(::Expr, ::Module, ::Bool, ::Bool, ::Bool, ::Bool) at /home/paul/.julia/packages/Weave/UOxmI/src/run.jl:230 [4] run_code(::Weave.CodeChunk, ::Weave.Report, ::Module) at /home/paul/ .julia/packages/Weave/UOxmI/src/run.jl:208 [5] eval_chunk(::Weave.CodeChunk, ::Weave.Report, ::Module) at /home/pau l/.julia/packages/Weave/UOxmI/src/run.jl:289 [6] run_chunk(::Weave.CodeChunk, ::Weave.WeaveDoc, ::Weave.Report, ::Mod ule) at /home/paul/.julia/packages/Weave/UOxmI/src/run.jl:130 [7] #run#34(::String, ::Module, ::String, ::Dict{String,Bool}, ::String, ::Nothing, ::String, ::Symbol, ::Bool, ::typeof(run), ::Weave.WeaveDoc) at /home/paul/.julia/packages/Weave/UOxmI/src/run.jl:94 [8] (::Base.var\"#kw##run\")(::NamedTuple{(:doctype, :mod, :out_path, :arg s, :fig_path, :fig_ext, :cache_path, :cache, :throw_errors),Tuple{String,Mo dule,String,Dict{String,Bool},String,Nothing,String,Symbol,Bool}}, ::typeof (run), ::Weave.WeaveDoc) at ./none:0 [9] #weave#16(::String, ::Symbol, ::String, ::Dict{String,Bool}, ::Modul e, ::String, ::Nothing, ::String, ::Symbol, ::Bool, ::Nothing, ::Nothing, : :Nothing, ::Array{String,1}, ::String, ::typeof(weave), ::String) at /home/ paul/.julia/packages/Weave/UOxmI/src/Weave.jl:121 [10] (::Weave.var\"#kw##weave\")(::NamedTuple{(:out_path, :cache, :cache_p ath, :doctype, :mod, :args),Tuple{String,Symbol,String,String,Module,Dict{S tring,Bool}}}, ::typeof(weave), ::String) at ./none:0 [11] top-level scope at /home/paul/.julia/dev/Dialysis/docs/makeweave.jl :13 Error: Test.FallbackTestSetException(\"There was an error during testing\") res Error: UndefVarError: res not defined Estimating $\\beta$ \u00b6 The model implies that \\omega_{jt} = \\Phi(w_{jt}) - \\beta_k k_{jt} - \\beta_l l_{jt} and y_{jt} - \\alpha q_{jt} - \\beta_k k_{jt} - \\beta_l l_{jt} = g(\\omega_{jt-1}) + \\eta_{jt} {#eq:eta} The timing and exogeneity assumptions imply that \\Er[\\eta_{jt} | k_{jt}, l_{jt}, w_{jt-1}] Given a value of $\\beta$, and our above estimates of $\\Phi$ and $\\alpha$, we can compute $\\omega$ from the equation above, and then estimate $g()$ and $\\eta$ by a nonparametric regression of $y_{jt} - \\alpha q_{jt} - \\beta_k k_{jt} - \\beta_l l_{jt}$ on $\\omega_{jt-1}$. $\\beta$ can then be estimated by finding the value of $\\beta$ that comes closest to satisfying the moment condition \\Er[\\eta(\\beta)_{jt} k_{jt}] = 0 \\text{ and } \\Er[\\eta(\\beta)_{jt} l_{jt}] = 0 To do this, we minimize Q_n(\\beta) = \\left( \\frac{1}{N} \\sum_{j,t} \\eta(\\beta)_{jt} (k_{jt}, l_{jt}) \\right) W_n \\left( \\frac{1}{N} \\sum_{j,t} \\eta(\\beta)_{jt} (k_{jt}, l_{jt}) \\right)' Problem 7: estimate $\\beta$ \u00b6 Write the body of the $Q_n(\\beta)$ function below. Use it to estimate $\\beta$. Compare your results with those of Grieco and McDevitt ( 2017 ). Optionally, explore robustness of your results to changes in the specification. # indicator for observations usable in estimation of \u03b2 dialysis[!,:inclag] = panellag(:inc1, dialysis, :provfs, :year, 1); dialysis[!,:inc2] = (dialysis[!,:inclag] .& (dialysis[!,:stations].>0) .& (dialysis[!,:labor].>0) .& (dialysis[!,:patient_years].>0) .& .!ismissing.(dialysis[!,:quality])); dialysis[ismissing.(dialysis[!,:inc2]),:inc2] .= false; (\u03c9func, \u03b7func) = errors_gm(:lpy, :logK, :logL, :quality, :\u03a6, :provfs, :year, dialysis, \u03b1; degree=1) function Qn(\u03b2) \u03b7 = \u03b7func(\u03b2) error(\"You must write the body of this function\") end using Optim try res = optimize(Qn, # objective [0.0, 0.0], # lower bounds, should not be needed, but # may help numerical performance [1.0, 1.0], # upper bounds [0.4, 0.2], # initial value Fminbox(BFGS()), # algorithm autodiff=:forward) \u03b2 = res.minimizer @show \u03b2 catch err @info err res = nothing \u03b2 = nothing end res Error: UndefVarError: res not defined Inference \u00b6 Since the estimation above consists of multiple steps, constructing standard errors is not quite as simple as for single step estimators. The estimate of $\\beta$ in the second step depends on the first step estimate of $\\alpha$, so the uncertainty in the first step estimate of $\\alpha$ can affect the standard error of $\\beta$. It is possible to derive a formula for the asymptotic standard error of $\\beta$ that takes all this into account, but the formula is somewhat long. See these notes for details . The somewhat long formula for asymptotic standard errors, has lead many applied economists to instead use the bootstrap to compute the standard error of multiple step estimators. Here are some of my notes about bootstrap along with Julia code . Grieco, Paul L. E., and Ryan C. McDevitt. 2017. \u201cProductivity and Quality in Health Care: Evidence from the Dialysis Industry.\u201d The Review of Economic Studies 84 (3): 1071\u20131105. https://doi.org/10.1093/restud/rdw042 .","title":"Assignment"},{"location":"dialysis/#about-this-document","text":"This document was created using Weave.jl. The code is available in on github . The same document generates both static webpages and a jupyter notebook. \\def\\indep{\\perp\\!\\!\\!\\perp} \\def\\Er{\\mathrm{E}} \\def\\R{\\mathbb{R}} \\def\\En{{\\mathbb{E}_n}} \\def\\Pr{\\mathrm{P}} \\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert} \\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} \\def\\inprob{\\,{\\buildrel p \\over \\rightarrow}\\,} \\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}","title":"About this document"},{"location":"dialysis/#introduction","text":"This assignment will reproduce some of the results of Grieco and McDevitt ( 2017 ).","title":"Introduction"},{"location":"dialysis/#getting-started","text":"https://vse.syzygy.ca provides a convenient browser based interface to Julia. Open it and log in. You now need to upload the Jupyter notebook for this assignment and open it. You could download it from that link and then upload to vse.syzygy.ca. Alternatively, open a terminal in syzygy (File -> New -> Terminal). This will open a Linux shell in your browser. To download the notebook to syzygy enter wget https://ubcecon567.github.io/Dialysis/dialysis.ipynb Clicking on the folder icon near the top left of the screen opens a file browser panel. Use it to open the dialysis.ipynb notebook. I recommend renaming your copy of this notebook. You can rename files by right clicking in the file browser panel. Now open your copy of the notebook. Notebooks consist of a series of \u201ccells\u201d of either text written in markdown or Julia code. If you double click on any of the text cells, you can see the markdown that created it. To go back to the formatted text, execute the cell by either clicking the play icon on the top of the page or typing ctrl and enter together.","title":"Getting started"},{"location":"dialysis/#julia-resources","text":"This assignment will try to explain aspects of Julia as needed. However, if at some point you feel lost, you may want to consult some of the following resources. Reading the first few sections of either QuantEcon or Think Julia is recommended.","title":"Julia resources"},{"location":"dialysis/#resources","text":"QuantEcon with Julia Think Julia A detailed introduction to Julia and programming more generally. Long, but recommended, especially if you\u2019re new to programming. From the julia prompt, you can access documentation with ?functionname . Some packages have better documentation than others. https://julialang.org/ is the website for Julia Documentation for core Julia can be found at https://docs.julialang.org/en/v1/ . All Julia packages also have a github page. Many of these include package specific documentation. Notes on Julia from ECON 622 much of this is part of QuantEcon, but not all The Julia Express short book with examples of Julia usage","title":"Resources"},{"location":"dialysis/#part-i-loading-and-exploring-the-data","text":"","title":"Part I: Loading and exploring the data"},{"location":"dialysis/#loading-packages","text":"Like many programming environments (R, Python, etc), Julia relies on packages for lots of its functionality.The following code will download and install all the packages required for this assignment (but the packages will still need to be loaded with using ... ). Execute this cell. It will take some time. While the cell is running, there will be a [*] to the left of it. This will change to [1] (or some other number) after the cell is finished running. The number indicates the order in which the cell was executed. You can execute cells out of order. This can be useful during development, but you should always make sure that your notebook works correctly when cells are executed in order before considering it complete (that is, make sure the \u201cRun -> Restart Kernel and Run all Cells\u201d menu option produces the output you want). Don\u2019t worry about understanding the details of the code in this section. using Pkg try using Dialysis # This assignment itself is in the \"Dialysis\" package. We will use some of the functions from it. catch Pkg.update() Pkg.add(PackageSpec(url=\"https://github.com/UBCECON567/Dialysis\")) using Dialysis end docdir = normpath(joinpath(dirname(Base.pathof(Dialysis)), \"..\",\"docs\")) Pkg.activate(docdir) Activating environment at `~/.julia/dev/Dialysis/docs/Project.toml` Pkg.instantiate()","title":"Loading packages"},{"location":"dialysis/#load-the-data","text":"Now let\u2019s get to work. I originally downloaded the data for this problem set from https://dialysisdata.org/content/dialysis-facility-report-data . As in Grieco and McDevitt ( 2017 ) the data comes from Dialysis Facility Reports (DFRs) created under contract to the Centers for Medicare and Medicaid Services (CMS). However, there are some differences. Most notably, this data covers 2006-2014, instead of 2004-2008 as in Grieco and McDevitt ( 2017 ) . The R script downloadDialysisData.R downloads, combines, and cleans the data. Unfortunately, dialysisdata.org has reorganized their website, and the data no longer seems to be available. Similar (likely identical) data is available from https://data.cms.gov/browse?q=dialysis . It might be useful to look at the documentation included with any of the \u201cDialysis Facility Report Data for FY20XX\u201d zip files. Anyway, the result of the R script is the dialysisFacilityReports.rda file contained in the git repository for this assignment. This R data file contains most of the variables used by Grieco and McDevitt ( 2017 ). using DataFrames # DataFrames.jl is a package for storing and # interacting with datasets dialysis = loaddata() # loaddata() is a function I wrote that is part # of Dialysis.jl. It returns a DataFrame typeof(dialysis) DataFrame We will begin our analysis with some exploratory statistics and figures. There are at least two reasons for this. First, we want to check for any anomalies in the data, which may indicate an error in our code, our understanding of the data, or the data itself. Second, we should try to see if there are any striking patterns in the data that deserve extra attention. We can get some information about all the variables in the data as follows describe(dialysis) 58\u00d78 DataFrame. Omitted printing of 1 columns \u2502 Row \u2502 variable \u2502 mean \u2502 min \u2502 median \u2502 max \u2502 nunique \u2502 nmissing \u2502 \u2502 \u2502 Symbol \u2502 Union\u2026 \u2502 Any \u2502 Union\u2026 \u2502 Any \u2502 Union\u2026 \u2502 Union\u2026 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 1 \u2502 provfs \u2502 \u2502 =\"012306\" \u2502 \u2502 =\"68256 7\" \u2502 7015 \u2502 \u2502 \u2502 2 \u2502 year \u2502 2010.23 \u2502 2006.0 \u2502 2010.0 \u2502 2014.0 \u2502 \u2502 \u2502 \u2502 3 \u2502 comorbidities \u2502 4.70985 \u2502 0.44444 \u2502 4.69811 \u2502 10.7619 \u2502 \u2502 7982 \u2502 \u2502 4 \u2502 comorbidities_p3 \u2502 4.70985 \u2502 0.44444 \u2502 4.69811 \u2502 10.7619 \u2502 \u2502 7982 \u2502 \u2502 5 \u2502 hemoglobin \u2502 9.77373 \u2502 3.6 \u2502 9.7727 \u2502 18.0 \u2502 \u2502 1608 \u2502 \u2502 6 \u2502 hemoglobin_p3 \u2502 9.77326 \u2502 3.6 \u2502 9.7715 \u2502 18.0 \u2502 \u2502 1608 \u2502 \u2502 7 \u2502 std_mortality \u2502 1.00545 \u2502 0.0 \u2502 0.96694 \u2502 4.97293 \u2502 \u2502 2239 \u2502 \u22ee \u2502 51 \u2502 patient_esrd_years \u2502 4.4062 \u2502 0.3121 \u2502 4.3491 \u2502 19.9425 \u2502 \u2502 131 \u2502 \u2502 52 \u2502 treatment_type \u2502 \u2502 Hemodialysis \u2502 \u2502 Unavail able \u2502 4 \u2502 \u2502 \u2502 53 \u2502 inspect_date \u2502 \u2502 1992-05-20 \u2502 \u2502 2015-05 -29 \u2502 3273 \u2502 329 \u2502 \u2502 54 \u2502 inspect_result \u2502 \u2502 . \u2502 \u2502 Unknown \u2502 12 \u2502 2011 \u2502 \u2502 55 \u2502 inspect_cfc_cites \u2502 0.293753 \u2502 0.0 \u2502 0.0 \u2502 10.0 \u2502 \u2502 3918 \u2502 \u2502 56 \u2502 inspect_std_cites \u2502 5.89393 \u2502 0.0 \u2502 4.0 \u2502 153.0 \u2502 \u2502 3921 \u2502 \u2502 57 \u2502 days_since_inspection \u2502 704.886 \u2502 -177.0 \u2502 557.0 \u2502 7454.0 \u2502 \u2502 329 \u2502 \u2502 58 \u2502 original_chain_name \u2502 \u2502 \u2502 \u2502 WELLSPA N DIALYSIS \u2502 189 \u2502 \u2502 The meaning of these variables are as follows: Variable Definition provfs provider identifier year year comorbidities average patient comorbidities hemoglobin average patient hemoglobin level std_mortality standardized mortality ratio std_hosp_days standardized hospitalization days std_hosp_admit standardized hospitalization admittance rate pct_septic percent of patients hospitalized due to septic infection n_hosp_admit number of hospitalizations n_hosp_patients patient_years_hd patient years at risk of hospitalization city city name provider name state state chain_name name of chain if provider is part of one profit_status whether for profit stations number of dialysis stations total_staff total staff dieticiansFT full-time renal dieticians dieticiansPT part-time renal dieticians nurseFT full-time nurses (>32 hours/week) nursePT part-time nurses (\\<32 hours/week) ptcareFT full-time patient care technicians ptcarePT part-time patient care technicians social_workerFT full-time social workers social_workerPT part-time social workers patient_months number of patient-months treated during the year patient_years_rom patient-years at risk of mortality pct_fistula the percentage of patient months in which the patient received dialysis through arteriovenous (AV) fistulae pct_female percent of female patients patient_age average age of patients patient_esrd_years average number of years patients have had end stage renal disease treatment_type types of treatment provided at facility inspect_date date of most recent inspection inspect_result result of most recent inspection inspect_cfc_cites number of condition for coverage deficiencies in most recent inspection inspect_std_cites number of standard deficiencies in most recent inspection days_since_inspection days since last inspection The raw data contains information on many variables in each of the previous 4 years. Staffing variables with no suffix are staff as of January 31, year as reported in year + 1. Staffing variables with \u201c.l1\u201d are staff as of January 31, year - 1 as reported in year + 1. If there were no reporting errors, the .l1 variables would equal the lag of the ones without .l1. However, you might find that this is not the case. As explained in downloadDialysisData.R , data collected in year Y has information on most variables in years Y-1, Y-2, Y-3, and Y-4. However, for some variables and survey years, only information in years Y-2, Y-3, Y-4 is included. For such variables, at year Y-1, I use the value reported in survey year Y if it is available. If not, I use the value reported in survey year Y+1. The variables ending with \u201c.p3\u201d instead use the convention to use use Y-2 values if available and the Y-1 ones if not. Again, if there were no reporting errors the variables with and without .p3 would be the same. There are three variables for the number of patients treated. The data documentation describes patient_months as \u201cPrevalent Hemodialysis Patient Months (7a): The monthly prevalent hemodialysis patient count at a facility includes all non-transient patients (home and in-center) who receive hemodialysis as of the last day of that calendar month. Incident patients (those who received ESRD treatment for the first time ever) are included in this count. Row 7a reports the number of prevalent hemodialysis patient months reported at the facility each year. The number of patient months over a time period is the sum of patients reported for the months covered by the time period. An individual patient may contribute up to 12 patient months per year.\u201d patient_years_rom is the number of patient years at risk of mortality. patient_years_hd is number of patient years at risk of hospitalization. Since hospitalization data is constructed from Medicare records, a patient is considered at risk of hospitalization only when one can be reasonably certain that a hospitalization would be billed to Medicare. Dialysis patients who pay for for hospitalization with other methods could have unobserved hospitalizations. The data guide explains, \u201cIdeally, this table includes only patients whose Medicare billing records include all hospitalizations for the period. To achieve this goal, we require that patients reach a certain level of Medicare-paid dialysis bills to be included in hospitalization statistics, or that patients have Medicare-paid inpatient claims during the period. For the purpose of analysis, each patient\u2019s follow-up time is broken into periods defined by time since dialysis initiation. For each patient, months within a given period are included if that month in the period is considered \u2018eligible\u2019; a month is deemed eligible if it is within two months of a month having at least \\$900 of Medicare-paid dialysis claims or at least one Medicare-paid inpatient claim. In setting this criterion, our aim is to achieve completeness of information on hospitalizations for all patients included in the years at risk.\u201d","title":"Load the data"},{"location":"dialysis/#create-some-variables","text":"Not all variables used Grieco and McDevitt ( 2017 ) are included here. Some variables will need to be transformed to be comparable to what is in the paper. For example, net investment in stations in year $t$ is the difference between the number of stations in year $t+1$ and year in $t$. # sort data by :provfs, :year # function names that end with ! indicate that the function will # modify one (or more) of its inputs. In this case, sort! modifies the # dialysis DataFrame sort!(dialysis, (:provfs, :year)) # things starting with : are Symbols. Names of variables within a # DataFrame must be Symbols, so they all start with : # we can access a single column of DataFrame by writing # dialysis[!,:stations] . This will be a 1 dimensional Array containing # of length equal to the number of rows in the dialysis DataFrame # panellag is a function defined in Dialysis.jl it creates lags and # leads of variables in panel data. It will insert missing values # where appropriate. # putting dialysis[!,:invest] on the left will create a new column in # the dialysis dataframe dialysis[!,:invest] = panellag(:stations, dialysis, :provfs, :year, -1) - dialysis[!,:stations]; # ; prevents the notebook from printing the # output of the last command. Otherwise, notebooks display the output # of the last command in each cell. We can also create labor and hiring. Note that the choices of giving 0.5 weight to part-time workers, including social workers, and weighting all types of staff equally are all somewhat arbitrary and may not agree exactly with what Grieco and McDevitt ( 2017 ) did. dialysis[!,:labor] = (dialysis[!,:nurseFT] + 0.5*dialysis[!,:nursePT]+ dialysis[!,:ptcareFT] + 0.5*dialysis[!,:ptcarePT] + dialysis[!,:dieticiansFT] + 0.5*dialysis[!,:dieticiansPT] + dialysis[!,:social_workerFT] + 0.5*dialysis[!,:social_workerPT]) dialysis[!,:hiring] = panellag(:labor, dialysis, :provfs, :year, -1) - dialysis[!,:labor]; Creating for profit and chain indicators. # create a Boolean for profit indicator dialysis[!,:for_profit] = dialysis[!,:profit_status].==\"For Profit\" # The dot in .== is an example of broadcasting. It's very common to # want to apply the same function to all elements of an # array. Broadcasting does exactly this. If A is an array, and f() is # a function that operates on scalars, then f.(A) will apply f to each # element of A and return an array of the results. The above .== # compares each element of the array dialysis[!,:profit_status] to the # scalar string \"For Profit\" # similarly create indicators for the two largest chains dialysis[!,:fresenius] = dialysis[!,:chain_name].==\"FRESENIUS\" dialysis[!,:davita] = dialysis[!,:chain_name].==\"DAVITA\"; State inspection rates are a bit more complicated to create. using Statistics # for mean, std, and so on # first make an indicator for inspected in the last year dialysis[!,:inspected_this_year] = ((dialysis[!,:days_since_inspection].>=0) .& (dialysis[!,:days_since_inspection].<365)) # then take the mean by state stateRates = by(dialysis, [:state, :year], # by(data, :var, f) will apply f to each group of data # with a different value of :var df -> mean(skipmissing(df[!,:inspected_this_year]))) # df -> mean(skipmissing(df[!,:inspected_this_year])) is a shorthand way # to define a function it's equalivant to # # function f(df) # mean(skipmissing(df[!,:inspected_this_year])) # end # # skipmissing skips missing values inside a DataFrame. Most arithmetic # functions will not do what you want if missing values are included. # rename the variable in the stateRates DataFrame rename!(stateRates, :x1 => :state_inspection_rate) # merge the stateRates with the dialysis data dialysis = join(dialysis, stateRates, on = [:state, :year]); Creating the number of competitors in the same city is somewhat similar. Note that Grieco and McDevitt ( 2017 ) use the number of competitors in the same HSA, which would be preferrable. However, this dataset does not contain information on HSAs. If you are feeling ambitious, you could try to find data linking city, state to HSA, and use that to calculate competitors in the same HSA. dialysis[!,:city] = uppercase.(dialysis[!,:city]) comps = by(dialysis,[:city,:year], df -> mapreduce((x) -> ifelse(ismissing(x),0,1*(x>0)), +, df[!,:patient_months]) ) rename!(comps, :x1 => :competitors) dialysis = join(dialysis, comps, on = [:city,:year]);","title":"Create some variables"},{"location":"dialysis/#problem-1-summary-statistics","text":"Creata a table (or multiple tables) similar to Tables 1-3 of Grieco and McDevitt ( 2017 ). Comment on any notable differences. The following code will help you get started. using Statistics # at the very least, you will need to change this list vars = [:patient_years_rom, :labor, :hiring] # You shouldn't neeed to change this function, but you can if you want function summaryTable(df, vars; funcs=[mean, std, x->length(collect(x))], colnames=[:Variable, :Mean, :StDev, :N]) # In case you want to search for information about the syntax used here, # [XXX for XXX] is called a comprehension # The ... is called the splat operator DataFrame([vars [[f(skipmissing(df[!,v])) for v in vars] for f in funcs]...], colnames) end summaryTable(dialysis, vars) 3\u00d74 DataFrame \u2502 Row \u2502 Variable \u2502 Mean \u2502 StDev \u2502 N \u2502 \u2502 \u2502 Any \u2502 Any \u2502 Any \u2502 Any \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 1 \u2502 patient_years_rom \u2502 67.1459 \u2502 45.6089 \u2502 48224 \u2502 \u2502 2 \u2502 labor \u2502 13.1744 \u2502 8.34365 \u2502 42512 \u2502 \u2502 3 \u2502 hiring \u2502 0.422357 \u2502 4.30587 \u2502 34665 \u2502","title":"Problem 1: Summary statistics"},{"location":"dialysis/#problem-2-exploratory-figures","text":"Create some figures to explore the data. Try to be creative. Are there any strange patterns or other obvious problems with the data? Here are some examples to get started. You may want to look at the StatPlots.jl, Plots.jl, or VegaLite.jl github pages for more examples.","title":"Problem 2: exploratory figures"},{"location":"dialysis/#comparing-output-measures","text":"using StatsPlots , Plots Plots.pyplot() dialysis[!,:patient_years] = dialysis[!,:patient_months]/12 # missings will mess up corrplot vars = [:patient_years, :patient_years_hd, :patient_years_rom] inc = completecases(dialysis[:,vars]) @df dialysis[inc,vars] corrplot(cols(vars))","title":"Comparing output measures"},{"location":"dialysis/#trends-over-time","text":"function yearPlot(var) data = dialysis[completecases(dialysis[:,[:year, var]]),:] scatter(data[!,:year], data[!,var], alpha=0.1, legend=:none, markersize=3, markerstrokewidth=0.0) yearmeans = by(data, :year, mean = var => x->mean(skipmissing(x)), q01 = var => x->quantile(skipmissing(x), 0.01), q10 = var => x->quantile(skipmissing(x), 0.1), q25 = var => x->quantile(skipmissing(x), 0.25), q50 = var => x->quantile(skipmissing(x), 0.50), q75 = var => x->quantile(skipmissing(x), 0.75), q90 = var => x->quantile(skipmissing(x), 0.9), q99 = var => x->quantile(skipmissing(x), 0.99)) @df yearmeans plot!(:year, :mean, colour = ^(:black), linewidth=4) @df yearmeans plot!(:year, cols(3:ncol(yearmeans)), colour = ^(:red), alpha=0.4, legend=:none, xlabel=\"year\", ylabel=String(var)) end yearPlot(:labor) The above plot shows a scatter of labor vs year. The black lines are average labor each year. The red lines are the 0.01, 0.1, 0.25, 0.5, 0.75, 0.9, and 0.99 quantiles conditional on year.","title":"Trends over time"},{"location":"dialysis/#part-ii-estimating-the-model","text":"","title":"Part II: estimating the model"},{"location":"dialysis/#problem-3-quality-measures","text":"Grieco and McDevitt ( 2017 ) use the residuals from regressing the infection rate on patient characteristics as a measure of quality. Since the infection rate is a noisy measure of quality, they instrument with the standardized mortality ratio as a second measure of quality. Medicare collects the data we are using in part to create the \u201cDialysis Facility Compare\u201d website, which is meant to allow consumers to compare quality of dialysis facilities. Browsing around the Dialysis Facility Compare or by looking at the first few pages of a sample Dialysis Facility Report , you will see that there are a number of other variables that Medicare considers indicators of quality. Pick one of these (it may or may not be included in the extract of data I provided), and argue for or against using it instead of or in addition to the septic infection rate and standardized mortality ratio. We can construct residuals from an OLS regression as follows: \"\"\" ols_residuals(data::AbstractDataFrame, y::Symbol, x::Array{Symbol,1}; intecept::Bool=true) This is a doc string. After executing this cell, if you type ?ols_residuals, you will see this text. Calculate residuals from an OLS regression of data[y] on data[x] Inputs: - `data` DataFrame containg y and x - `y` Symbol specifying y variable - `x` Symbol specifying x variables Output: - Vector of residuals of length = nrow(data) \"\"\" function ols_residuals(data::DataFrame, y::Symbol, x::Array{Symbol,1}; # arguments following the # are optional intercept::Bool=true ) # The :: are type specifications. They could be left out, and this # function would still work just fine. One of their purposes are to # document what inputs this function expects, and throw useful error # messages if someone tries calling the function on the wrong types. inc = completecases(data[:,[y, x...]]) # deal with missing Y = disallowmissing(data[inc,y]) if (intercept) X = [ones(sum(inc)) data[inc,x]] else X = data[inc,x] end X = disallowmissing(convert(Matrix, X)) # you can type Greek and some other LaTeX characters by typing their LaTeX # code followed by tab, e.g. \\beta<TAB> and \\in<TAB> \u03b2 = X \\ Y # \u03b2 \u2208 argmin_b || X*b - Y || \u03f5 = Y - X*\u03b2 if (any(.!inc)) # add back in missings resid = Array{Union{Missing,eltype(\u03f5)},1}(undef, nrow(data)) resid .= missing resid[inc] = \u03f5 return(resid) else # no missing, just return \u03f5 return(\u03f5) end end q = -ols_residuals(dialysis, :pct_septic, [:days_since_inspection, :patient_age, :pct_female, :patient_esrd_years, :pct_fistula, :comorbidities, :hemoglobin]); Of course, regression is common enough that there are already Julia packages for it. I included the ols_residuals only for pedagogical purposes. Whenever there exists a well-used package, it is (usually) better to use the package than try to write your own functions. Here\u2019s how to accomplish the same thing using FixedEffectModels.jl. using FixedEffectModels dialysis[!,:idcat] = categorical(dialysis[!,:provfs]) # FixedEffectModels requires clustering and fixed effect variables to # be categorical qreg = reg(dialysis, @formula(pct_septic ~ days_since_inspection + patient_age + pct_female + patient_esrd_years + pct_fistula + comorbidities + hemoglobin), Vcov.cluster(:idcat),save=true) # saves residuals in augmentdf dialysis[!,:quality] = -qreg.augmentdf[!,:residuals] # Let's test that these results are the same from ols_residuals println(\"Mean absolute difference = $(mean(skipmissing( abs.(q.- dialysis[!,:quality]) )))\") Mean absolute difference = 5.453941239030147e-14 # using $(expr) in a string will insert the result of expr in the$ # string using Test @test all(skipmissing(q .\u2248 dialysis[!,:quality])) == true Test Passed Testing is an important part of software development. The Test.jl package provides help function for running tests. See these notes from 622 for more information about testing.","title":"Problem 3: Quality measures"},{"location":"dialysis/#problem-4-ols-and-fixed-effects-estimates","text":"Reproduce columns 2,3, 5, and 6 of Table 5. The syntax for fixed effects regression is shown below: # you may want to use patient_years_hd or patient_years_rom instead log_infmiss = x->ifelse(!ismissing(x) && x>0, log(x), missing) # -Inf confuses reg() dialysis[!,:lpy] = log_infmiss.(dialysis[!,:patient_months]./12) dialysis[!,:logL] = log_infmiss.(dialysis[!,:labor]) dialysis[!,:logK] = log_infmiss.(dialysis[!,:stations]) # you may want to restrict sample to match sample that can be used in model estimates reg(dialysis, @formula(lpy ~ quality + logK + logL + fe(idcat)), Vcov.cluster(:idcat)) Fixed Effect Model ========================================================================= Number of obs: 32013 Degrees of freedom: 4 R2: 0.882 R2 Adjusted: 0.882 F Statistic: 312.361 p-value: 0.000 R2 within: 0.144 Iterations: 1 Converged: true ========================================================================= Estimate Std.Error t value Pr(>|t|) Lower 95% Upper 95% ------------------------------------------------------------------------- quality -0.00921389 0.000589458 -15.6311 0.000 -0.0103692 -0.00805853 logK 0.171904 0.028534 6.02452 0.000 0.115976 0.227831 logL 0.470537 0.0171159 27.4912 0.000 0.43699 0.504085 ========================================================================= Be sure to add the other columns. If you\u2019d like, you could use RegressionTables.jl to produce tables that look a lot like the ones in the paper.","title":"Problem 4: OLS and fixed effects estimates"},{"location":"dialysis/#estimation-of-alpha","text":"As discussed in section 5 of Grieco and McDevitt ( 2017 ), the coefficient on quality, $\\alpha$, is estimated from y_{jt} = \\alpha q_{jt} + \\Phi(\\underbrace{h_{jt}, k_{jt}, l_{jt}, x_{jt}}_{w_{jt}}) + \\epsilon_{jt} with a second noisy measure of quality, $z_{jt}$, used to instrument for $q_{jt}$. To estimate $\\alpha$, first the exogenous variables, $w$, can be partialed out to give: y_{jt} - \\Er[y_{jt} | w_{jt} ] = \\alpha (q_{jt} - \\Er[q_{jt}|w_{jt}]) + \\epsilon_{jt} where we used the assumption that $\\Er[\\epsilon_{jt} | w_{jt} ] = 0$ and the fact that $\\Er[\\Phi(w) | w] = \\Phi(w)$. Under the assumption that $\\Er[\\epsilon| z, w] = 0$, we can estimate $\\alpha$ based on the moment condition: \\begin{align*} 0 = & \\Er[\\epsilon f(z,w) ] \\\\ 0 = & \\Er\\left[ \\left(y_{jt} - \\Er[y_{jt} | w_{jt} ] - \\alpha (q_{jt} - \\Er[q_{jt}|w_{jt}])\\right) f(z_{jt},w_{jt}) \\right] \\end{align*} If $Var(\\epsilon|z,w)$ is constant, the efficient choice of $f(z,w)$ is \\Er[\\frac{\\partial \\epsilon}{\\partial \\alpha} |z, w ] = \\Er[q| z, w] - \\Er[q|w] To estimate $\\alpha$, we simply replace these conditional expectations with regression estimates, and replace the unconditional expectation with a sample average. Let $\\hat{\\Er}[y|w]$ denote a nonparmetric estimate of the regression of $y$ on $w$. Then, \\hat{\\alpha} = \\frac{\\sum_{j,t} (y_{jt} - \\hat{E}[y|w_{jt}])(\\hat{E}[q|z_{jt},w_{jt}] - \\hat{E}[q|w_{jt}])} {\\sum_{j,t} (q_{jt} - \\hat{E}[q|w_{jt}])(\\hat{E}[q|z_{jt},w_{jt}] - \\hat{E}[q|w_{jt}])} The function partiallinearIV in Dialysis.jl will estimate this model. Also included are two methods for estimating $\\hat{E}[y|w]$. polyreg estimates a polynomial series regression, that is it regresses $y$ on a polynomial of degree $d$ in $w$. To allow the regression to approximate any function, the degree must increase with the sample size, but to control variance, the degree must not increase too quickly. We will not worry too much about the choice of degree here. An alternative method (and what Grieco and McDevitt ( 2017 ) used) is local linear regression. To estimate $\\hat{E}[y|x_{jt}]$, local linear regression estimates a linear regression of $y$ on $x$, but weights observations by how close $x_{it}$ is to $x_{jt}$. That is, \\hat{E}[y|x_{jt}] = x_{jt} \\hat{\\beta}(x_jt) where \\hat{\\beta}(x_jt) = \\argmin_\\beta \\sum_{i,t} (y_{it} - x_{it}\\beta)^2 k((x_{it} - x_{jt})/h_n) Here $k()$ is some function with its maximum at 0 (and has some other properties), like $k(x) \\propto e^{-x^2}$. The bandwidth, $h_n$, determines how much weight to place on observations close to vs far from $x_{jt}$. Similar to the degree in polynomial regression, the bandwidth must decrease toward 0 with sample size allow local linear regression to approximate any function, but to control variance the bandwidth must not decrease too quickly. We will not worry too much about the choice of bandwidth. Anyway, the function locallinear in Dialysis.jl estimates a local linear regression.","title":"Estimation of $\\alpha$"},{"location":"dialysis/#problem-5-alpha","text":"Estimate $\\alpha$ using the following code. You may want to modify some aspects of it and/or report estimates of $\\alpha$ for different choices of instrument, nonparametric estimation method, degree or bandwidth. Compare your estimate(s) of $\\alpha$ with the ones in Tables 5 and 6 of Grieco and McDevitt ( 2017 ). # create indicator for observations usable in estimation of \u03b1 inc1 = ((dialysis[!,:patient_months] .> 0) .& (dialysis[!,:labor] .> 0) .& (dialysis[!,:stations] .> 0) .& .!ismissing.(dialysis[!,:quality]) .& .!ismissing.(dialysis[!,:std_mortality]) .& (dialysis[!,:invest].==0) .& (dialysis[!,:hiring].!=0)); inc1[ismissing.(inc1)] .= false; dialysis[!,:inc1] = inc1; dialysis[!,:lsmr] = log.(dialysis[!,:std_mortality] .+ .01) # As degree \u2192 \u221e and/or bandwidth \u2192 0, whether we use :std_mortality or # some transformation as the instrument should not matter. However, # for fixed degree or bandwidth it will have some (hopefully small) # impact. (\u03b1, \u03a6, \u03b1reg, eyqz)=partiallinearIV(:lpy, # y :quality, # q :lsmr, # z [:hiring, :logL, :logK, :state_inspection_rate, :competitors], # w dialysis[findall(dialysis[!,:inc1]),:]; npregress=(xp, xd,yd)->polyreg(xp,xd,yd,degree=1), parts=true # You may want to change the degree here # # You could also change `polyreg` to # `locallinear` and `degree` to # `bandwidthmultiplier` # # locallinear will likely take some time to # compute (\u2248350 seconds on my computer) ) # we will need these later in step 2 dialysis[!,:\u03a6] = similar(dialysis[!,:lpy]) dialysis[:,:\u03a6] .= missing rows = findall(dialysis[!,:inc1]) dialysis[rows,:\u03a6] = \u03a6 dialysis[!,:ey] = similar(dialysis[!,:lpy]) dialysis[:,:ey] .= missing dialysis[rows,:ey] = eyqz[:,1] dialysis[!,:eq] = similar(dialysis[!,:lpy]) dialysis[:,:eq] .= missing dialysis[rows,:eq] = eyqz[:,2] dialysis[!,:ez] = similar(dialysis[!,:lpy]) dialysis[:,:ez] .= missing dialysis[rows,:ez] = eyqz[:,3] \u03b1 -0.016731503377462428","title":"Problem 5: $\\alpha$"},{"location":"dialysis/#brief-introduction-to-gmm","text":"The coefficients on labor and capital are estimated by GMM. The idea of GMM is as follows. We have a model that implies \\Er[c(y,x;\\theta) | z ] = 0 where $y$, $x$, and $z$ are observed variables. $c(y,x;\\theta)$ is some known function of the data and some parameters we want to estimate, $\\theta$. Often, $c(y,x;\\theta)$ are the residuals from some equation. For example, for linear IV, we\u2019d have c(y,x;\\theta) = y - x\\theta The conditional moment restriction above implies that \\Er[c(y,x;\\theta)f(z) ] = 0 for any function $f()$. We can then estimate $\\theta$ by replacing the population expectation with a sample average and finding $\\hat{\\theta}$ such that \\En[c(y,x;\\hat{\\theta})f(z) ] \\approx 0 The dimension of $f(z)$ should be greater than or equal to the dimension of $\\theta$, so we have at least as many equations as unknowns. We find this $\\hat{\\theta}$ by minimizing a quadratic form of these equations. That is, \\hat{\\theta} = \\argmin_\\theta \\En[g_i(\\theta)] W_n \\En[g_i(\\theta)]' were $g_i(\\theta) = c(y_i, x_i;\\theta)f(z_i)$, and $W_n$ is some positive definite weighting matrix.","title":"Brief introduction to GMM"},{"location":"dialysis/#problem-6-ols-by-gmm","text":"As practice with GMM, use it to estimate a simple regression model, y = x\\beta + \\epsilon assuming $\\Er[\\epsilon|x] = 0$. Test your code on simulated data. The following will help you get started. function sim_ols(n; \u03b2 = ones(3)) x = randn(n, length(\u03b2)) \u03f5 = randn(n) y = x*\u03b2 + \u03f5 return(x,y) end \u03b2 = ones(2) (x, y) = sim_ols(100; \u03b2=\u03b2) \u03b2ols = (x'*x) \\ (x'*y) function gmm_objective(\u03b2) gi = (y - x*\u03b2) .* x Egi = mean(gi, dims=1) error(\"This is incomplete; you must finish it\") # It is is likely that the code you will write will return a 1 x 1, # 2 dimensional array. For compatibility with Optim, you need to # return a scalar. If foo is a 1x1 array, write `foo[1]` to return a scalar instead of # 1x1 array end # minimizer gmm_objective using Optim # github page : https://github.com/JuliaNLSolvers/Optim.jl # docs : http://julianlsolvers.github.io/Optim.jl/stable/ try res = optimize(gmm_objective, zeros(size(\u03b2)), # initial value BFGS(), # algorithm, see http://julianlsolvers.github.io/Optim.jl/stable/ autodiff=:forward) \u03b2gmm = res.minimizer catch err @info err \u03b2gmm = \u03b2ols res = nothing end @test \u03b2gmm \u2248 \u03b2ols Error During Test at none:1 Test threw exception Expression: \u03b2gmm \u2248 \u03b2ols UndefVarError: \u03b2gmm not defined Stacktrace: [1] top-level scope at /home/paul/.julia/dev/Dialysis/docs/makeweave.jl: 10 [2] eval at ./boot.jl:330 [inlined] [3] capture_output(::Expr, ::Module, ::Bool, ::Bool, ::Bool, ::Bool) at /home/paul/.julia/packages/Weave/UOxmI/src/run.jl:230 [4] run_code(::Weave.CodeChunk, ::Weave.Report, ::Module) at /home/paul/ .julia/packages/Weave/UOxmI/src/run.jl:208 [5] eval_chunk(::Weave.CodeChunk, ::Weave.Report, ::Module) at /home/pau l/.julia/packages/Weave/UOxmI/src/run.jl:289 [6] run_chunk(::Weave.CodeChunk, ::Weave.WeaveDoc, ::Weave.Report, ::Mod ule) at /home/paul/.julia/packages/Weave/UOxmI/src/run.jl:130 [7] #run#34(::String, ::Module, ::String, ::Dict{String,Bool}, ::String, ::Nothing, ::String, ::Symbol, ::Bool, ::typeof(run), ::Weave.WeaveDoc) at /home/paul/.julia/packages/Weave/UOxmI/src/run.jl:94 [8] (::Base.var\"#kw##run\")(::NamedTuple{(:doctype, :mod, :out_path, :arg s, :fig_path, :fig_ext, :cache_path, :cache, :throw_errors),Tuple{String,Mo dule,String,Dict{String,Bool},String,Nothing,String,Symbol,Bool}}, ::typeof (run), ::Weave.WeaveDoc) at ./none:0 [9] #weave#16(::String, ::Symbol, ::String, ::Dict{String,Bool}, ::Modul e, ::String, ::Nothing, ::String, ::Symbol, ::Bool, ::Nothing, ::Nothing, : :Nothing, ::Array{String,1}, ::String, ::typeof(weave), ::String) at /home/ paul/.julia/packages/Weave/UOxmI/src/Weave.jl:121 [10] (::Weave.var\"#kw##weave\")(::NamedTuple{(:out_path, :cache, :cache_p ath, :doctype, :mod, :args),Tuple{String,Symbol,String,String,Module,Dict{S tring,Bool}}}, ::typeof(weave), ::String) at ./none:0 [11] top-level scope at /home/paul/.julia/dev/Dialysis/docs/makeweave.jl :13 Error: Test.FallbackTestSetException(\"There was an error during testing\") res Error: UndefVarError: res not defined","title":"Problem 6: OLS by GMM"},{"location":"dialysis/#estimating-beta","text":"The model implies that \\omega_{jt} = \\Phi(w_{jt}) - \\beta_k k_{jt} - \\beta_l l_{jt} and y_{jt} - \\alpha q_{jt} - \\beta_k k_{jt} - \\beta_l l_{jt} = g(\\omega_{jt-1}) + \\eta_{jt} {#eq:eta} The timing and exogeneity assumptions imply that \\Er[\\eta_{jt} | k_{jt}, l_{jt}, w_{jt-1}] Given a value of $\\beta$, and our above estimates of $\\Phi$ and $\\alpha$, we can compute $\\omega$ from the equation above, and then estimate $g()$ and $\\eta$ by a nonparametric regression of $y_{jt} - \\alpha q_{jt} - \\beta_k k_{jt} - \\beta_l l_{jt}$ on $\\omega_{jt-1}$. $\\beta$ can then be estimated by finding the value of $\\beta$ that comes closest to satisfying the moment condition \\Er[\\eta(\\beta)_{jt} k_{jt}] = 0 \\text{ and } \\Er[\\eta(\\beta)_{jt} l_{jt}] = 0 To do this, we minimize Q_n(\\beta) = \\left( \\frac{1}{N} \\sum_{j,t} \\eta(\\beta)_{jt} (k_{jt}, l_{jt}) \\right) W_n \\left( \\frac{1}{N} \\sum_{j,t} \\eta(\\beta)_{jt} (k_{jt}, l_{jt}) \\right)'","title":"Estimating $\\beta$"},{"location":"dialysis/#problem-7-estimate-beta","text":"Write the body of the $Q_n(\\beta)$ function below. Use it to estimate $\\beta$. Compare your results with those of Grieco and McDevitt ( 2017 ). Optionally, explore robustness of your results to changes in the specification. # indicator for observations usable in estimation of \u03b2 dialysis[!,:inclag] = panellag(:inc1, dialysis, :provfs, :year, 1); dialysis[!,:inc2] = (dialysis[!,:inclag] .& (dialysis[!,:stations].>0) .& (dialysis[!,:labor].>0) .& (dialysis[!,:patient_years].>0) .& .!ismissing.(dialysis[!,:quality])); dialysis[ismissing.(dialysis[!,:inc2]),:inc2] .= false; (\u03c9func, \u03b7func) = errors_gm(:lpy, :logK, :logL, :quality, :\u03a6, :provfs, :year, dialysis, \u03b1; degree=1) function Qn(\u03b2) \u03b7 = \u03b7func(\u03b2) error(\"You must write the body of this function\") end using Optim try res = optimize(Qn, # objective [0.0, 0.0], # lower bounds, should not be needed, but # may help numerical performance [1.0, 1.0], # upper bounds [0.4, 0.2], # initial value Fminbox(BFGS()), # algorithm autodiff=:forward) \u03b2 = res.minimizer @show \u03b2 catch err @info err res = nothing \u03b2 = nothing end res Error: UndefVarError: res not defined","title":"Problem 7: estimate $\\beta$"},{"location":"dialysis/#inference","text":"Since the estimation above consists of multiple steps, constructing standard errors is not quite as simple as for single step estimators. The estimate of $\\beta$ in the second step depends on the first step estimate of $\\alpha$, so the uncertainty in the first step estimate of $\\alpha$ can affect the standard error of $\\beta$. It is possible to derive a formula for the asymptotic standard error of $\\beta$ that takes all this into account, but the formula is somewhat long. See these notes for details . The somewhat long formula for asymptotic standard errors, has lead many applied economists to instead use the bootstrap to compute the standard error of multiple step estimators. Here are some of my notes about bootstrap along with Julia code . Grieco, Paul L. E., and Ryan C. McDevitt. 2017. \u201cProductivity and Quality in Health Care: Evidence from the Dialysis Industry.\u201d The Review of Economic Studies 84 (3): 1071\u20131105. https://doi.org/10.1093/restud/rdw042 .","title":"Inference"},{"location":"license/","text":"These docs are licensed under a Creative Commons Attribution-ShareAlike 4.0 International License and were written by Paul Schrimpf.","title":"License"}]}