{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\ntitle       : \"Assignment: reproducing Grieco & McDevitt (2017)\"\nsubtitle    : \nauthor      : Paul Schrimpf\ndate        : `j using Dates; print(Dates.today())`\nbibliography: \"dialysis.bib\"\n---\n\n<a rel=\"license\"\nhref=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative\nCommons License\" style=\"border-width:0\"\nsrc=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\"\n/></a><br />This work is licensed under a <a rel=\"license\"\nhref=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative\nCommons Attribution-ShareAlike 4.0 International License</a>.\n\n### About this document {-}\n\nThis document was created using Weave.jl. The code is available in\n[on github](https://github.com/ECON567/Dialysis). The same\ndocument generates both static webpages and associated jupyter\nnotebooks.\n\n$$\n\\def\\indep{\\perp\\!\\!\\!\\perp}\n\\def\\Er{\\mathrm{E}}\n\\def\\R{\\mathbb{R}}\n\\def\\En{{\\mathbb{E}_n}}\n\\def\\Pr{\\mathrm{P}}\n\\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert}\n\\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\inprob{\\,{\\buildrel p \\over \\rightarrow}\\,} \n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,} \n$$\n\n# Introduction\n\nThis assignment will reproduce some of the results of @grieco2017. \n\n## Getting started \n\n[https://vse.syzygy.ca](https://vse.syzygy.ca) provides a convenient\nbrowser based interface to Julia. Open it and log in. This assignment\nis in a git repository at\n[https://github.com/UBCECON567/Dialysis](https://github.com/UBCECON567/Dialysis). Start\nby cloning the git repository to your syzygy directory. Open a\nterminal in syzygy (File -> New -> Terminal). This will open a Linux\nshell in your browser. To clone the git repository, enter \n\n`\ngit clone https://github.com/UBCECON567/Dialysis\n`\n\nThis will create a directory called `Dialysis` containing all the\nfiles related to this assignment. \n\nClicking on the folder icon near the top left of the screen opens a\nfile browser panel. Use it to open the `Dialysis/notebooks`\nfolder. You can complete this assignment by modifying the\n`dialysis.ipynb` notebook. I recommend creating a copy of this\nnotebook, and then working on the copy. You can create a copy by right\nclicking in the file browser panel. Now open your copy of the\nnotebook.\n\nNotebooks consist of a series of \"cells\" of either text written in\nmarkdown or Julia code. If you double click on any of the text cells,\nyou can see the markdown that created it. To go back to the formatted\ntext, execute the cell by either clicking the play icon on the top of\nthe page or typing ctrl and enter together. \n\n\n## Julia resources\n\nThis assignment will try to explain aspects of Julia as\nneeded. However, it at some point you feel lost, you may want to\nconsult some of the following resources. Reading the first few\nsections of either QuantEcon or Think Julia is recommended.\n\n### Resources\n\n- [QuantEcon with Julia](https://lectures.quantecon.org/jl/)\n\n- [Think Julia](https://benlauwens.github.io/ThinkJulia.jl/latest/book.html#_colophon) \n  A detailed introduction to Julia and programming more\n  generally. Long, but recommended, especially if you're new to\n  programming. \n\n- From the julia prompt, you can access documentation with\n  `?functionname`. Some packages have better documentation than\n  others. \n\n- [https://julialang.org/](https://julialang.org/) is the website for\n  Julia\n  \n- Documentation for core Julia can be found at\n  [https://docs.julialang.org/en/v1/](https://docs.julialang.org/en/v1/). All\n  Julia packages also have a github page. Many of these will include\n  package specific documentation.\n\n- [Notes on Julia from ECON\n  628](https://github.com/ubcecon/ECON628_2018) much of this is part\n  of QuantEcon, but not all\n\n- [The Julia Express](https://github.com/bkamins/The-Julia-Express)\n  short book with examples of Julia usage\n  \n\n<!-- -------------------------------------------------------------------------------- -->\n\n# Part I: Loading and exploring the data\n\n## Loading packages\n\nLike many programming environments (R, Python, etc), Julia relies on\npackages for lots of its functionality.The following code will\ndownload and install all the packages required for this\nassignment (but the packages will still need to be loaded with `using\n...`). Execute this cell. It will take some time. While the cell \nis running, there will be a `[*]` to the left of it. This will change\nto `[1]` (or some other number) after the cell is finished\nrunning. The number indicates the order in which the cell was\nexecuted. You can execute cells out of order. This can be useful\nduring development, but you should always make sure that your notebook\nworks correctly when cells are executed in order before considering it\ncomplete (that is, make sure the \"Run -> Restart Kernel and Run all\nCells\" menu option produces the output you want). Don't worry about\nunderstanding the details of the code in this section."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Pkg # loads the Pkg package \nPkg.activate(\"..\") # loads environment specified in \"../Project.toml\"\n                   # This contains a list of packages and their\n                   # versions\nPkg.instantiate()  # installs all packages in the environment"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some useful functions for this assignment are in\n`../src/Dialysis.jl`. We load those functions now."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Revise\nif (!(\"../src\" âˆˆ LOAD_PATH))\n  push!(LOAD_PATH, \"../src\") # so that `using` knows where to find Dialysis.jl\nend\nusing Dialysis"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The functions in Dialysis.jl are organized into a module, \njust like any other Julia package. Revise.jl is a package to make it\neasier to develop packages (like Dialysis.jl). In particular, `using\nRevise` will make it so that as soon as you save any modifications to\nDialysis.jl, those modifications will be loaded into your Julia\nsession without you doing anything extra. \n\n\n## Load the data\n\nNow let's get to work. I originally downloaded the data for this\nproblem set from\n[https://dialysisdata.org/content/dialysis-facility-report-data](https://dialysisdata.org/content/dialysis-facility-report-data).\nAs in @grieco2017 the data comes from Dialysis Facility Reports (DFRs)\ncreated under contract to the Centers for Medicare and Medicaid\nServices (CMS). However, there are some differences. Most notably,\nthis data covers 2006-2014, instead of 2004-2008 as in @grieco2017 .\n\nThe R script\n[https://bitbucket.org/paulschrimpf/econ565/src/master/assignments/production-R/dialysis/downloadDialysisData.R](downloadDialysisData.R)\ndownloads, combines, and cleans the data. Unfortunately, dialysisdata.org\nhas reorganized their website, and the data no longer seems to be\navailable. Similar (likely identical) data is available\nfrom\n[https://data.cms.gov/browse?q=dialysis](https://data.cms.gov/browse?q=dialysis). It\nmight be useful to look at the documentation included with any of the\n\"Dialysis Facility Report Data for FY20XX\" zip files. Anyway, the\nresult of the R script is the `dialysisFacilityReports.rda` file\ncontained in this git repository. This R data file contains most of\nthe variables used by @grieco2017."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using DataFrames  # DataFrames.jl is a package for storing and\n                  # interacting with datasets\ndialysis = loaddata() # loaddata() is a function I wrote that is part\n                      # of Dialysis.jl. It returns a DataFrame\ntypeof(dialysis)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will begin our analysis with some exploratory statistics and\nfigures. There are at least two reasons for this. First, we want to\ncheck for any anomalies in the data, which may indicate an error in\nour code, our understanding of the data, or the data itself. Second,\nwe should try to see if there are any striking patterns in the data\nthat deserve extra attention. We can get some information about all\nthe variables in the data as follows"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "describe(dialysis)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The meaning of these variables are as follows:\n\n Variable | Definition                                            \n----------|--------------------------------------------------------\nprovfs | provider identifier\nyear | year \ncomorbidities | average patient comorbidities\nhemoglobin | average patient hemoglobin level\nstd_mortality | standardized mortality ratio\nstd_hosp_days | standardized hospitalization days\nstd_hosp_admit | standardized hospitalization admittance rate\npct_septic | percent of patients hospitalized due to septic infection\nn_hosp_admit | number of hospitalizations\nn_hosp_patients | \npatient_years_hd | patient years at risk of hospitalization\ncity | city\nname | provider name\nstate | state\nchain_name | name of chain if provider is part of one\nprofit_status | whether for profit\nstations | number of dialysis stations\ntotal_staff | total staff\ndieticiansFT | full-time renal dieticians\ndieticiansPT | part-time renal dieticians\nnurseFT | full-time nurses (>32 hours/week)\nnursePT | part-time nurses (<32 hours/week)\nptcareFT | full-time patient care technicians\nptcarePT | part-time patient care technicians\nsocial_workerFT | full-time social workers\nsocial_workerPT | part-time social workers\npatient_months | number of patient-months treated during the year\npatient_years_rom | patient-years at risk of mortality\npct_fistula | the percentage of patient months in which the patient received dialysis through arteriovenous (AV) fistulae\npct_female | percent of female patients\npatient_age | average age of patients\npatient_esrd_years | average number of years patients have had end stage renal disease\ntreatment_type | types of treatment provided at facility \ninspect_date | date of most recent inspection\ninspect_result | result of most recent inspection\ninspect_cfc_cites | number of condition for coverage deficiencies in most recent inspection\ninspect_std_cites | number of standard deficiencies in most recent inspection\ndays_since_inspection | days since last inspection \n\nThe raw data contains information on many variables in each of the\nprevious 4 years. Staffing variables with no suffix are staff as of\nJanuary 31, year as reported in year + 1.  Staffing variables with\n\".l1\" are staff as of January 31, year - 1 as reported in year +\n1. If there were no reporting errors, the .l1 variables would equal\nthe lag of the ones without .l1. However, you might find that this is\nnot the case.\n\nAs explained in downloadDialysisData.R, data collected in year Y has\ninformation on most variables in years Y-1, Y-2, Y-3, and\nY-4. However, for some variables and survey years, only information in\nyears Y-2, Y-3, Y-4 is included. For such variables, at year Y-1, I\nuse the value reported in survey year Y if it is available. If not, I\nuse the value reported in survey year Y+1. The variables ending with\n\".p3\" instead use the convention to use use Y-2 values if available\nand the Y-1 ones if not. Again, if there were no reporting errors the\nvariables with and without .p3 would be the same. \n\nThere are three variables for the number of patients treated. The data\ndocumentation describes `patient_months` as\n\n\"Prevalent Hemodialysis Patient Months (7a): The monthly prevalent\nhemodialysis patient count at a facility includes all non-transient\npatients (home and in-center) who receive hemodialysis as of the last\nday of that calendar month. Incident patients (those who received ESRD\ntreatment for the first time ever) are included in this count.  Row 7a\nreports the number of prevalent hemodialysis patient months  reported\nat  the  facility  each  year.  The  number  of  patient  months  over\na  time period  is  the  sum  of  patients  reported  for  the  months\ncovered  by  the  time  period.  An individual patient may contribute\nup to 12 patient months per year.\"\n\n`patient_years_rom` is the number of patient years at risk of\nmortality. `patient_years_hd` is number of patient years at risk of\nhospitalization. Since hospitalization data is constructed from\nMedicare records, a patient is considered at risk of hospitalization\nonly when one can be reasonably certain that a hospitalization would\nbe billed to Medicare. Dialysis patients who pay for for\nhospitalization with other methods could have unobserved\nhospitalizations. The data guide explains,\n\n\"Ideally, this table includes only patients whose Medicare billing\nrecords include all hospitalizations for the period.  To achieve this\ngoal, we require that patients reach a certain level of Medicare-paid\ndialysis bills to be included in hospitalization statistics, or that\npatients have Medicare-paid inpatient claims during the period.  For\nthe purpose of analysis, each patientâ€™s follow-up time is broken into\nperiods defined by time since dialysis initiation. For each patient,\nmonths within a given period are included if that month in the period\nis considered â€˜eligibleâ€™; a month is deemed eligible if it is within\ntwo months of a month having at least \\$900 of Medicare-paid dialysis\nclaims or at least one Medicare-paid inpatient claim.  In setting this\ncriterion, our aim is to achieve completeness of information on\nhospitalizations for all patients included in the years at risk.\"\n\n## Create some variables\n\nNot all variables used @grieco2017 are included here. Some\nvariables will need to be transformed to be comparable to what is in\nthe paper. For example, net investment in stations in year $t$ is the\ndifference between the number of stations in year $t+1$ and year in\n$t$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# sort data by :provfs, :year\n# function names that end with ! indicate that the function will\n# modify one (or more) of its inputs. In this case, sort! modifies the\n# dialysis DataFrame\nsort!(dialysis, (:provfs, :year))\n# things starting with : are Symbols. Names of variables within a\n# DataFrame must be Symbols, so they all start with :\n\n# we can access a single column of DataFrame by writing\n# dialysis[:stations] . This will be a 1 dimensional Array containing\n# of length equal to the number of rows in the dialysis DataFrame\n\n# panellag is a function defined in Dialysis.jl it creates lags and\n# leads of variables in panel data. It will insert missing values\n# where appropriate.\n\n# putting dialysis[:invest] on the left will create a new column in\n# the dialysis dataframe\ndialysis[:invest] = panellag(:stations, dialysis, :provfs, :year, -1) -\n  dialysis[:stations]; # ; prevents the notebook from printing the\n# output of the last command. Otherwise, notebooks display the output\n# of the last command in each cell."
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also create labor and hiring. Note that the choices of giving\n0.5 weight to part-time workers, including social workers, and\nweighting all types of staff equally are all somewhat arbitrary and\nmay not agree exactly with what @grieco2017 did."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "dialysis[:labor] = (dialysis[:nurseFT] + 0.5*dialysis[:nursePT]+\n                    dialysis[:ptcareFT] + 0.5*dialysis[:ptcarePT] +\n                    dialysis[:dieticiansFT] + 0.5*dialysis[:dieticiansPT] +\n                    dialysis[:social_workerFT] + 0.5*dialysis[:social_workerPT])\ndialysis[:hiring] = panellag(:labor, dialysis, :provfs, :year, -1) -\n  dialysis[:labor];"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating for profit and chain indicators."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# create a Boolean for profit indicator\ndialysis[:for_profit] = dialysis[:profit_status].==\"For Profit\"\n# The dot in .== is an example of broadcasting. It's very common to\n# want to apply the same function to all elements of an\n# array. Broadcasting does exactly this. If A is an array, and f() is\n# a function that operates on scalars, then f.(A) will apply f to each\n# element of A and return an array of the results. The above .==\n# compares each element of the array dialysis[:profit_status] to the\n# scalar string \"For Profit\"\n\n# similarly create indicators for the two largest chains\ndialysis[:fresenius] = dialysis[:chain_name].==\"FRESENIUS\"\ndialysis[:davita] = dialysis[:chain_name].==\"DAVITA\";"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "State inspection rates are a bit more complicated to create."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Statistics # for mean, std, and so on \n# first make an indicator for inspected in the last year\ndialysis[:inspected_this_year] =\n  ((dialysis[:days_since_inspection].>=0) .&\n   (dialysis[:days_since_inspection].<365))\n# then take the mean by state\nstateRates = by(dialysis, [:state, :year],\n                # by(data, :var, f) will apply f to each group of data\n                # with a different value of :var\n                df -> mean(skipmissing(df[:inspected_this_year])))\n# df -> mean(skipmissing(df[:inspected_this_year])) is a shorthand way\n# to define a function it's equalivant to\n#\n# function f(df)\n#   mean(skipmissing(df[:inspected_this_year]))\n# end\n#\n# skipmissing skips missing values inside a DataFrame. Most arithmetic\n# functions will not do what you want if missing values are included.\n\n# rename the variable in the stateRates DataFrame\nrename!(stateRates, :x1 => :state_inspection_rate)\n# merge the stateRates with the dialysis data\ndialysis = join(dialysis, stateRates, on = [:state, :year]);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the number of competitors in the same city is somewhat\nsimilar. Note that @grieco2017 use the number of competitors in the\nsame HSA, which would be preferrable. However, this dataset does not\ncontain information on HSAs. If you are feeling ambitious, you could\ntry to find data linking city, state to HSA, and use that to calculate\ncompetitors in the same HSA."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "dialysis[:city] = uppercase.(dialysis[:city]) \ncomps = by(dialysis,[:city,:year],\n           df -> mapreduce((x) -> ifelse(ismissing(x),0,1*(x>0)), +, df[:patient_months])\n           )\nrename!(comps, :x1 => :competitors)\ndialysis = join(dialysis, comps, on = [:city,:year]);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1: Summary statistics\n\nCreata a table (or multiple tables) similar to Tables 1-3 of\n@grieco2017. Comment on any notable differences. The following code\nwill help you get started."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Statistics\n\n# at the very least, you will need to change this list\nvars = [:patient_years_rom, :labor, :hiring]\n\n# You shouldn't neeed to change this function, but you can if you want\nfunction summaryTable(df, vars;\n                      funcs=[mean, std, x->length(collect(x))],\n                      colnames=[:Variable, :Mean, :StDev, :N])\n  # In case you want to search for information about the syntax used here, \n  # [XXX for XXX] is called a comprehension\n  # The ... is called the splat operator\n  DataFrame([vars [[f(skipmissing(df[v])) for v in vars] for f in funcs]...], colnames)  \nend\nsummaryTable(dialysis, vars)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2: exploratory figures\n\nCreate some figures to explore the data. Try to\nbe creative.  Are there any strange patterns or other obvious\nproblems with the data?\n\nHere are some examples to get started. You may want to look at the\nStatPlots.jl, Plots.jl, or VegaLite.jl github pages for more examples.\n\n### Comparing output measures"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using StatPlots , Plots\nPlots.gr(fmt=:png)\ndialysis[:patient_years] = dialysis[:patient_months]/12\n# missings will mess up corrplot\nvars = [:patient_years, :patient_years_hd, :patient_years_rom]\ninc = completecases(dialysis[vars])\n@df dialysis[inc,:] corrplot([:patient_years :patient_years_hd :patient_years_rom])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trends over time"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function yearPlot(var)\n  data = dialysis[completecases(dialysis[[:year, var]]),:]\n  scatter(data[:year], data[var], alpha=0.1, legend=:none,\n          markersize=3, markerstrokewidth=0.0)\n  yearmeans = by(data, :year,\n                 mean = var => x->mean(skipmissing(x)),\n                 q01  = var => x->quantile(skipmissing(x), 0.01),\n                 q10  = var => x->quantile(skipmissing(x), 0.1),\n                 q25  = var => x->quantile(skipmissing(x), 0.25),\n                 q50  = var => x->quantile(skipmissing(x), 0.50),\n                 q75  = var => x->quantile(skipmissing(x), 0.75),\n                 q90  = var => x->quantile(skipmissing(x), 0.9),\n                 q99  = var => x->quantile(skipmissing(x), 0.99))\n  @df yearmeans plot!(:year, :mean, colour = ^(:black), linewidth=4)\n  @df yearmeans plot!(:year, cols(3:ncol(yearmeans)),\n                      colour = ^(:red), alpha=0.3, legend=:none,\n                      xlabel=\"year\", ylabel=String(var))\nend\nyearPlot(:labor)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above plot shows a scatter of labor vs year. The black lines are\naverage labor each year. The red lines are the 0.01, 0.1, 0.25, 0.5,\n0.75, 0.9, and 0.99 quantiles conditional on year.\n\n# Part II: estimating the model\n\n## Problem 3: Quality measures\n\n@grieco2017 use the residuals from regressing the infection rate on\npatient characteristics as a measure of quality. Since the infection\nrate is a noisy measure of quality, they instrument with the\nstandardized mortality ratio as a second measure of quality. Medicare\ncollects the data we are using in part to create the \"Dialysis\nFacility Compare\" website, which is meant to allow consumers to\ncompare quality of dialysis facilities. Browsing around the [Dialysis\nFacility\nCompare](https://www.medicare.gov/dialysisfacilitycompare/#profile&pid=522543&pdist=2.0&loc=53129&lat=42.9388315&lng=-87.997164&sort=12%7CASC&dist=0&previouspage=results&profTab=2)\nor by looking at the first few pages of a [sample Dialysis Facility\nReport](https://data.cms.gov/Medicare/Sample-Dialysis-Facility-Report-for-Current-Year/82bq-h92z),\nyou will see that there are a number of other variables that Medicare\nconsiders indicators of quality. Pick one of these (it may or may not\nbe included in the extract of data I provided), and argue for or\nagainst using it instead of or in addition to the septic infection rate and\nstandardized mortality ratio. \n\nWe can construct residuals from an OLS regression as follows:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\n    ols_residuals(data::AbstractDataFrame, y::Symbol,\n                  x::Array{Symbol,1}; intecept::Bool=true)\n\nThis is a doc string. After executing this cell, if you type ?ols_residuals, you\nwill see this text. \n\nCalculate residuals from an OLS regression of data[y] on data[x]\n\nInputs:\n - `data` DataFrame containg y and x\n - `y` Symbol specifying y variable\n - `x` Symbol specifying x variables\n\nOutput:\n - Vector of residuals of length = nrow(data)\n\"\"\"\nfunction ols_residuals(data::DataFrame,  y::Symbol,\n                       x::Array{Symbol,1};\n                       # arguments following the \n                       # are optional\n                       intercept::Bool=true  \n                       )\n  # The :: are type specifications. They could be left out, and this\n  # function would still work just fine. One of their purposes are to\n  # document what inputs this function expects, and throw useful error\n  # messages if someone tries calling the function on the wrong types.\n\n  inc = completecases(data[[y, x...]]) # deal with missing\n  Y = disallowmissing(data[y][inc])\n  if (intercept) \n    X = [ones(sum(inc)) data[x][inc,:]]\n  else\n    X = data[x][inc,:]\n  end\n  X = disallowmissing(convert(Matrix, X))\n  \n  # you can type Greek and some other LaTeX characters by typing their LaTeX\n  # code followed by tab, e.g.  \\beta<TAB> and \\in<TAB>\n  Î² = X \\ Y # Î² âˆˆ argmin_b || X*b - Y || \n  Ïµ = Y - X*Î²  \n  if (any(.!inc)) # add back in missings\n    resid = Array{Union{Missing,eltype(Ïµ)},1}(undef, nrow(data))\n    resid .= missing\n    resid[inc] = Ïµ\n    return(resid)\n  else # no missing, just return Ïµ\n    return(Ïµ)\n  end\nend\nq = -ols_residuals(dialysis, :pct_septic, [:days_since_inspection,\n                                           :patient_age,\n                                           :pct_female,\n                                           :patient_esrd_years,\n                                           :pct_fistula,\n                                           :comorbidities,\n                                           :hemoglobin]);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, regression is common enough that there are already Julia\npackages for it. I included the `ols_residuals` only for pedagogical\npurposes. Whenever there exists a well-used package, it is (usually)\nbetter to use the package than try to write your own functions. Here's\nhow to accomplish the same thing using FixedEffectModels.jl."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using FixedEffectModels\n\ndialysis[:idcat] = categorical(dialysis[:provfs])\n# FixedEffectModels requires clustering and fixed effect variables to\n# be categorical\n\nqreg = reg(dialysis, @model(pct_septic ~ days_since_inspection + patient_age +\n                            pct_female + patient_esrd_years + pct_fistula + comorbidities +\n                            hemoglobin, vcov=cluster(idcat)),\n           save=true) # saves residuals in augmentdf\ndialysis[:quality] = -qreg.augmentdf[:residuals]\n\n# Let's test that these results are the same from ols_residuals\nprintln(\"Mean absolute difference = $(mean(skipmissing( abs.(q.- dialysis[:quality]) )))\") \n# using $(expr) in a string will insert the result of expr in the$\n# string \n\nusing Test\n@test all(skipmissing(q .â‰ˆ dialysis[:quality])) == true"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing is an important part of software development. The\n[Test.jl](https://docs.julialang.org/en/v1/stdlib/Test/index.html) \npackage provides help function for running tests. See [these notes from\n628](https://nbviewer.jupyter.org/github/ubcecon/ECON628_2018/blob/master/notebooks/testing.ipynb)\nfor more information about testing. \n\n## Problem 4: OLS and fixed effects estimates\n\nReproduce columns 2,3, 5, and 6 of Table 5. The syntax for fixed\neffects regression is shown below:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# you may want to use patient_years_hd or patient_years_rom instead\nlog_infmiss = x->ifelse(!ismissing(x) && x>0, log(x), missing) # -Inf confuses reg()\ndialysis[:lpy] = log_infmiss.(dialysis[:patient_months]./12)\ndialysis[:logL] = log_infmiss.(dialysis[:labor])\ndialysis[:logK] = log_infmiss.(dialysis[:stations])\n\n# you may want to restrict sample to match sample that can be used in model estimates \nreg(dialysis, @model(lpy ~ quality + logK + logL, fe = idcat, vcov =\n                     cluster(idcat)))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Be sure to add the other columns. If you'd like, you could use\n[RegressionTables.jl](https://github.com/jmboehm/RegressionTables.jl)\nto produce tables that look a lot like the ones in the paper. \n\n## Estimation of $\\alpha$\n\nAs discussed in section 5 of @grieco2017, the coefficient on quality,\n$\\alpha$, is estimated from \n$$\ny_{jt} = \\alpha q_{jt} + \\Phi(\\underbrace{h_{jt}, k_{jt}, l_{jt}, x_{jt}}_{w_{jt}}) +\n\\epsilon_{jt}\n$$\nwith a second noisy measure of quality, $z_{jt}$, used to instrument\nfor $\\alpha$. To estimate $\\alpha$, first the exogenous variables,\n$w$, can be partialed out to give:\n$$\ny_{jt} - \\Er[y_{jt} | w_{jt} ] = \\alpha (q_{jt} - \\Er[q_{jt}|w_{jt}]) +\n\\epsilon_{jt}\n$$\nwhere we used the assumption that $\\Er[\\epsilon_{jt} | w_{jt} ] = 0$\nand the fact that $\\Er[\\Phi(w) | w] = \\Phi(w)$. Under the assumption\nthat $\\Er[\\epsilon| z, w] = 0$, we can estimate $\\alpha$ based on the\nmoment condition:\n$$\n\\begin{align*}\n0 = & \\Er[\\epsilon f(z,w) ] \\\\\n0 = & \\Er\\left[ \\left(y_{jt} - \\Er[y_{jt} | w_{jt} ] - \\alpha\n(q_{jt} - \\Er[q_{jt}|w_{jt}])\\right) f(z_{jt},w_{jt}) \\right]\n\\end{align*}\n$$\nIf $Var(\\epsilon|z,w)$ is constant, the efficient choice of $f(z,w)$\nis \n$$\n\\Er[\\frac{\\partial \\epsilon}{\\partial \\alpha} |z, w ] = \\Er[q| z, w] - \\Er[q|w]\n$$\nTo estimate $\\alpha$, we simply replace these conditional expectations with\nregression estimates, and replace the unconditional expectation with a\nsample average. Let $\\hat{\\Er}[y|w]$ denote a nonparmetric estimate of\nthe regression of $y$ on $w$. Then, \n$$\n\\hat{\\alpha} = \\frac{\\sum_{j,t} (y_{jt} -\n\\hat{E}[y|w_{jt}])(\\hat{E}[q|z_{jt},w_{jt}] - \\hat{E}[q|w_{jt}])} \n{\\sum_{j,t} (q_{jt} - \\hat{E}[q|w_{jt}])(\\hat{E}[q|z_{jt},w_{jt}] - \\hat{E}[q|w_{jt}])} \n$$\nThe function `partiallinearIV` in Dialysis.jl will estimate this\nmodel. Also included are two methods for estimating\n$\\hat{E}[y|w]$. `polyreg` estimates a polynomial series regression,\nthat is it regresses $y$ on a polynomial of degree $d$ in $w$. To\nallow the regression to approximate any function, the degree must\nincrease with the sample size, but to control variance the degree must\nnot increase too quickly. We will not worry too much about the choice\nof degree here. \n\nAn alternative method (and what @grieco2017 used) is\nlocal linear regression. To estimate $\\hat{E}[y|x_{jt}]$, local linear\nregression estimates a linear regression of $y$ on $x$, but weights\nobservations by how close $x_{it}$ is to $x_{jt}$. That is, \n$$\n\\hat{E}[y|x_{jt}] = x_{jt} \\hat{\\beta}(x_jt) \n$$\nwhere \n$$\n\\hat{\\beta}(x_jt) = \\argmin_\\beta \\sum_{i,t} (y_{it} -\nx_{it}\\beta)^2 k((x_{it} - x_{jt})/h_n)\n$$\nHere $k()$ is some function with its maximum at 0 (and has some other\nproperties), like $k(x) \\propto e^{-x^2}$. The bandwidth, $h_n$,\ndetermines how much weight to place on observations close to vs far\nfrom $x_{jt}$. Similar to the degree in polynomial regression, the\nbandwidth must decrease toward 0 with sample size allow local linear\nregression to approximate any function, but to control variance the\nbandwidth must not decrease too quickly. We will not worry too much\nabout the choice of bandwidth. Anyway, the function `locallinear` in\nDialysis.jl estimates a local linear regression. \n\n## Problem 5: $\\alpha$\n\nEstimate $\\alpha$ using the following code. You may want to modify\nsome aspects of it and/or report estimates of $\\alpha$ for different\nchoices of instrument, nonparametric estimation method, degree or\nbandwidth. Compare your estimate(s) of $\\alpha$ with the ones in\nTables 5 and 6 of @grieco2017."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# create indicator for observations usable in estimation of Î±\ninc1 = ((dialysis[:patient_months] .> 0) .& (dialysis[:labor] .> 0) .&\n           (dialysis[:stations] .> 0) .&\n           .!ismissing.(dialysis[:quality]) .&\n           .!ismissing.(dialysis[:std_mortality]) .&\n           (dialysis[:invest].==0) .&\n           (dialysis[:hiring].!=0));\ninc1[ismissing.(inc1)] .= false;\ndialysis[:inc1] = inc1;\n\ndialysis[:lsmr] = log.(dialysis[:std_mortality] .+ .01)\n# As degree â†’ âˆž and/or bandwidth â†’ 0, whether we use :std_mortality or\n# some transformation as the instrument should not matter. However,\n# for fixed degree or bandwidth it will have some (hopefully small)\n# impact. \n\n(Î±, Î¦, Î±reg, eyqz)=partiallinearIV(:lpy,  # y \n                         :quality, # q\n                         :lsmr,   # z\n                         [:hiring, :logL, :logK,\n                         :state_inspection_rate, :competitors], # w\n                         dialysis[findall(dialysis[:inc1]),:];\n                         npregress=(xp, xd,yd)->polyreg(xp,xd,yd,degree=1),\n                         parts=true                      \n                         # You may want to change the degree here\n                         #\n                         # You could also change `polyreg`  to\n                         # `locallinear` and `degree` to\n                         # `bandwidthmultiplier`\n                         #\n                         # locallinear will likely take some time to\n                         # compute (â‰ˆ350 seconds on my computer)\n                         ) \n\n# we will need these later in step 2\ndialysis[:Î¦] = similar(dialysis[:lpy])\ndialysis[:Î¦] .= missing\ndialysis[:Î¦][findall(dialysis[:inc1])] = Î¦\ndialysis[:ey] = similar(dialysis[:lpy])\ndialysis[:ey] .= missing\ndialysis[:ey][findall(dialysis[:inc1])] = eyqz[:,1]\ndialysis[:eq] = similar(dialysis[:lpy])\ndialysis[:eq] .= missing\ndialysis[:eq][findall(dialysis[:inc1])] = eyqz[:,2]\ndialysis[:ez] = similar(dialysis[:lpy])\ndialysis[:ez] .= missing\ndialysis[:ez][findall(dialysis[:inc1])] = eyqz[:,3]\n\nÎ±"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Brief introduction to GMM\n\nThe coefficients on labor and capital are estimated by GMM. The idea\nof GMM is as follows. We have a model that implies \n$$\n\\Er[c(y,x;\\theta) | z ] = 0\n$$\nwhere $y$, $x$, and $z$ are observed variables. $c(y,x;\\theta)$ is\nsome known function of the data and some parameters we want to\nestimate, $\\theta$. Often, $c(y,x;\\theta)$ are the residuals from some\nequation. For example, for linear IV, we'd have\n$$ c(y,x;\\theta) = y - x\\theta $$ \nThe conditional moment restriction above implies that\n$$\n\\Er[c(y,x;\\theta)f(z) ] = 0\n$$\nfor any function $f()$. We can then estimate $\\theta$ by replacing the\npopulation expectation with a sample average and finding\n$\\hat{\\theta}$ such that\n$$\n\\En[c(y,x;\\hat{\\theta})f(z) ] \\approx 0\n$$\nThe dimension of $f(z)$ should be greater than or equal to the\ndimension of $\\theta$, so we have at least as many equations as\nunknowns. We find this $\\hat{\\theta}$ by minimizing a quadratic form\nof these equations. That is,\n$$\n\\hat{\\theta} = \\argmin_\\theta \\En[g_i(\\theta)] W_n \\En[g_i(\\theta)]'\n$$\nwere $g_i(\\theta) = c(y_i, x_i;\\theta)f(z_i)$, and $W_n$ is some\npositive definite weighting matrix. \n\n\n## Problem 6: OLS by GMM\n\nAs practice with GMM, use it to estimate a simple regression model, \n$$\ny = x\\beta + \\epsilon\n$$\nassuming $\\Er[\\epsilon|x] = 0$. Test your code on simulated data. The\nfollowing will help you get started."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function sim_ols(n; Î² = ones(3))\n  x = randn(n, length(Î²))\n  Ïµ = randn(n)\n  y = x*Î² + Ïµ\n  return(x,y)\nend\nÎ² = ones(2)\n(x, y) = sim_ols(100; Î²=Î²)\nÎ²ols = (x'*x) \\ (x'*y)\n\nfunction gmm_objective(Î²)\n  gi = (y - x*Î²) .* x\n  Egi = mean(gi, dims=1)\n  error(\"This is incomplete; you must finish it\")\n\n  # It is is likely that the code you will write will return a 1 x 1,\n  # 2 dimensional array. For compatibility with Optim, you need to\n  # return a scalar. If foo is a 1x1 array, write `foo[1]` to return a scalar instead of\n  # 1x1 array\nend\n\n# minimizer gmm_objective\nusing Optim # github page : https://github.com/JuliaNLSolvers/Optim.jl\n# docs : http://julianlsolvers.github.io/Optim.jl/stable/\ntry \n  res = optimize(gmm_objective,\n                 zeros(size(Î²)), # initial value\n                 BFGS(), # algorithm, see http://julianlsolvers.github.io/Optim.jl/stable/\n                 autodiff=:forward)\n  Î²gmm = res.minimizer\ncatch err\n  @info err\n  Î²gmm = Î²ols\n  res = nothing\nend\n@test Î²gmm â‰ˆ Î²ols\nres"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimating $\\beta$\n\nThe model\nimplies that \n$$\n\\omega_{jt} = \\Phi(w_{jt}) - \\beta_k k_{jt} - \\beta_l l_{jt} \n$$ {#eq:omega}\nand \n$$\ny_{jt} - \\alpha q_{jt} - \\beta_k k_{jt} - \\beta_l l_{jt} =\ng(\\omega_{jt-1}) + \\eta_{jt} \n$$ {#eq:eta}\nThe timing and exogeneity assumptions imply that \n$$\n\\Er[\\eta_{jt} | k_{jt}, l_{jt}, w_{jt-1}]\n$$\nGiven a value of $\\beta$, and our above estimates of $\\Phi$ and\n$\\alpha$, we can compute $\\omega$ from @eq:omega, and then estimate\n$g()$ and $\\eta$ by a nonparametric regression of\n$y_{jt} - \\alpha q_{jt} - \\beta_k k_{jt} - \\beta_l l_{jt}$ on\n$\\omega_{jt-1}$. $\\beta$ can then be estimated by finding the value of\n$\\beta$ that comes closest to satisfying the moment condition\n$$\n\\Er[\\eta(\\beta)_{jt} k_{jt}] = 0 \\text{ and } \\Er[\\eta(\\beta)_{jt} l_{jt}]\n= 0\n$$\nTo do this, we minimize \n$$\nQ_n(\\beta) = \\left( \\frac{1}{N} \\sum_{j,t} \\eta(\\beta)_{jt} (k_{jt}, l_{jt}) \\right) W_n \n\\left( \\frac{1}{N} \\sum_{j,t} \\eta(\\beta)_{jt} (k_{jt}, l_{jt}) \\right)'\n$$\n\n\n## Problem 7: estimate $\\beta$\n\nWrite the body of the $Q_n(\\beta)$ function below. Use it to estimate\n$\\beta$. Compare your results with those of @grieco2017. Optionally,\nexplore robustness of your results to changes in the specification."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# indicator for observations usable in estimation of Î²\ndialysis[:inclag] = panellag(:inc1, dialysis, :provfs, :year, 1);\ndialysis[:inc2] = (dialysis[:inclag] .&\n                   (dialysis[:stations].>0) .&\n                   (dialysis[:labor].>0) .&\n                   (dialysis[:patient_years].>0) .&\n                   .!ismissing.(dialysis[:quality]));\ndialysis[:inc2][ismissing.(dialysis[:inc2])] .= false;\n\n(Ï‰func, Î·func) = errors_gm(:lpy, :logK, :logL, :quality, :Î¦, :provfs, :year,\n                           dialysis, Î±; degree=1)\nfunction Qn(Î²)\n  Î· = Î·func(Î²)\n  error(\"You must write the body of this function\")\nend\n\nusing Optim\ntry \n  res = optimize(Qn,    # objective\n                 [0.0, 0.0], # lower bounds, should not be needed, but\n                 # may help numerical performance \n                 [1.0, 1.0], # upper bounds \n                 [0.4, 0.2], # initial value               \n                 Fminbox(BFGS()),  # algorithm\n                 autodiff=:forward)\n  Î² = res.minimizer\n  @show Î²\ncatch err\n  @info err\n  res = nothing\n  Î² = nothing\nend\nres"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n\nMany, perhaps most, estimators in econometrics are\nextrumem estimators. That is, many estimators are defined by\n$$\n\\hat{\\theta} = \\argmax_{\\theta \\in \\Theta} \\hat{Q}_n(\\theta)\n$$\nwhere $\\hat{Q}_n(\\theta)$ is some objective function that depends on\ndata. Examples include maximum likelihood,\n$$\n\\hat{Q}_n(\\theta) = \\frac{1}{n} \\sum_{i=1}^n f(z_i | \\theta)\n$$\nnonlinear least squares,\n$$\n\\hat{Q}_n(\\theta) = \\frac{1}{n} \\sum_{i=1}^n (y_i - h(x_i,\\theta))^2\n$$\nand as we are using for this example, GMM,\n$$\n\\hat{Q}_n(\\theta) = \\left(\\frac{1}{n} \\sum_{i=1}^n g(z_i,\n\\theta)\\right)' \\hat{W} \\left(\\frac{1}{n} \\sum_{i=1}^n g(z_i,\n\\theta)\\right).\n$$\nSee @newey1994 for more details and examples.\n\nWe will encounter extremum estimators often in this course, so it is\nuseful to be familiar with their statistical properties. However,\nsince this course is not focused on econometrics, we will just state\nsome basic \"high-level\" conditions for consistency and asymptotic\nnormality, and only give brief sketches of proofs. Our main goal is to find\nthe asymptotic distribution of $\\hat{\\theta}$, so that we can report\nstandard errors and confidence regions. If $Q_n$ is differentiable,\nthen \n$$\n  0 = \\nabla \\hat{Q}_n(\\hat{\\theta})\n$$\nTaking a first order expansion around $\\theta_0$, \n$$\n  0 \\approx \\nabla \\hat{Q}_n(\\theta_0) + (\\hat{\\theta}-\\theta_0) \\nabla^2 Q_n(\\theta_0)\n$$\nRearranging and multiplying by $\\sqrt{n}$, gives\n$$\n\\sqrt{n}(\\hat{\\theta}-\\theta_0) \\approx (\\nabla^2 Q_n(\\theta_0))^{-1}\n\\sqrt{n} \\hat{Q}_n(\\theta_0)\n$$\nIf a law of large number applies to $\\nabla^2 Q_n(\\theta_0)$ and a\ncentral limit theorem applies to $\\sqrt{n} \\hat{Q}_n(\\theta_0)$, then\n$\\sqrt{n}(\\hat{\\theta}-\\theta_0)$ will be asymptotically normal. The\nfollowing theorem states this idea somewhat more precisely.\n\n### Consistency\n\n**Consistency for extremum estimators**: assume\n\n1. $\\hat{Q}_n(\\theta)$ converges uniformly in probability to\n   $Q_0(\\theta)$\n\n2. $Q_0(\\theta)$ is uniquely maximized at $\\theta_0$.\n\n3. $\\Theta$ is compact and $Q_0(\\theta)$ is continuous.\n\nThen $\\hat{\\theta} \\inprob \\theta_0$\n</div>\n\n\n### Asymptotic normality\n\n**Asymptotic normality for extremum estimators**: assume\n\n1. $\\hat{\\theta} \\inprob \\theta_0$\n\n2. $\\theta_0 \\in interior(\\Theta)$\n\n3. $\\hat{Q}_n(\\theta)$ is twice continuously differentiable in open $N$\n   containing $\\theta$, and $\\sup_{\\theta \\in N} \\Vert \\nabla^2\n                             \\hat{Q}_n(\\theta) - H(\\theta) \\Vert\n                             \\inprob 0$ with $H(\\theta_0)$ nonsingular\n4. $\\sqrt{n} \\nabla \\hat{Q}_n(\\theta_0) \\indist N(0,\\Sigma)$\n\nThen $\\sqrt{n} (\\hat{\\theta} - \\theta_0) \\indist N\\left(0,H^{-1} \\Sigma\n  H^{-1} \\right)$\n\n### GMM \n\nFor a GMM objective function of the form:\n$$ [1/n \\sum_i g_i(\\theta)] W_n [1/n \\sum g_i(\\theta)]$$, \nif we assume:\n\n1. $1/\\sqrt{n} \\sum_i g_i(\\theta_0) \\indist N(0,\\Sigma)$\n\n2. $1/n \\sum_i \\nabla g_i(\\theta) \\inprob E[\\nabla g(\\theta)] = D$, \n   $W_n \\inprob W$\n\n3. $(D'WD)$ is nonsingular.\n\nthen the above theorem for asymptotic normality of extremum\nestimators implies that \n$$\n\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\indist N(0,\\Omega)\n$$\nwhere \n$$\n \\Omega= (D'WD)^{-1} (D' W \\Sigma W D) (D'WD)^{-1}.\n$$\nIf we additionally assume $W_n \\inprob \\Sigma^{-1}$, e.g. observations\nare independent and $W_n =\n\\widehat{Var}(g_i(\\theta))^{-1}$, then the asymptotic variance\nsimplifies to $(D' \\Sigma D)^{-1}$. This choice of $W$ is efficient in\nthat it leads to the smallest asymptotic variance. \n\n### 2-step estimators\n\nThe above applies to estimators that come from minimizing a single\nobjective function. This application involves a multi-step\nestimator. First, we estimated $\\alpha$ and $\\Phi()$, then we\nestimated $\\beta$ by GMM using moments of the form:\n$$\n\\hat{\\beta} = \\argmin [1/n \\sum_i g_i(\\beta, \\hat{\\alpha},\n \\hat{\\Phi})] W_n [1/n \\sum g_i(\\beta, \\hat{\\alpha}, \\hat{\\Phi}) ] \n$$\nA similar expansion as above will give\n$$\n\\sqrt{n}(\\hat{\\beta} - \\beta_0) \\approx -(D'WD)^{-1} D' W \\left(1/\\sqrt{n}\n\\sum g_i(\\beta, \\hat{\\alpha}, \\hat{\\Phi}) \\right)\n$$\nwhere $D=\\Er[\\nabla_\\beta g(\\beta, \\alpha, \\Phi)]$. If we also expand\n$g_i$ in terms of $\\alpha$ and $\\Phi$, we will get, \n$$\n\\sqrt{n}(\\hat{\\beta} - \\beta_0) \\approx -(D'WD)^{-1} D' W \\left(1/\\sqrt{n}\n\\sum g_i(\\beta_0, \\alpha_0, \\Phi_0) \\right) + \\Er[\\frac{\\partial g_i}{\\partial\n\\alpha}(\\beta,\\alpha,\\Phi)] \\sqrt{n}(\\hat{\\alpha}-\\alpha_0) + \\Er[\\frac{\\partial g_i}{\\partial\n\\Phi}(\\beta,\\alpha,\\Phi) \\sqrt{n}(\\hat{\\Phi}-\\Phi_0) ]. \n$$\nSo, there will be additional variance in $\\hat{\\beta}$ from the\nestimation of $\\hat{\\alpha}$ and $\\hat{\\Phi}$. Since $\\alpha$ is\nfinite dimensional, it is not too difficult to derive the distribution\nof $\\sqrt{n}(\\hat{\\alpha}-\\alpha_0)$ and estimate $\\Er[\\frac{\\partial g_i}{\\partial\n\\alpha}(\\beta,\\alpha,\\Phi)]$. However, $\\hat{\\Phi}$ is a function and\nis more difficult to deal with. Under some strong assumptions, \n$$\n \\Er[\\frac{\\partial g_i}{\\partial\n\\Phi}(\\beta,\\alpha,\\Phi) \\sqrt{n}(\\hat{\\Phi}-\\Phi_0) ] \\indist N,\n$$\nbut the needed assumptions are slightly restrictive and are tedious to\nstate. An alternative approach is to redefine $g_i$ to ensure that\n$E[\\frac{\\partial g_i}{\\partial \\Phi}] = 0$. This can be done by\nletting \n$$\n\\begin{align*}\n   g_{jt}(\\beta, \\alpha, \\Phi) = & \\left(y_{jt} - \\alpha q_{jt} -\n   x_{jt} \\beta - h(\\Phi(w_{jt-1}) -\n   x_{jt-1}\\beta) \\right) \\left(x_{jt} -\n   \\Er[x|\\Phi(w_{jt-1}) - x_{jt-1}\\beta]\\right) + \\\\\n   & - \\left(y_{jt-1} - \\alpha\n   q_{jt-1} - \\Phi(w_{jt-1})\\right) h'(\\Phi(w_{jt-1}) - x_{jt-1}\\beta) \\left(x_{jt} -\n   \\Er[x|\\Phi(w_{jt-1}) - x_{jt-1}\\beta]\\right)\n\\end{align*}\n$$\nwhere $x = (k, l)$, and \n$h(\\Phi(w_{jt-1}) - x_{jt-1}) =  \\Er[y_{jt} - \\alpha q_{jt} - x_{jt}\n \\beta |\\Phi(w_{jt-1}) - x_{jt-1}\\beta]$. It is not too difficult to\nverify that \n$$\n\\Er[g_{jt}(\\beta_0,\\alpha_0,\\Phi_0)] = 0\n$$\nand \n$$\n0 = D_\\Phi \\Er[g_{jt}(\\beta_0,\\alpha_0,\\Phi_0)]\\;\\;,\\;\\; 0 = D_h \\Er[g_{jt}(\\beta_0,\\alpha_0,\\Phi_0)] \n$$\nIn other words, these moment conditions are orthogonal in the sense of\n@chernozhukov2018. Estimation error in $\\Phi$ and $h$ only has second\norder effects on the estimate of $\\beta$. Under appropriate\nassumptions, these second order effects will vanish quickly enough\nthat they can be ignored in the asymptotic distribution of\n$\\hat{\\beta}$. \n\nWe can similarly deal with the uncertainty in $\\hat{\\alpha}$ by\nredefining the moment condition to be orthogonal with respect to\n$\\alpha$. Let\n$$\n\\tilde{g}_{jt}(\\beta,\\alpha,\\Phi) = g_{jt}(\\beta,\n\\alpha, \\Phi) - \\Er[ D_\\alpha g_{jt}(\\beta_0,\n\\alpha, \\Phi)] \\frac{(y_{jt} - \\alpha q_{jt} - \\Phi(w_{jt}))\n(\\Er[q|z_{jt},w_{jt}] - \\Er[q|w_{jt}])}{\\Er[(q -\n\\Er[q|w])(\\Er[q|z,w]-\\Er[q|w])]}.\n$$\nThen $\\Er[\\tilde{g}_{jt}(\\beta_0,\\alpha_0,\\Phi_0)] = 0$ and \n$$\n0 = D_\\Phi \\Er[\\tilde{g}_{jt}(\\beta_0,\\alpha_0,\\Phi_0)]\\;\\;,\\;\\; 0 = D_\\alpha\n\\Er[\\tilde{g}_{jt}(\\beta_0,\\alpha_0,\\Phi_0)].\n$$\nHence, \n$$\n\\begin{align*}\n\\frac{1}{\\sqrt{n}} \\sum_{j,t} \\tilde{g}_{jt}(\\beta_0, \\hat{\\alpha}, \\hat{\\Phi} )\n= & \\frac{1}{\\sqrt{n}} \\sum_{j,t} \\tilde{g}_{jt}(\\beta_0, \\alpha_0,\n\\Phi_0 ) + o_p(1) \n\\end{align*}\n$$\n\nUnder appropriate assumptions a CLT will apply to $\\tilde{g}_{jt}$, so \n$$\n\\frac{1}{\\sqrt{n}}  \\sum_{j,t} \\tilde{g}_{jt}(\\hat{\\beta}, \\hat{\\alpha},\n\\hat{\\Phi} ) = \\frac{1}{\\sqrt{n}}  \\sum_{j,t} \\tilde{g}_{jt}(\\beta_0,\\alpha_0,\\Phi_0) + o_p(1)\n\\indist N(0,\\Sigma)\n$$\nFurthermore, $\\Sigma$ can estimated by taking the sample (clustered)\ncovariance of\n$\\tilde{g}_{jt}(\\beta,\\hat{\\alpha},\\hat{\\Phi})$. Denote this by\n$\\hat{\\Sigma}(\\beta)$. We then have\n$$\n    Q_n^{CUE}(\\beta_0) = \\left(\\frac{1}{n}  \\sum_{j,t} \\tilde{g}_{jt}(\\beta_0, \\hat{\\alpha},\n    \\hat{\\Phi} ) \\right)' \\hat{\\Sigma(\\beta_0)}^{-1} \\left(\\frac{1}{n}\n    \\sum_{j,t} \\tilde{g}_{jt}(\\beta_0, \\hat{\\alpha}, \\hat{\\Phi} )\n    \\right) \\indist \\chi^2_{dim(\\tilde{g}_{jt})}.\n$$\nThis can be used to test $H_0: \\beta = \\beta_0$, or to form a\nconfidence region by taking all values of $\\beta$ for which the test\nfails to reject. Such an inference procedure is robust to\nidentification problems in that it does not require an assumption\nabout $D = D_\\beta \\Er[g_{jt}(\\beta,\\alpha_0,\\Phi_0)]$ being full rank or $Q_n$ being uniquely minimized\nat $\\beta_0$. If we are willing to assume strong identification in that \n$(D' \\Sigma(\\beta_0)^{-1} D)$ is invertible (and, loosely speaking, the estimated\nversion of this is far from singular relative to its estimation\nerror), then we can take an expansion to get \n$$\n\\sqrt{n}(\\hat{\\beta} - \\beta_0) = (D'\\Sigma^{-1} D)^{-1} D'\\Sigma^{-1}\n\\frac{1}{\\sqrt{n}} \\tilde{g}_{jt}(\\beta_0, \\alpha_0, \\Phi_0) + o_p(1)\n\\indist N(0, (D' \\Sigma^{-1} D)^{-1}).\n$$\n                                              \n\nSee my notes from 628 on [extremum estimation](http://faculty.arts.ubc.ca/pschrimpf/628/extremumEstimation.html)\nand on\n[bootstrap](http://faculty.arts.ubc.ca/pschrimpf/628/bootstrap.html)\nfor more details.\n\nThe `objective_gm` function in Dialysis.jl constructs the $Q_n^{CUE}$\nobjective function described above. The following code minimizes it\nand plots a confidence region for $\\beta$ based on inverting the\n$\\chi^2$ test described above."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "(obj, momenti, cue, varfn) = objective_gm(:lpy, :logK, :logL, :quality, :Î¦,\n                                          :provfs, :year,\n                                          [:logK, :logL], dialysis,\n                                          clusterid=:idcat,\n                                          npregress=(xp,xd,yd)->polyreg(xp,xd,yd,degree=1,\n                                                                        deriv=true));\nres = optimize(b->cue(b,Î±),    # objective\n               [0.0, 0.0], # lower bounds, should not be needed, but\n               # may help numerical performance \n               [1.0, 1.0], # upper bounds \n               [0.4, 0.2], # initial value               \n               Fminbox(BFGS()),  # algorithm\n               autodiff=:forward)\nres\n# calculate standard errors based on normal approximation\nÎ²hat = res.minimizer\ngi = momenti(Î²hat,Î±)\n(Î£, n) = varfn(gi)\nusing ForwardDiff\nusing LinearAlgebra # for diag\nD = ForwardDiff.jacobian(Î²->mean(momenti(Î²,Î±), dims=1), Î²hat)\nVÎ² = inv(D'*inv(Î£)*D)/n\n@show Î²hat\n@show sqrt.(diag(VÎ²))\nnothing;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Plots, Distributions\nPlots.gr()\nlb = [0., 0.]\nub = [1.,  1.]\nntest = 200\nÎ²test = [rand(2).*(ub-lb) .+ lb for i in 1:ntest];\nÎ²test = vcat(Î²test'...)\n\nfn = Î²->cue(Î²,Î±)\npval = Vector{Float64}(undef, ntest);\nqval = similar(pval);\nThreads.@threads for i in 1:size(Î²test,1)\n  qval[i] = fn(Î²test[i,:]);\n  pval[i] = cdf(Chisq(2),qval[i]);\nend\n\ncrit = 0.9\nfig=scatter(Î²test[:,1],Î²test[:,2], group=(pval.<crit), legend=false,\n            markersize=4, markerstrokewidth=0.0, seriesalpha=1.0,\n            palette=:isolum, xlabel=\"Î²k\", ylabel=\"Î²l\")\nngrid = 20\nb = range.(lb,ub, length=ngrid)\npc = Matrix{Float64}(undef, length(b[1]), length(b[2]))\nqc = similar(pc)\nThreads.@threads for i in CartesianIndices(pc)\n  qc[i] = fn([b[1][i[1]],b[2][i[2]]]);\n  pc[i] = cdf(Chisq(2),qc[i]);\nend\n\nfig=contour!(b[1],b[2],pc',\n            levels = [0.75, 0.9, 0.95, 0.99],\n             contour_labels=true)\ndisplay(fig)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we plot $Q^{CUE}_n(\\beta)$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Plots.plotly()\nsurface(b[1],b[2],qc', xlabel=\"Î²k\", ylabel=\"Î²l\")"
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.0.3"
    },
    "kernelspec": {
      "name": "julia-1.0",
      "display_name": "Julia 1.0.3",
      "language": "julia"
    }
  },
  "nbformat": 4
}
