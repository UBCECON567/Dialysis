<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Paul Schrimpf">
        
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>Inference -  </title>
        <link href="../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../css/font-awesome.min.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atelier-forest-light.min.css">
        <link href="../assets/Documenter.css" rel="stylesheet">
        <link href="../assets/extra.css" rel="stylesheet">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

        <script src="../js/jquery-1.10.2.min.js" defer></script>
        <script src="../js/bootstrap-3.0.3.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href=".."> </a>
                </div>

                <!-- Expanded navigation -->
                <div class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li >
                                <a href="..">Function Reference</a>
                            </li>
                            <li >
                                <a href="../dialysis/">Assignment</a>
                            </li>
                            <li class="active">
                                <a href="./">Inference</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">About <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../license/">License</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li >
                                <a rel="next" href="../dialysis/">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li >
                                <a rel="prev" href="../license/">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li>
                                <a href="https://github.com/UBCECON567/Dialysis"><i class="fa fa-github"></i> GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#inference-for-grieco-mcdevitt-replication">Inference for Grieco &amp; McDevitt Replication</a></li>
            <li><a href="#consistency">Consistency</a></li>
            <li><a href="#asymptotic-normality">Asymptotic normality</a></li>
            <li><a href="#gmm">GMM</a></li>
            <li><a href="#2-step-estimators">2-step estimators</a></li>
        <li class="main "><a href="#results">Results</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<p><a href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a></p>
<p>This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike
4.0 International
License</a></p>
<p>
<script type="math/tex; mode=display">
\def\indep{\perp\!\!\!\perp}
\def\Er{\mathrm{E}}
\def\R{\mathbb{R}}
\def\En{{\mathbb{E}_n}}
\def\Pr{\mathrm{P}}
\newcommand{\norm}[1]{\left\Vert {#1} \right\Vert}
\newcommand{\abs}[1]{\left\vert {#1} \right\vert}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\def\inprob{\,{\buildrel p \over \rightarrow}\,} 
\def\indist{\,{\buildrel d \over \rightarrow}\,} 
</script>
</p>
<p>This document discusses inference for the two-step production function
estimator in Grieco &amp; McDevitt.</p>
<h1 id="inference-for-grieco-mcdevitt-replication">Inference for Grieco &amp; McDevitt Replication<a class="headerlink" href="#inference-for-grieco-mcdevitt-replication" title="Permanent link">&para;</a></h1>
<p>Many, perhaps most, estimators in econometrics are extrumem estimators.
That is, many estimators are defined by <script type="math/tex; mode=display">
\hat{\theta} = \argmax_{\theta \in \Theta} \hat{Q}_n(\theta)
</script> where $\hat{Q}_n(\theta)$ is some objective function that depends on
data. Examples include maximum likelihood, <script type="math/tex; mode=display">
\hat{Q}_n(\theta) = \frac{1}{n} \sum_{i=1}^n f(z_i | \theta)
</script> nonlinear least squares, <script type="math/tex; mode=display">
\hat{Q}_n(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - h(x_i,\theta))^2
</script> and as we are using for this example, GMM, <script type="math/tex; mode=display">
\hat{Q}_n(\theta) = \left(\frac{1}{n} \sum_{i=1}^n g(z_i,
\theta)\right)' \hat{W} \left(\frac{1}{n} \sum_{i=1}^n g(z_i,
\theta)\right).
</script> See Newey and McFadden (<a href="#ref-newey1994">1994</a>) for more details and
examples.</p>
<p>We will encounter extremum estimators often in this course, so it is
useful to be familiar with their statistical properties. However, since
this course is not focused on econometrics, we will just state some
basic “high-level” conditions for consistency and asymptotic normality,
and only give brief sketches of proofs. Our main goal is to find the
asymptotic distribution of $\hat{\theta}$, so that we can report
standard errors and confidence regions. If $Q_n$ is differentiable, then
<script type="math/tex; mode=display">
  0 = \nabla \hat{Q}_n(\hat{\theta})
</script> Taking a first order expansion around $\theta_0$, <script type="math/tex; mode=display">
  0 \approx \nabla \hat{Q}_n(\theta_0) + (\hat{\theta}-\theta_0) \nabla^2 Q_n(\theta_0)
</script> Rearranging and multiplying by $\sqrt{n}$, gives <script type="math/tex; mode=display">
\sqrt{n}(\hat{\theta}-\theta_0) \approx (\nabla^2 Q_n(\theta_0))^{-1}
\sqrt{n} \hat{Q}_n(\theta_0)
</script> If a law of large number applies to $\nabla^2 Q_n(\theta_0)$ and a
central limit theorem applies to $\sqrt{n} \hat{Q}_n(\theta_0)$, then
$\sqrt{n}(\hat{\theta}-\theta_0)$ will be asymptotically normal. The
following theorem states this idea somewhat more precisely.</p>
<h3 id="consistency">Consistency<a class="headerlink" href="#consistency" title="Permanent link">&para;</a></h3>
<p><strong>Consistency for extremum estimators</strong>: assume</p>
<ol>
<li>
<p>$\hat{Q}_n(\theta)$ converges uniformly in probability to
    $Q_0(\theta)$</p>
</li>
<li>
<p>$Q_0(\theta)$ is uniquely maximized at $\theta_0$.</p>
</li>
<li>
<p>$\Theta$ is compact and $Q_0(\theta)$ is continuous.</p>
</li>
</ol>
<p>Then $\hat{\theta} \inprob \theta_0$</p>
<h3 id="asymptotic-normality">Asymptotic normality<a class="headerlink" href="#asymptotic-normality" title="Permanent link">&para;</a></h3>
<p><strong>Asymptotic normality for extremum estimators</strong>: assume</p>
<ol>
<li>
<p>$\hat{\theta} \inprob \theta_0$</p>
</li>
<li>
<p>$\theta_0 \in interior(\Theta)$</p>
</li>
<li>
<p>$\hat{Q}<em N="N" _in="\in" _theta="\theta">n(\theta)$ is twice continuously differentiable in open $N$
    containing $\theta$, and
    $\sup</em> \Vert \nabla^2  \hat{Q}_n(\theta) - H(\theta) \Vert  \inprob 0$
    with $H(\theta_0)$ nonsingular</p>
</li>
<li>
<p>$\sqrt{n} \nabla \hat{Q}_n(\theta_0) \indist N(0,\Sigma)$</p>
</li>
</ol>
<p>Then
$\sqrt{n} (\hat{\theta} - \theta_0) \indist N\left(0,H^{-1} \Sigma  H^{-1} \right)$</p>
<h3 id="gmm">GMM<a class="headerlink" href="#gmm" title="Permanent link">&para;</a></h3>
<p>For a GMM objective function of the form:
<script type="math/tex; mode=display"> [1/n \sum_i g_i(\theta)] W_n [1/n \sum g_i(\theta)]</script>, if we assume:</p>
<ol>
<li>
<p>$1/\sqrt{n} \sum_i g_i(\theta_0) \indist N(0,\Sigma)$</p>
</li>
<li>
<p>$1/n \sum_i \nabla g_i(\theta) \inprob E[\nabla g(\theta)] = D$,
    $W_n \inprob W$</p>
</li>
<li>
<p>$(D&rsquo;WD)$ is nonsingular.</p>
</li>
</ol>
<p>then the above theorem for asymptotic normality of extremum estimators
implies that <script type="math/tex; mode=display">
\sqrt{n}(\hat{\theta} - \theta_0) \indist N(0,\Omega)
</script> where <script type="math/tex; mode=display">
 \Omega= (D'WD)^{-1} (D' W \Sigma W D) (D'WD)^{-1}.
</script> If we additionally assume $W_n \inprob \Sigma^{-1}$,
e.g. observations are independent and
$W_n = \widehat{Var}(g_i(\theta))^{-1}$, then the asymptotic variance
simplifies to $(D&rsquo; \Sigma D)^{-1}$. This choice of $W$ is efficient in
that it leads to the smallest asymptotic variance.</p>
<h3 id="2-step-estimators">2-step estimators<a class="headerlink" href="#2-step-estimators" title="Permanent link">&para;</a></h3>
<p>The above applies to estimators that come from minimizing a single
objective function. This application involves a multi-step estimator.
First, we estimated $\alpha$ and $\Phi()$, then we estimated $\beta$ by
GMM using moments of the form: <script type="math/tex; mode=display">
\hat{\beta} = \argmin [1/n \sum_i g_i(\beta, \hat{\alpha},
 \hat{\Phi})] W_n [1/n \sum g_i(\beta, \hat{\alpha}, \hat{\Phi}) ] 
</script> A similar expansion as above will give <script type="math/tex; mode=display">
\sqrt{n}(\hat{\beta} - \beta_0) \approx -(D'WD)^{-1} D' W \left(1/\sqrt{n}
\sum g_i(\beta, \hat{\alpha}, \hat{\Phi}) \right)
</script> where $D=\Er[\nabla_\beta g(\beta, \alpha, \Phi)]$. If we also expand
$g_i$ in terms of $\alpha$ and $\Phi$, we will get, <script type="math/tex; mode=display">
\sqrt{n}(\hat{\beta} - \beta_0) \approx -(D'WD)^{-1} D' W \left(1/\sqrt{n}
\sum g_i(\beta_0, \alpha_0, \Phi_0) \right) + \Er[\frac{\partial g_i}{\partial
\alpha}(\beta,\alpha,\Phi)] \sqrt{n}(\hat{\alpha}-\alpha_0) + \Er[\frac{\partial g_i}{\partial
\Phi}(\beta,\alpha,\Phi) \sqrt{n}(\hat{\Phi}-\Phi_0) ]. 
</script> So, there will be additional variance in $\hat{\beta}$ from the
estimation of $\hat{\alpha}$ and $\hat{\Phi}$. Since $\alpha$ is finite
dimensional, it is not too difficult to derive the distribution of
$\sqrt{n}(\hat{\alpha}-\alpha_0)$ and estimate
$\Er[\frac{\partial g_i}{\partial \alpha}(\beta,\alpha,\Phi)]$. However,
$\hat{\Phi}$ is a function and is more difficult to deal with. Under
some strong assumptions, <script type="math/tex; mode=display">
 \Er[\frac{\partial g_i}{\partial
\Phi}(\beta,\alpha,\Phi) \sqrt{n}(\hat{\Phi}-\Phi_0) ] \indist N,
</script> but the needed assumptions are slightly restrictive and are tedious
to state. An alternative approach is to redefine $g_i$ to ensure that
$E[\frac{\partial g_i}{\partial \Phi}] = 0$. This can be done by letting
<script type="math/tex; mode=display">
\begin{align*}
   g_{jt}(\beta, \alpha, \Phi) = & \left(y_{jt} - \alpha q_{jt} -
   x_{jt} \beta - h(\Phi(w_{jt-1}) -
   x_{jt-1}\beta) \right) \left(x_{jt} -
   \Er[x|\Phi(w_{jt-1}) - x_{jt-1}\beta]\right) + \\
   & - \left(y_{jt-1} - \alpha
   q_{jt-1} - \Phi(w_{jt-1})\right) h'(\Phi(w_{jt-1}) - x_{jt-1}\beta) \left(x_{jt} -
   \Er[x|\Phi(w_{jt-1}) - x_{jt-1}\beta]\right)
\end{align*}
</script> where $x = (k, l)$, and
$h(\Phi(w_{jt-1}) - x_{jt-1}) = \Er[y_{jt} - \alpha q_{jt} - x_{jt}  \beta |\Phi(w_{jt-1}) - x_{jt-1}\beta]$.
It is not too difficult to verify that <script type="math/tex; mode=display">
\Er[g_{jt}(\beta_0,\alpha_0,\Phi_0)] = 0
</script> and <script type="math/tex; mode=display">
0 = D_\Phi \Er[g_{jt}(\beta_0,\alpha_0,\Phi_0)]\;\;,\;\; 0 = D_h \Er[g_{jt}(\beta_0,\alpha_0,\Phi_0)] 
</script> In other words, these moment conditions are orthogonal in the sense
of Chernozhukov et al. (<a href="#ref-chernozhukov2018">2018</a>). Estimation error
in $\Phi$ and $h$ only has second order effects on the estimate of
$\beta$. Under appropriate assumptions, these second order effects will
vanish quickly enough that they can be ignored in the asymptotic
distribution of $\hat{\beta}$.</p>
<p>We can similarly deal with the uncertainty in $\hat{\alpha}$ by
redefining the moment condition to be orthogonal with respect to
$\alpha$. Let <script type="math/tex; mode=display">
\tilde{g}_{jt}(\beta,\alpha,\Phi) = g_{jt}(\beta,
\alpha, \Phi) - \Er[ D_\alpha g_{jt}(\beta_0,
\alpha, \Phi)] \frac{(y_{jt} - \alpha q_{jt} - \Phi(w_{jt}))
(\Er[q|z_{jt},w_{jt}] - \Er[q|w_{jt}])}{\Er[(q -
\Er[q|w])(\Er[q|z,w]-\Er[q|w])]}.
</script> Then $\Er[\tilde{g}_{jt}(\beta_0,\alpha_0,\Phi_0)] = 0$ and <script type="math/tex; mode=display">
0 = D_\Phi \Er[\tilde{g}_{jt}(\beta_0,\alpha_0,\Phi_0)]\;\;,\;\; 0 = D_\alpha
\Er[\tilde{g}_{jt}(\beta_0,\alpha_0,\Phi_0)].
</script> Hence, <script type="math/tex; mode=display">
\begin{align*}
\frac{1}{\sqrt{n}} \sum_{j,t} \tilde{g}_{jt}(\beta_0, \hat{\alpha}, \hat{\Phi} )
= & \frac{1}{\sqrt{n}} \sum_{j,t} \tilde{g}_{jt}(\beta_0, \alpha_0,
\Phi_0 ) + o_p(1) 
\end{align*}
</script>
</p>
<p>Under appropriate assumptions a CLT will apply to $\tilde{g}<em jt="jt">{jt}$, so
<script type="math/tex; mode=display">
\frac{1}{\sqrt{n}}  \sum_{j,t} \tilde{g}_{jt}(\hat{\beta}, \hat{\alpha},
\hat{\Phi} ) = \frac{1}{\sqrt{n}}  \sum_{j,t} \tilde{g}_{jt}(\beta_0,\alpha_0,\Phi_0) + o_p(1)
\indist N(0,\Sigma)
</script> Furthermore, $\Sigma$ can estimated by taking the sample (clustered)
covariance of $\tilde{g}</em>(\beta,\hat{\alpha},\hat{\Phi})$. Denote
this by $\hat{\Sigma}(\beta)$. We then have <script type="math/tex; mode=display">
    Q_n^{CUE}(\beta_0) = \left(\frac{1}{n}  \sum_{j,t} \tilde{g}_{jt}(\beta_0, \hat{\alpha},
    \hat{\Phi} ) \right)' \hat{\Sigma(\beta_0)}^{-1} \left(\frac{1}{n}
    \sum_{j,t} \tilde{g}_{jt}(\beta_0, \hat{\alpha}, \hat{\Phi} )
    \right) \indist \chi^2_{dim(\tilde{g}_{jt})}.
</script> This can be used to test $H_0: \beta = \beta_0$, or to form a
confidence region by taking all values of $\beta$ for which the test
fails to reject. Such an inference procedure is robust to identification
problems in that it does not require an assumption about
$D = D_\beta \Er[g_{jt}(\beta,\alpha_0,\Phi_0)]$ being full rank or
$Q_n$ being uniquely minimized at $\beta_0$. If we are willing to assume
strong identification in that $(D&rsquo; \Sigma(\beta_0)^{-1} D)$ is
invertible (and, loosely speaking, the estimated version of this is far
from singular relative to its estimation error), then we can take an
expansion to get <script type="math/tex; mode=display">
\sqrt{n}(\hat{\beta} - \beta_0) = (D'\Sigma^{-1} D)^{-1} D'\Sigma^{-1}
\frac{1}{\sqrt{n}} \tilde{g}_{jt}(\beta_0, \alpha_0, \Phi_0) + o_p(1)
\indist N(0, (D' \Sigma^{-1} D)^{-1}).
</script>
</p>
<p>See my notes from 628 and 622 on <a href="https://schrimpf.github.io/GMMInference.jl/extremumEstimation/">extremum
estimation</a>
and on
<a href="https://schrimpf.github.io/GMMInference.jl/bootstrap/">bootstrap</a> for
more details.</p>
<h1 id="results">Results<a class="headerlink" href="#results" title="Permanent link">&para;</a></h1>
<p>First, we must load the data and prepare the data. (This repeats what
was done in the assignment).</p>
<pre><code class="julia">using Pkg
try 
  using Dialysis # This assignment itself is in the &quot;Dialysis&quot; package. We will use some of the functions from it. 
catch
  Pkg.add(&quot;https://github.com/UBCECON567/Dialysis&quot;)
end
docdir = normpath(joinpath(dirname(Base.pathof(Dialysis)), &quot;..&quot;,&quot;docs&quot;))
Pkg.activate(docdir)
</code></pre>

<pre><code>Activating environment at `~/.julia/dev/Dialysis/docs/Project.toml`
</code></pre>
<pre><code class="julia">Pkg.instantiate()

using DataFrames                  
dialysis = loaddata()
sort!(dialysis, (:provfs, :year))
dialysis[!,:invest] = panellag(:stations, dialysis, :provfs, :year, -1) -
  dialysis[!,:stations]; 
dialysis[!,:labor] = (dialysis[!,:nurseFT] + 0.5*dialysis[!,:nursePT]+
                    dialysis[!,:ptcareFT] + 0.5*dialysis[!,:ptcarePT] +
                    dialysis[!,:dieticiansFT] + 0.5*dialysis[!,:dieticiansPT] +
                    dialysis[!,:social_workerFT] + 0.5*dialysis[!,:social_workerPT])
dialysis[!,:hiring] = panellag(:labor, dialysis, :provfs, :year, -1) -
  dialysis[!,:labor];
dialysis[!,:for_profit] = dialysis[!,:profit_status].==&quot;For Profit&quot;
dialysis[!,:fresenius] = dialysis[!,:chain_name].==&quot;FRESENIUS&quot;
dialysis[!,:davita] = dialysis[!,:chain_name].==&quot;DAVITA&quot;;
using Statistics # for mean, std, and so on 
dialysis[!,:inspected_this_year] =
  ((dialysis[!,:days_since_inspection].&gt;=0) .&amp;
   (dialysis[!,:days_since_inspection].&lt;365))
stateRates = by(dialysis, [:state, :year],
                df -&gt; mean(skipmissing(df[!,:inspected_this_year])))
rename!(stateRates, :x1 =&gt; :state_inspection_rate)
dialysis = join(dialysis, stateRates, on = [:state, :year]);
dialysis[!,:city] = uppercase.(dialysis[!,:city]) 
comps = by(dialysis,[:city,:year],
           df -&gt; mapreduce((x) -&gt; ifelse(ismissing(x),0,1*(x&gt;0)), +, df[!,:patient_months])
           )
rename!(comps, :x1 =&gt; :competitors)
dialysis = join(dialysis, comps, on = [:city,:year]);

using FixedEffectModels

dialysis[!,:idcat] = categorical(dialysis[!,:provfs])
qreg = reg(dialysis, @formula(pct_septic ~ days_since_inspection + patient_age +
                              pct_female + patient_esrd_years + pct_fistula + comorbidities +
                              hemoglobin), Vcov.cluster(:idcat),save=true) # saves residuals in augmentdf
dialysis[!,:quality] = -qreg.augmentdf[!,:residuals];
log_infmiss = x-&gt;ifelse(!ismissing(x) &amp;&amp; x&gt;0, log(x), missing) # -Inf confuses reg()
dialysis[!,:lpy] = log_infmiss.(dialysis[!,:patient_months]./12);
dialysis[!,:logL] = log_infmiss.(dialysis[!,:labor]);
dialysis[!,:logK] = log_infmiss.(dialysis[!,:stations]);
</code></pre>

<p>Now, we re-run the first step of estimation.</p>
<pre><code class="julia">inc1 = ((dialysis[!,:patient_months] .&gt; 0) .&amp; (dialysis[!,:labor] .&gt; 0) .&amp;
           (dialysis[!,:stations] .&gt; 0) .&amp;
           .!ismissing.(dialysis[!,:quality]) .&amp;
           .!ismissing.(dialysis[!,:std_mortality]) .&amp;
           (dialysis[!,:invest].==0) .&amp;
           (dialysis[!,:hiring].!=0));
inc1[ismissing.(inc1)] .= false;
dialysis[!,:inc1] = inc1;

dialysis[!,:lsmr] = log.(dialysis[!,:std_mortality] .+ .01)

(α, Φ, αreg, eyqz)=partiallinearIV(:lpy,  # y 
                         :quality, # q
                         :lsmr,   # z
                         [:hiring, :logL, :logK,
                         :state_inspection_rate, :competitors], # w
                         dialysis[findall(dialysis[!,:inc1]),:];
                         npregress=(xp, xd,yd)-&gt;polyreg(xp,xd,yd,degree=1),
                         parts=true                      
                         ) 

# we will need these later in step 2
dialysis[!,:Φ] = similar(dialysis[!,:lpy])
dialysis[:,:Φ] .= missing
rows = findall(dialysis[!,:inc1])
dialysis[rows,:Φ] = Φ
dialysis[!,:ey] = similar(dialysis[!,:lpy])
dialysis[:,:ey] .= missing
dialysis[rows,:ey] = eyqz[:,1]
dialysis[!,:eq] = similar(dialysis[!,:lpy])
dialysis[:,:eq] .= missing
dialysis[rows,:eq] = eyqz[:,2]
dialysis[!,:ez] = similar(dialysis[!,:lpy])
dialysis[:,:ez] .= missing
dialysis[rows,:ez] = eyqz[:,3]

α
</code></pre>

<pre><code>-0.016731503377462428
</code></pre>
<p>The <code>objective_gm</code> function in Dialysis.jl constructs the $Q_n^{CUE}$
objective function described above. The following code minimizes it and
plots a confidence region for $\beta$ based on inverting the $\chi^2$
test described above.</p>
<pre><code class="julia">using Optim

(obj, momenti, cue, varfn) = objective_gm(:lpy, :logK, :logL, :quality, :Φ,
                                          :provfs, :year,
                                          [:logK, :logL], dialysis,
                                          clusterid=:idcat,
                                          npregress=(xp,xd,yd)-&gt;polyreg(xp,xd,yd,degree=1,
                                                                        deriv=true));
res = optimize(b-&gt;cue(b,α),    # objective
               [0.0, 0.0], # lower bounds, should not be needed, but
               # may help numerical performance 
               [1.0, 1.0], # upper bounds 
               [0.4, 0.2], # initial value               
               Fminbox(BFGS()),  # algorithm
               autodiff=:forward)
res
</code></pre>

<pre><code>* Status: success

 * Candidate solution
    Minimizer: [2.01e-20, 1.50e-01]
    Minimum:   2.340586e+00

 * Found with
    Algorithm:     Fminbox with BFGS
    Initial Point: [4.00e-01, 2.00e-01]

 * Convergence measures
    |x - x'|               = 3.00e-17 ≰ 0.0e+00
    |x - x'|/|x'|          = 2.00e-16 ≰ 0.0e+00
    |f(x) - f(x')|         = 0.00e+00 ≤ 0.0e+00
    |f(x) - f(x')|/|f(x')| = 0.00e+00 ≤ 0.0e+00
    |g(x)|                 = 4.44e+00 ≰ 1.0e-08

 * Work counters
    Seconds run:   183  (vs limit Inf)
    Iterations:    7
    f(x) calls:    1273
    ∇f(x) calls:   1273
</code></pre>
<pre><code class="julia"># calculate standard errors based on normal approximation
βhat = res.minimizer
gi = momenti(βhat,α)
(Σ, n) = varfn(gi)
using ForwardDiff
using LinearAlgebra # for diag
D = ForwardDiff.jacobian(β-&gt;mean(momenti(β,α), dims=1), βhat)
Vβ = inv(D'*inv(Σ)*D)/n
@show βhat
</code></pre>

<pre><code>βhat = [2.0072100763983107e-20, 0.14987299477155056]
</code></pre>
<pre><code class="julia">@show sqrt.(diag(Vβ))
</code></pre>

<pre><code>sqrt.(diag(Vβ)) = [1.4692926354291502, 0.05948171117514949]
</code></pre>
<pre><code class="julia">nothing;
</code></pre>

<pre><code class="julia">using Plots, Distributions
Plots.gr()
lb = [0., 0.]
ub = [1.,  1.]
ntest = 200
βtest = [rand(2).*(ub-lb) .+ lb for i in 1:ntest];
βtest = vcat(βtest'...)

fn = β-&gt;cue(β,α)
pval = Vector{Float64}(undef, ntest);
qval = similar(pval);
Threads.@threads for i in 1:size(βtest,1)
  qval[i] = fn(βtest[i,:]);
  pval[i] = cdf(Chisq(2),qval[i]);
end

crit = 0.9
fig=scatter(βtest[:,1],βtest[:,2], group=(pval.&lt;crit), legend=false,
            markersize=4, markerstrokewidth=0.0, seriesalpha=1.0,
            palette=:isolum, xlabel=&quot;βk&quot;, ylabel=&quot;βl&quot;)
ngrid = 20
b = range.(lb,ub, length=ngrid)
pc = Matrix{Float64}(undef, length(b[1]), length(b[2]))
qc = similar(pc)
Threads.@threads for i in CartesianIndices(pc)
  qc[i] = fn([b[1][i[1]],b[2][i[2]]]);
  pc[i] = cdf(Chisq(2),qc[i]);
end

fig=contour!(b[1],b[2],pc',
            levels = [0.75, 0.9, 0.95, 0.99],
             contour_labels=true)
display(fig)
</code></pre>

<p><img alt="" src="../figures/dialysis-inference_4_1.png" /></p>
<p>Here we plot $Q^{CUE}_n(\beta)$.</p>
<pre><code class="julia">Plots.plotly()
surface(b[1],b[2],qc', xlabel=&quot;βk&quot;, ylabel=&quot;βl&quot;)
</code></pre>

<pre><code>Plot{Plots.PlotlyBackend() n=1}
</code></pre>
<div class="references hanging-indent" id="refs">
<div id="ref-chernozhukov2018">
<p>Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo,
Christian Hansen, Whitney Newey, and James Robins. 2018.
“Double/Debiased Machine Learning for Treatment and Structural
Parameters.” <em>The Econometrics Journal</em> 21 (1): C1–C68.
<a href="https://doi.org/10.1111/ectj.12097">https://doi.org/10.1111/ectj.12097</a>.</p>
</div>
<div id="ref-newey1994">
<p>Newey, Whitney K., and Daniel McFadden. 1994. “Chapter 36 Large Sample
Estimation and Hypothesis Testing.” In, 4:2111–2245. Handbook of
Econometrics. Elsevier.
<a href="https://doi.org/https://doi.org/10.1016/S1573-4412(05)80005-4">https://doi.org/https://doi.org/10.1016/S1573-4412(05)80005-4</a>.</p>
</div>
</div></div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Paul Schrimpf</p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../assets/mathjaxhelper.js" defer></script>
        <script src="../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
